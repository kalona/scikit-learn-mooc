Search.setIndex({"alltitles": {"(Estimated) predicted probabilities": [[141, "estimated-predicted-probabilities"]], "(Estimated) predicted probabilities in multi-class problems": [[163, "estimated-predicted-probabilities-in-multi-class-problems"]], "0. Presentation of the dataset": [[108, "presentation-of-the-dataset"]], "1. Linear model inspection": [[108, "linear-model-inspection"]], "1. The machine learning pipeline": [[5, "the-machine-learning-pipeline"]], "2. Adapting model complexity to the data": [[5, "adapting-model-complexity-to-the-data"]], "2. RandomForest feature_importances_": [[108, "randomforest-feature-importances"]], "3. Feature importance by permutation": [[108, "feature-importance-by-permutation"]], "3. Specific models": [[5, "specific-models"]], "A look at random forests": [[119, "a-look-at-random-forests"]], "A surprising association?": [[108, null]], "API": [[2, "api"]], "Accuracy as a baseline": [[142, "accuracy-as-a-baseline"]], "Acknowledgement": [[0, null]], "Adaptive Boosting (AdaBoost)": [[109, null]], "Aggregating": [[110, "aggregating"]], "Analysis": [[89, "analysis"], [89, "id1"]], "Analysis of hyperparameter search results": [[153, null]], "Appendix": [[165, null]], "Automated tuning": [[178, null]], "Bagging": [[110, null]], "Bagging complex pipelines": [[110, "bagging-complex-pipelines"]], "Bagging in scikit-learn": [[110, "bagging-in-scikit-learn"]], "Before getting started": [[12, "before-getting-started"], [21, "before-getting-started"], [32, "before-getting-started"], [38, "before-getting-started"], [55, "before-getting-started"], [70, "before-getting-started"], [170, "before-getting-started"], [182, "before-getting-started"]], "Benefits of using feature selection": [[125, null]], "Bias versus variance trade-off": [[54, null]], "Biases in the data": [[5, "biases-in-the-data"]], "Bootstrap resampling": [[110, "bootstrap-resampling"]], "Bringing value: The bigger picture beyond machine-learning": [[5, "bringing-value-the-bigger-picture-beyond-machine-learning"]], "Build a classification decision tree": [[157, null]], "Caveats of feature selection": [[31, null]], "Checking the variability of the coefficients": [[108, "checking-the-variability-of-the-coefficients"]], "Choice of cross-validation": [[19, null]], "Choice of the output/the labeled dataset": [[5, "choice-of-the-output-the-labeled-dataset"]], "Choosing an encoding strategy": [[84, "choosing-an-encoding-strategy"]], "Classification": [[142, null]], "Classification dataset": [[158, "classification-dataset"]], "Classification metrics": [[29, null]], "Classifier predictions": [[142, "classifier-predictions"]], "Comparing a model with simple baselines": [[18, null]], "Comparing model performance with a simple baseline": [[91, null]], "Concluding remarks": [[5, null], [165, null]], "Confusion matrix and derived metrics": [[142, "confusion-matrix-and-derived-metrics"]], "Course presentation": [[35, "course-presentation"]], "Create helper functions": [[161, "create-helper-functions"]], "Creating decision rules by hand": [[73, "creating-decision-rules-by-hand"]], "Cross-validation framework": [[101, null]], "Data loading": [[128, "data-loading"], [134, "data-loading"]], "Data preparation": [[76, "data-preparation"], [81, "data-preparation"]], "Datasets description": [[1, null]], "Decision tree for regression": [[162, null]], "Decision tree in classification": [[167, null]], "Decision tree in regression": [[176, null]], "Decision tree models": [[165, null]], "Details about default hyperparameters": [[119, "details-about-default-hyperparameters"]], "Dispatch columns to a specific processor": [[85, "dispatch-columns-to-a-specific-processor"]], "Effect of regularization": [[133, "effect-of-regularization"]], "Effect of the max_depth parameter": [[161, "effect-of-the-max-depth-parameter"]], "Effect of the sample size in cross-validation": [[95, null]], "Encoding nominal categories (without assuming any order)": [[84, "encoding-nominal-categories-without-assuming-any-order"]], "Encoding of categorical variables": [[84, null]], "Encoding ordinal categories": [[84, "encoding-ordinal-categories"]], "Engineering non-linear features": [[132, "engineering-non-linear-features"]], "Ensemble based on boosting": [[9, null]], "Ensemble method using bootstrapping": [[10, null]], "Ensemble of models": [[165, null]], "Evaluate our predictive pipeline": [[84, "evaluate-our-predictive-pipeline"]], "Evaluating model performance": [[165, null]], "Evaluation": [[152, "evaluation"]], "Evaluation and different probability thresholds": [[142, "evaluation-and-different-probability-thresholds"]], "Evaluation and hyperparameter tuning": [[152, null]], "Evaluation of the model with cross-validation": [[85, "evaluation-of-the-model-with-cross-validation"]], "Feature importance": [[108, null], [108, "id1"]], "Feature scaling and regularization": [[133, "feature-scaling-and-regularization"]], "Figure attributions": [[0, "figure-attributions"]], "Finally we can score the model using cross-validation:": [[90, "finally-we-can-score-the-model-using-cross-validation"]], "First comparison of GBDT vs. random forests": [[115, "first-comparison-of-gbdt-vs-random-forests"]], "First look at our dataset": [[73, null]], "First model with scikit-learn": [[80, null]], "First we load the dataset": [[90, "first-we-load-the-dataset"]], "Fit a model and make predictions": [[80, "fit-a-model-and-make-predictions"]], "Fitting a more powerful model": [[85, "fitting-a-more-powerful-model"]], "Fitting a scikit-learn model on numerical data": [[64, null]], "Follow scikit-learn on social networks": [[35, "follow-scikit-learn-on-social-networks"]], "Follow the MOOC": [[35, null]], "Get certified": [[5, "get-certified"]], "Glossary": [[2, null]], "Goals of this lesson": [[5, null]], "Going further with machine learning": [[5, "going-further-with-machine-learning"]], "Gradient-boosting decision tree": [[115, null]], "Handling categorical data": [[67, null]], "Histogram gradient-boosting decision trees": [[117, "histogram-gradient-boosting-decision-trees"]], "How the predictions are used": [[5, "how-the-predictions-are-used"]], "Hyperparameter tuning": [[117, null], [165, null]], "Hyperparameter tuning by grid-search": [[150, null]], "Hyperparameter tuning by randomized-search": [[154, null]], "Hyperparameter tuning with ensemble methods": [[11, null]], "Hyperparameters of decision tree": [[168, null]], "Identify categorical variables": [[84, "identify-categorical-variables"]], "Identify numerical data": [[79, "identify-numerical-data"]], "Impact of the regularization on the weights": [[131, "impact-of-the-regularization-on-the-weights"], [137, "impact-of-the-regularization-on-the-weights"]], "Impact of the regularization on with non-linear feature engineering": [[131, "impact-of-the-regularization-on-with-non-linear-feature-engineering"], [137, "impact-of-the-regularization-on-with-non-linear-feature-engineering"]], "Importance of decision tree hyperparameters on generalization": [[161, null]], "Influence of the parameter C on the decision boundary": [[131, "influence-of-the-parameter-c-on-the-decision-boundary"], [137, "influence-of-the-parameter-c-on-the-decision-boundary"]], "Introduction": [[35, null]], "Introductory example to ensemble models": [[118, null]], "Intuitions on linear models": [[37, null]], "Intuitions on tree-based models": [[169, null]], "Last lesson!": [[5, null]], "Learning curve": [[95, "learning-curve"]], "Learning more about scikit-learn": [[5, "learning-more-about-scikit-learn"]], "Lessons learned": [[108, "lessons-learned"]], "Limitation of selecting feature using a model": [[126, null]], "Linear models": [[165, null]], "Linear models for classification": [[141, null]], "Linear models with sparse coefficients (Lasso)": [[108, "linear-models-with-sparse-coefficients-lasso"]], "Linear regression using scikit-learn": [[138, null]], "Linear regression without scikit-learn": [[140, null]], "Loading the adult census dataset": [[73, "loading-the-adult-census-dataset"]], "Loading the dataset": [[152, "loading-the-dataset"]], "Loading the dataset with Pandas": [[80, "loading-the-dataset-with-pandas"]], "Loading the entire dataset": [[79, "loading-the-entire-dataset"]], "MOOC material": [[35, "mooc-material"]], "Machine Learning Concepts": [[165, null]], "Machine learning is a small part of the problem most of the times": [[5, "machine-learning-is-a-small-part-of-the-problem-most-of-the-times"]], "Main exercise": [[128, "main-exercise"], [134, "main-exercise"]], "Main take-away": [[13, null], [22, null], [33, null], [39, null], [57, null], [71, null], [171, null], [183, null]], "Main terms used in this course": [[2, "main-terms-used-in-this-course"]], "Manual tuning": [[180, null]], "Mathematical explanation": [[91, null]], "Model definition": [[128, "model-definition"], [134, "model-definition"]], "Model evaluation using cross-validation": [[76, null]], "Model fitting with preprocessing": [[81, "model-fitting-with-preprocessing"]], "Modeling non-additive feature interactions": [[132, "modeling-non-additive-feature-interactions"]], "Module overview": [[12, null], [21, null], [32, null], [38, null], [55, null], [70, null], [170, null], [182, null]], "More detail regarding cross_validate": [[101, "more-detail-regarding-cross-validate"]], "Multi-step feature engineering": [[132, "multi-step-feature-engineering"]], "Nested cross-validation": [[20, null], [96, null]], "Non i.i.d. data": [[100, null]], "Non-linear feature engineering for Linear Regression": [[139, null]], "Non-linear feature engineering for Logistic Regression": [[132, null]], "Non-linear feature engineering for linear models": [[40, null]], "Notebook Recap": [[73, "notebook-recap"], [80, "notebook-recap"], [139, "notebook-recap"]], "Notebook recap": [[76, "notebook-recap"], [79, "notebook-recap"]], "Notebook timings": [[3, null]], "Objectives and time schedule": [[12, "objectives-and-time-schedule"], [21, "objectives-and-time-schedule"], [32, "objectives-and-time-schedule"], [38, "objectives-and-time-schedule"], [55, "objectives-and-time-schedule"], [70, "objectives-and-time-schedule"], [170, "objectives-and-time-schedule"], [182, "objectives-and-time-schedule"]], "One-hot encoding of categorical variables": [[87, "one-hot-encoding-of-categorical-variables"], [89, "one-hot-encoding-of-categorical-variables"]], "Other hyperparameters in decision trees": [[161, "other-hyperparameters-in-decision-trees"]], "Other useful glossaries": [[2, "other-useful-glossaries"]], "Our predictive model": [[150, "our-predictive-model"], [152, "our-predictive-model"], [154, "our-predictive-model"]], "Overfit-generalization-underfit": [[102, null]], "Overfitting and underfitting": [[56, null]], "Overfitting vs. underfitting": [[102, "overfitting-vs-underfitting"]], "Prediction models versus causal models": [[5, "prediction-models-versus-causal-models"]], "Preprocessing for numerical features": [[81, null]], "Prerequisites": [[35, "prerequisites"], [128, "prerequisites"], [134, "prerequisites"]], "Question": [[14, null], [14, null], [14, null], [15, null], [15, null], [15, null], [15, null], [16, null], [16, null], [16, null], [17, null], [17, null], [17, null], [17, null], [17, null], [17, null], [23, null], [23, null], [23, null], [24, null], [25, null], [26, null], [26, null], [27, null], [27, null], [27, null], [27, null], [28, null], [28, null], [28, null], [28, null], [28, null], [28, null], [28, null], [28, null], [28, null], [28, null], [28, null], [28, null], [34, null], [34, null], [36, null], [36, null], [36, null], [36, null], [36, null], [41, null], [41, null], [41, null], [41, null], [41, null], [41, null], [41, null], [42, null], [42, null], [42, null], [43, null], [43, null], [43, null], [43, null], [43, null], [43, null], [43, null], [43, null], [43, null], [46, null], [46, null], [46, null], [46, null], [46, null], [46, null], [46, null], [46, null], [46, null], [48, null], [48, null], [48, null], [48, null], [50, null], [50, null], [52, null], [52, null], [52, null], [52, null], [52, null], [52, null], [59, null], [59, null], [59, null], [59, null], [59, null], [59, null], [59, null], [59, null], [59, null], [60, null], [60, null], [63, null], [63, null], [63, null], [63, null], [65, null], [65, null], [65, null], [65, null], [65, null], [65, null], [65, null], [68, null], [68, null], [68, null], [68, null], [72, null], [72, null], [72, null], [72, null], [72, null], [72, null], [72, null], [172, null], [172, null], [172, null], [173, null], [173, null], [173, null], [174, null], [174, null], [174, null], [175, null], [175, null], [177, null], [177, null], [177, null], [177, null], [179, null], [179, null], [179, null], [179, null], [179, null], [181, null], [181, null], [181, null], [181, null], [185, null], [185, null], [185, null], [185, null], [185, null], [185, null], [185, null]], "Random forest": [[117, "random-forest"]], "Random forests": [[119, null]], "Reference pipeline (no numerical scaling and integer-coded categories)": [[87, "reference-pipeline-no-numerical-scaling-and-integer-coded-categories"], [89, "reference-pipeline-no-numerical-scaling-and-integer-coded-categories"]], "Regression": [[145, null]], "Regression dataset": [[158, "regression-dataset"]], "Regression metrics": [[30, null]], "Regularization in linear model": [[44, null]], "Regularization of linear regression model": [[133, null]], "Sample grouping": [[94, null]], "Scale of coefficients": [[108, "scale-of-coefficients"]], "Scaling numerical features": [[87, "scaling-numerical-features"], [89, "scaling-numerical-features"]], "Select features based on their data type": [[84, "select-features-based-on-their-data-type"]], "Selecting the best model": [[165, null]], "Selection based on data types": [[85, "selection-based-on-data-types"]], "Separate the data and the target": [[80, "separate-the-data-and-the-target"]], "Set and get hyperparameters in scikit-learn": [[151, null]], "Sign of coefficients": [[108, "sign-of-coefficients"]], "Societal impact": [[5, "societal-impact"]], "Speeding-up gradient-boosting": [[116, null]], "Stability of the cross-validation estimates": [[101, "stability-of-the-cross-validation-estimates"]], "Strategies to encode categories": [[84, "strategies-to-encode-categories"]], "Stratification": [[99, null]], "Studying machine learning further": [[5, "studying-machine-learning-further"]], "Summary": [[95, "summary"], [101, "summary"]], "Summary and take-away messages": [[132, "summary-and-take-away-messages"]], "Summary:": [[102, "summary"]], "Table of contents": [[4, null], [165, null]], "Tabular data exploration": [[62, null]], "Take Away": [[108, "take-away"]], "Technical craft is not all": [[5, "technical-craft-is-not-all"]], "The Ames housing dataset": [[104, null]], "The California housing dataset": [[107, null]], "The adult census dataset": [[103, null]], "The big messages of the MOOC": [[5, "the-big-messages-of-the-mooc"]], "The bike rides dataset": [[105, null]], "The blood transfusion dataset": [[106, null]], "The issue of class imbalance": [[142, "the-issue-of-class-imbalance"]], "The need for a validation set": [[150, "the-need-for-a-validation-set"]], "The need for cross-validation": [[76, "the-need-for-cross-validation"]], "The penguins datasets": [[158, null]], "The predictive modeling pipeline": [[165, null]], "The variables (columns) in the dataset": [[73, "the-variables-columns-in-the-dataset"]], "Then we create the pipeline": [[90, "then-we-create-the-pipeline"]], "To go further": [[13, "to-go-further"], [22, "to-go-further"], [33, "to-go-further"], [39, "to-go-further"], [57, "to-go-further"], [71, "to-go-further"], [171, "to-go-further"], [183, "to-go-further"]], "Topics we have not covered": [[5, "topics-we-have-not-covered"]], "Train-test data split": [[80, "train-test-data-split"]], "Train-test split the dataset": [[79, "train-test-split-the-dataset"]], "Training error vs testing error": [[101, "training-error-vs-testing-error"]], "Tuning the regularization parameter": [[133, "tuning-the-regularization-parameter"]], "Tuning using a grid-search": [[150, "tuning-using-a-grid-search"]], "Tuning using a randomized-search": [[154, "tuning-using-a-randomized-search"]], "Using numerical and categorical variables together": [[85, null]], "Validation and evaluation matter": [[5, "validation-and-evaluation-matter"]], "Validation and learning curves": [[58, null]], "Validation curve": [[102, "validation-curve"]], "Visual inspection of the data": [[73, "visual-inspection-of-the-data"]], "Visualizing scikit-learn pipelines in Jupyter": [[90, null]], "We are an open-source community": [[5, "we-are-an-open-source-community"]], "Welcome!": [[35, null]], "What is noise?": [[102, "what-is-noise"]], "What you will learn": [[12, "what-you-will-learn"], [21, "what-you-will-learn"], [32, "what-you-will-learn"], [38, "what-you-will-learn"], [55, "what-you-will-learn"], [70, "what-you-will-learn"], [170, "what-you-will-learn"], [182, "what-you-will-learn"]], "Which encoder should I use?": [[87, "which-encoder-should-i-use"], [89, "which-encoder-should-i-use"]], "With hyperparameter tuning": [[152, "with-hyperparameter-tuning"]], "Without hyperparameter tuning": [[152, "without-hyperparameter-tuning"]], "Working with numerical data": [[79, null]], "Wrap-up": [[13, "wrap-up"], [22, "wrap-up"], [33, "wrap-up"], [39, "wrap-up"], [57, "wrap-up"], [71, "wrap-up"], [171, "wrap-up"], [183, "wrap-up"]], "classification": [[2, "classification"]], "classifier": [[2, "classifier"]], "cross-validation": [[2, "cross-validation"]], "data matrix, input data": [[2, "data-matrix-input-data"]], "discussion": [[108, "discussion"]], "early stopping": [[2, "early-stopping"]], "estimator": [[2, "estimator"]], "feature, variable, attribute, descriptor, covariate": [[2, "feature-variable-attribute-descriptor-covariate"]], "generalization performance, predictive performance, statistical performance": [[2, "generalization-performance-predictive-performance-statistical-performance"]], "hyperparameters": [[2, "hyperparameters"]], "infer, inference": [[2, "infer-inference"]], "learned parameters": [[2, "learned-parameters"]], "meta-estimator": [[2, "meta-estimator"]], "model": [[2, "model"]], "model state": [[2, "model-state"]], "overfitting": [[2, "overfitting"]], "predict, prediction": [[2, "predict-prediction"]], "predictor": [[2, "predictor"]], "regression": [[2, "regression"]], "regressor": [[2, "regressor"]], "regularization, penalization": [[2, "regularization-penalization"]], "sample, instance, observation": [[2, "sample-instance-observation"]], "supervised learning": [[2, "supervised-learning"]], "target, label, annotation": [[2, "target-label-annotation"]], "test set": [[2, "test-set"]], "train set": [[2, "train-set"]], "train, learn, fit": [[2, "train-learn-fit"]], "transformer": [[2, "transformer"]], "underfitting": [[2, "underfitting"]], "unsupervised learning": [[2, "unsupervised-learning"]], "validation set": [[2, "validation-set"]], "\u2705 Quiz": [[34, null], [36, null]], "\u2705 Quiz Intro.01": [[48, null]], "\u2705 Quiz M1.01": [[63, null]], "\u2705 Quiz M1.02": [[65, null]], "\u2705 Quiz M1.03": [[68, null]], "\u2705 Quiz M2.01": [[60, null]], "\u2705 Quiz M2.02": [[52, null]], "\u2705 Quiz M2.03": [[50, null]], "\u2705 Quiz M3.01": [[181, null]], "\u2705 Quiz M3.02": [[179, null]], "\u2705 Quiz M4.01": [[41, null]], "\u2705 Quiz M4.02": [[42, null]], "\u2705 Quiz M4.03": [[43, null]], "\u2705 Quiz M5.01": [[172, null]], "\u2705 Quiz M5.02": [[173, null]], "\u2705 Quiz M5.03": [[174, null]], "\u2705 Quiz M5.04": [[175, null]], "\u2705 Quiz M6.01": [[14, null]], "\u2705 Quiz M6.02": [[15, null]], "\u2705 Quiz M6.03": [[16, null]], "\u2705 Quiz M7.01": [[23, null]], "\u2705 Quiz M7.02": [[24, null]], "\u2705 Quiz M7.03": [[25, null]], "\u2705 Quiz M7.04": [[26, null]], "\u2705 Quiz M7.05": [[27, null]], "\ud83c\udfa5 Analysis of hyperparameter search results": [[184, null]], "\ud83c\udfa5 Bias versus Variance": [[51, null]], "\ud83c\udfa5 Comparing train and test errors": [[53, null]], "\ud83c\udfa5 Concluding remarks": [[6, null]], "\ud83c\udfa5 Introducing machine-learning concepts": [[49, null]], "\ud83c\udfa5 Intuitions on ensemble models: bagging": [[7, null]], "\ud83c\udfa5 Intuitions on ensemble models: boosting": [[8, null]], "\ud83c\udfa5 Intuitions on linear models": [[45, null]], "\ud83c\udfa5 Intuitions on regularized linear models": [[47, null]], "\ud83c\udfa5 Intuitions on tree-based models": [[166, null]], "\ud83c\udfa5 Overfitting and Underfitting": [[61, null]], "\ud83c\udfa5 Validation of a model": [[66, null]], "\ud83c\udfa5 Visualizing scikit-learn pipelines in Jupyter": [[69, null]], "\ud83c\udfc1 Wrap-up quiz 1": [[72, null]], "\ud83c\udfc1 Wrap-up quiz 2": [[59, null]], "\ud83c\udfc1 Wrap-up quiz 3": [[185, null]], "\ud83c\udfc1 Wrap-up quiz 4": [[46, null]], "\ud83c\udfc1 Wrap-up quiz 5": [[177, null]], "\ud83c\udfc1 Wrap-up quiz 6": [[17, null]], "\ud83c\udfc1 Wrap-up quiz 7": [[28, null]], "\ud83d\udcc3 Solution for Exercise 01": [[127, null]], "\ud83d\udcc3 Solution for Exercise M1.01": [[75, null]], "\ud83d\udcc3 Solution for Exercise M1.02": [[82, null]], "\ud83d\udcc3 Solution for Exercise M1.03": [[83, null]], "\ud83d\udcc3 Solution for Exercise M1.04": [[88, null]], "\ud83d\udcc3 Solution for Exercise M1.05": [[89, null]], "\ud83d\udcc3 Solution for Exercise M2.01": [[97, null]], "\ud83d\udcc3 Solution for Exercise M3.01": [[155, null]], "\ud83d\udcc3 Solution for Exercise M3.02": [[156, null]], "\ud83d\udcc3 Solution for Exercise M4.01": [[134, null]], "\ud83d\udcc3 Solution for Exercise M4.02": [[135, null]], "\ud83d\udcc3 Solution for Exercise M4.03": [[136, null]], "\ud83d\udcc3 Solution for Exercise M4.04": [[137, null]], "\ud83d\udcc3 Solution for Exercise M5.01": [[163, null]], "\ud83d\udcc3 Solution for Exercise M5.02": [[164, null]], "\ud83d\udcc3 Solution for Exercise M6.01": [[120, null]], "\ud83d\udcc3 Solution for Exercise M6.02": [[121, null]], "\ud83d\udcc3 Solution for Exercise M6.03": [[122, null]], "\ud83d\udcc3 Solution for Exercise M6.04": [[123, null]], "\ud83d\udcc3 Solution for Exercise M7.01": [[98, null]], "\ud83d\udcc3 Solution for Exercise M7.02": [[146, null]], "\ud83d\udcc3 Solution for Exercise M7.03": [[147, null]], "\ud83d\udcdd Exercise 01": [[124, null]], "\ud83d\udcdd Exercise M1.01": [[74, null]], "\ud83d\udcdd Exercise M1.02": [[77, null]], "\ud83d\udcdd Exercise M1.03": [[78, null]], "\ud83d\udcdd Exercise M1.04": [[86, null]], "\ud83d\udcdd Exercise M1.05": [[87, null]], "\ud83d\udcdd Exercise M2.01": [[92, null]], "\ud83d\udcdd Exercise M3.01": [[148, null]], "\ud83d\udcdd Exercise M3.02": [[149, null]], "\ud83d\udcdd Exercise M4.01": [[128, null]], "\ud83d\udcdd Exercise M4.02": [[129, null]], "\ud83d\udcdd Exercise M4.03": [[130, null]], "\ud83d\udcdd Exercise M4.04": [[131, null]], "\ud83d\udcdd Exercise M5.01": [[159, null]], "\ud83d\udcdd Exercise M5.02": [[160, null]], "\ud83d\udcdd Exercise M6.01": [[111, null]], "\ud83d\udcdd Exercise M6.02": [[112, null]], "\ud83d\udcdd Exercise M6.03": [[113, null]], "\ud83d\udcdd Exercise M6.04": [[114, null]], "\ud83d\udcdd Exercise M7.01": [[93, null]], "\ud83d\udcdd Exercise M7.02": [[143, null]], "\ud83d\udcdd Exercise M7.03": [[144, null]], "\ud83d\udea7 Feature selection": [[165, null]], "\ud83d\udea7 Interpretation": [[165, null]]}, "docnames": ["appendix/acknowledgement", "appendix/datasets_intro", "appendix/glossary", "appendix/notebook_timings", "appendix/toc_redirect", "concluding_remarks", "concluding_remarks_video", "ensemble/bagging_slides", "ensemble/boosting_slides", "ensemble/ensemble_boosting_index", "ensemble/ensemble_bootstrap_index", "ensemble/ensemble_hyperparameters_index", "ensemble/ensemble_module_intro", "ensemble/ensemble_module_take_away", "ensemble/ensemble_quiz_m6_01", "ensemble/ensemble_quiz_m6_02", "ensemble/ensemble_quiz_m6_03", "ensemble/ensemble_wrap_up_quiz", "evaluation/cross_validation_baseline_index", "evaluation/cross_validation_choices_index", "evaluation/cross_validation_nested_index", "evaluation/evaluation_module_intro", "evaluation/evaluation_module_take_away", "evaluation/evaluation_quiz_m7_01", "evaluation/evaluation_quiz_m7_02", "evaluation/evaluation_quiz_m7_03", "evaluation/evaluation_quiz_m7_04", "evaluation/evaluation_quiz_m7_05", "evaluation/evaluation_wrap_up_quiz", "evaluation/metrics_classification_index", "evaluation/metrics_regression_index", "feature_selection/feature_selection_limitation_index", "feature_selection/feature_selection_module_intro", "feature_selection/feature_selection_module_take_away", "feature_selection/feature_selection_quiz", "index", "interpretation/interpretation_quiz", "linear_models/linear_models_intuitions_index", "linear_models/linear_models_module_intro", "linear_models/linear_models_module_take_away", "linear_models/linear_models_non_linear_index", "linear_models/linear_models_quiz_m4_01", "linear_models/linear_models_quiz_m4_02", "linear_models/linear_models_quiz_m4_03", "linear_models/linear_models_regularization_index", "linear_models/linear_models_slides", "linear_models/linear_models_wrap_up_quiz", "linear_models/regularized_linear_models_slides", "ml_concepts/quiz_intro_01", "ml_concepts/slides", "overfit/bias_vs_variance_quiz_m2_03", "overfit/bias_vs_variance_slides", "overfit/learning_validation_curves_quiz_m2_02", "overfit/learning_validation_curves_slides", "overfit/overfit_bias_variance_index", "overfit/overfit_module_intro", "overfit/overfit_overfitting_underfitting_index", "overfit/overfit_take_away", "overfit/overfit_validation_learning_curves_index", "overfit/overfit_wrap_up_quiz", "overfit/overfitting_vs_under_fitting_quiz_m2_01", "overfit/overfitting_vs_under_fitting_slides", "predictive_modeling_pipeline/01_tabular_data_exploration_index", "predictive_modeling_pipeline/01_tabular_data_exploration_quiz_m1_01", "predictive_modeling_pipeline/02_numerical_pipeline_index", "predictive_modeling_pipeline/02_numerical_pipeline_quiz_m1_02", "predictive_modeling_pipeline/02_numerical_pipeline_video_cross_validation", "predictive_modeling_pipeline/03_categorical_pipeline_index", "predictive_modeling_pipeline/03_categorical_pipeline_quiz_m1_03", "predictive_modeling_pipeline/03_categorical_pipeline_visualization_video", "predictive_modeling_pipeline/predictive_modeling_module_intro", "predictive_modeling_pipeline/predictive_modeling_module_take_away", "predictive_modeling_pipeline/wrap_up_quiz", "python_scripts/01_tabular_data_exploration", "python_scripts/01_tabular_data_exploration_ex_01", "python_scripts/01_tabular_data_exploration_sol_01", "python_scripts/02_numerical_pipeline_cross_validation", "python_scripts/02_numerical_pipeline_ex_00", "python_scripts/02_numerical_pipeline_ex_01", "python_scripts/02_numerical_pipeline_hands_on", "python_scripts/02_numerical_pipeline_introduction", "python_scripts/02_numerical_pipeline_scaling", "python_scripts/02_numerical_pipeline_sol_00", "python_scripts/02_numerical_pipeline_sol_01", "python_scripts/03_categorical_pipeline", "python_scripts/03_categorical_pipeline_column_transformer", "python_scripts/03_categorical_pipeline_ex_01", "python_scripts/03_categorical_pipeline_ex_02", "python_scripts/03_categorical_pipeline_sol_01", "python_scripts/03_categorical_pipeline_sol_02", "python_scripts/03_categorical_pipeline_visualization", "python_scripts/cross_validation_baseline", "python_scripts/cross_validation_ex_01", "python_scripts/cross_validation_ex_02", "python_scripts/cross_validation_grouping", "python_scripts/cross_validation_learning_curve", "python_scripts/cross_validation_nested", "python_scripts/cross_validation_sol_01", "python_scripts/cross_validation_sol_02", "python_scripts/cross_validation_stratification", "python_scripts/cross_validation_time", "python_scripts/cross_validation_train_test", "python_scripts/cross_validation_validation_curve", "python_scripts/datasets_adult_census", "python_scripts/datasets_ames_housing", "python_scripts/datasets_bike_rides", "python_scripts/datasets_blood_transfusion", "python_scripts/datasets_california_housing", "python_scripts/dev_features_importance", "python_scripts/ensemble_adaboost", "python_scripts/ensemble_bagging", "python_scripts/ensemble_ex_01", "python_scripts/ensemble_ex_02", "python_scripts/ensemble_ex_03", "python_scripts/ensemble_ex_04", "python_scripts/ensemble_gradient_boosting", "python_scripts/ensemble_hist_gradient_boosting", "python_scripts/ensemble_hyperparameters", "python_scripts/ensemble_introduction", "python_scripts/ensemble_random_forest", "python_scripts/ensemble_sol_01", "python_scripts/ensemble_sol_02", "python_scripts/ensemble_sol_03", "python_scripts/ensemble_sol_04", "python_scripts/feature_selection_ex_01", "python_scripts/feature_selection_introduction", "python_scripts/feature_selection_limitation_model", "python_scripts/feature_selection_sol_01", "python_scripts/linear_models_ex_01", "python_scripts/linear_models_ex_02", "python_scripts/linear_models_ex_03", "python_scripts/linear_models_ex_04", "python_scripts/linear_models_feature_engineering_classification", "python_scripts/linear_models_regularization", "python_scripts/linear_models_sol_01", "python_scripts/linear_models_sol_02", "python_scripts/linear_models_sol_03", "python_scripts/linear_models_sol_04", "python_scripts/linear_regression_in_sklearn", "python_scripts/linear_regression_non_linear_link", "python_scripts/linear_regression_without_sklearn", "python_scripts/logistic_regression", "python_scripts/metrics_classification", "python_scripts/metrics_ex_01", "python_scripts/metrics_ex_02", "python_scripts/metrics_regression", "python_scripts/metrics_sol_01", "python_scripts/metrics_sol_02", "python_scripts/parameter_tuning_ex_02", "python_scripts/parameter_tuning_ex_03", "python_scripts/parameter_tuning_grid_search", "python_scripts/parameter_tuning_manual", "python_scripts/parameter_tuning_nested", "python_scripts/parameter_tuning_parallel_plot", "python_scripts/parameter_tuning_randomized_search", "python_scripts/parameter_tuning_sol_02", "python_scripts/parameter_tuning_sol_03", "python_scripts/trees_classification", "python_scripts/trees_dataset", "python_scripts/trees_ex_01", "python_scripts/trees_ex_02", "python_scripts/trees_hyperparameters", "python_scripts/trees_regression", "python_scripts/trees_sol_01", "python_scripts/trees_sol_02", "toc", "trees/slides", "trees/trees_classification_index", "trees/trees_hyperparameters_index", "trees/trees_intuitions_index", "trees/trees_module_intro", "trees/trees_module_take_away", "trees/trees_quiz_m5_01", "trees/trees_quiz_m5_02", "trees/trees_quiz_m5_03", "trees/trees_quiz_m5_04", "trees/trees_regression_index", "trees/trees_wrap_up_quiz", "tuning/parameter_tuning_automated_index", "tuning/parameter_tuning_automated_quiz_m3_02", "tuning/parameter_tuning_manual_index", "tuning/parameter_tuning_manual_quiz_m3_01", "tuning/parameter_tuning_module_intro", "tuning/parameter_tuning_module_take_away", "tuning/parameter_tuning_parallel_plot_video", "tuning/parameter_tuning_wrap_up_quiz"], "envversion": {"sphinx": 62, "sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1}, "filenames": ["appendix/acknowledgement.md", "appendix/datasets_intro.md", "appendix/glossary.md", "appendix/notebook_timings.md", "appendix/toc_redirect.md", "concluding_remarks.md", "concluding_remarks_video.md", "ensemble/bagging_slides.md", "ensemble/boosting_slides.md", "ensemble/ensemble_boosting_index.md", "ensemble/ensemble_bootstrap_index.md", "ensemble/ensemble_hyperparameters_index.md", "ensemble/ensemble_module_intro.md", "ensemble/ensemble_module_take_away.md", "ensemble/ensemble_quiz_m6_01.md", "ensemble/ensemble_quiz_m6_02.md", "ensemble/ensemble_quiz_m6_03.md", "ensemble/ensemble_wrap_up_quiz.md", "evaluation/cross_validation_baseline_index.md", "evaluation/cross_validation_choices_index.md", "evaluation/cross_validation_nested_index.md", "evaluation/evaluation_module_intro.md", "evaluation/evaluation_module_take_away.md", "evaluation/evaluation_quiz_m7_01.md", "evaluation/evaluation_quiz_m7_02.md", "evaluation/evaluation_quiz_m7_03.md", "evaluation/evaluation_quiz_m7_04.md", "evaluation/evaluation_quiz_m7_05.md", "evaluation/evaluation_wrap_up_quiz.md", "evaluation/metrics_classification_index.md", "evaluation/metrics_regression_index.md", "feature_selection/feature_selection_limitation_index.md", "feature_selection/feature_selection_module_intro.md", "feature_selection/feature_selection_module_take_away.md", "feature_selection/feature_selection_quiz.md", "index.md", "interpretation/interpretation_quiz.md", "linear_models/linear_models_intuitions_index.md", "linear_models/linear_models_module_intro.md", "linear_models/linear_models_module_take_away.md", "linear_models/linear_models_non_linear_index.md", "linear_models/linear_models_quiz_m4_01.md", "linear_models/linear_models_quiz_m4_02.md", "linear_models/linear_models_quiz_m4_03.md", "linear_models/linear_models_regularization_index.md", "linear_models/linear_models_slides.md", "linear_models/linear_models_wrap_up_quiz.md", "linear_models/regularized_linear_models_slides.md", "ml_concepts/quiz_intro_01.md", "ml_concepts/slides.md", "overfit/bias_vs_variance_quiz_m2_03.md", "overfit/bias_vs_variance_slides.md", "overfit/learning_validation_curves_quiz_m2_02.md", "overfit/learning_validation_curves_slides.md", "overfit/overfit_bias_variance_index.md", "overfit/overfit_module_intro.md", "overfit/overfit_overfitting_underfitting_index.md", "overfit/overfit_take_away.md", "overfit/overfit_validation_learning_curves_index.md", "overfit/overfit_wrap_up_quiz.md", "overfit/overfitting_vs_under_fitting_quiz_m2_01.md", "overfit/overfitting_vs_under_fitting_slides.md", "predictive_modeling_pipeline/01_tabular_data_exploration_index.md", "predictive_modeling_pipeline/01_tabular_data_exploration_quiz_m1_01.md", "predictive_modeling_pipeline/02_numerical_pipeline_index.md", "predictive_modeling_pipeline/02_numerical_pipeline_quiz_m1_02.md", "predictive_modeling_pipeline/02_numerical_pipeline_video_cross_validation.md", "predictive_modeling_pipeline/03_categorical_pipeline_index.md", "predictive_modeling_pipeline/03_categorical_pipeline_quiz_m1_03.md", "predictive_modeling_pipeline/03_categorical_pipeline_visualization_video.md", "predictive_modeling_pipeline/predictive_modeling_module_intro.md", "predictive_modeling_pipeline/predictive_modeling_module_take_away.md", "predictive_modeling_pipeline/wrap_up_quiz.md", "python_scripts/01_tabular_data_exploration.py", "python_scripts/01_tabular_data_exploration_ex_01.py", "python_scripts/01_tabular_data_exploration_sol_01.py", "python_scripts/02_numerical_pipeline_cross_validation.py", "python_scripts/02_numerical_pipeline_ex_00.py", "python_scripts/02_numerical_pipeline_ex_01.py", "python_scripts/02_numerical_pipeline_hands_on.py", "python_scripts/02_numerical_pipeline_introduction.py", "python_scripts/02_numerical_pipeline_scaling.py", "python_scripts/02_numerical_pipeline_sol_00.py", "python_scripts/02_numerical_pipeline_sol_01.py", "python_scripts/03_categorical_pipeline.py", "python_scripts/03_categorical_pipeline_column_transformer.py", "python_scripts/03_categorical_pipeline_ex_01.py", "python_scripts/03_categorical_pipeline_ex_02.py", "python_scripts/03_categorical_pipeline_sol_01.py", "python_scripts/03_categorical_pipeline_sol_02.py", "python_scripts/03_categorical_pipeline_visualization.py", "python_scripts/cross_validation_baseline.py", "python_scripts/cross_validation_ex_01.py", "python_scripts/cross_validation_ex_02.py", "python_scripts/cross_validation_grouping.py", "python_scripts/cross_validation_learning_curve.py", "python_scripts/cross_validation_nested.py", "python_scripts/cross_validation_sol_01.py", "python_scripts/cross_validation_sol_02.py", "python_scripts/cross_validation_stratification.py", "python_scripts/cross_validation_time.py", "python_scripts/cross_validation_train_test.py", "python_scripts/cross_validation_validation_curve.py", "python_scripts/datasets_adult_census.py", "python_scripts/datasets_ames_housing.py", "python_scripts/datasets_bike_rides.py", "python_scripts/datasets_blood_transfusion.py", "python_scripts/datasets_california_housing.py", "python_scripts/dev_features_importance.py", "python_scripts/ensemble_adaboost.py", "python_scripts/ensemble_bagging.py", "python_scripts/ensemble_ex_01.py", "python_scripts/ensemble_ex_02.py", "python_scripts/ensemble_ex_03.py", "python_scripts/ensemble_ex_04.py", "python_scripts/ensemble_gradient_boosting.py", "python_scripts/ensemble_hist_gradient_boosting.py", "python_scripts/ensemble_hyperparameters.py", "python_scripts/ensemble_introduction.py", "python_scripts/ensemble_random_forest.py", "python_scripts/ensemble_sol_01.py", "python_scripts/ensemble_sol_02.py", "python_scripts/ensemble_sol_03.py", "python_scripts/ensemble_sol_04.py", "python_scripts/feature_selection_ex_01.py", "python_scripts/feature_selection_introduction.py", "python_scripts/feature_selection_limitation_model.py", "python_scripts/feature_selection_sol_01.py", "python_scripts/linear_models_ex_01.py", "python_scripts/linear_models_ex_02.py", "python_scripts/linear_models_ex_03.py", "python_scripts/linear_models_ex_04.py", "python_scripts/linear_models_feature_engineering_classification.py", "python_scripts/linear_models_regularization.py", "python_scripts/linear_models_sol_01.py", "python_scripts/linear_models_sol_02.py", "python_scripts/linear_models_sol_03.py", "python_scripts/linear_models_sol_04.py", "python_scripts/linear_regression_in_sklearn.py", "python_scripts/linear_regression_non_linear_link.py", "python_scripts/linear_regression_without_sklearn.py", "python_scripts/logistic_regression.py", "python_scripts/metrics_classification.py", "python_scripts/metrics_ex_01.py", "python_scripts/metrics_ex_02.py", "python_scripts/metrics_regression.py", "python_scripts/metrics_sol_01.py", "python_scripts/metrics_sol_02.py", "python_scripts/parameter_tuning_ex_02.py", "python_scripts/parameter_tuning_ex_03.py", "python_scripts/parameter_tuning_grid_search.py", "python_scripts/parameter_tuning_manual.py", "python_scripts/parameter_tuning_nested.py", "python_scripts/parameter_tuning_parallel_plot.py", "python_scripts/parameter_tuning_randomized_search.py", "python_scripts/parameter_tuning_sol_02.py", "python_scripts/parameter_tuning_sol_03.py", "python_scripts/trees_classification.py", "python_scripts/trees_dataset.py", "python_scripts/trees_ex_01.py", "python_scripts/trees_ex_02.py", "python_scripts/trees_hyperparameters.py", "python_scripts/trees_regression.py", "python_scripts/trees_sol_01.py", "python_scripts/trees_sol_02.py", "toc.md", "trees/slides.md", "trees/trees_classification_index.md", "trees/trees_hyperparameters_index.md", "trees/trees_intuitions_index.md", "trees/trees_module_intro.md", "trees/trees_module_take_away.md", "trees/trees_quiz_m5_01.md", "trees/trees_quiz_m5_02.md", "trees/trees_quiz_m5_03.md", "trees/trees_quiz_m5_04.md", "trees/trees_regression_index.md", "trees/trees_wrap_up_quiz.md", "tuning/parameter_tuning_automated_index.md", "tuning/parameter_tuning_automated_quiz_m3_02.md", "tuning/parameter_tuning_manual_index.md", "tuning/parameter_tuning_manual_quiz_m3_01.md", "tuning/parameter_tuning_module_intro.md", "tuning/parameter_tuning_module_take_away.md", "tuning/parameter_tuning_parallel_plot_video.md", "tuning/parameter_tuning_wrap_up_quiz.md"], "indexentries": {}, "objects": {}, "objnames": {}, "objtypes": {}, "terms": {"": [2, 3, 5, 16, 22, 24, 25, 28, 34, 35, 41, 46, 72, 73, 76, 77, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 115, 117, 118, 120, 121, 125, 126, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 141, 142, 143, 145, 146, 147, 150, 151, 152, 153, 154, 157, 158, 161, 162, 182, 185], "0": [0, 2, 3, 17, 28, 36, 41, 43, 46, 65, 72, 73, 75, 76, 79, 80, 81, 82, 83, 84, 85, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 161, 162, 163, 164, 177, 179, 185], "00": [28, 81, 101, 105, 134, 138, 150], "000": [46, 48, 72, 88, 91, 99, 101, 107, 115, 116, 124, 127, 141, 145, 155, 156], "000000": [79, 81, 91, 105, 107], "000000e": [91, 133], "000001": 154, "000003": 154, "000013": [153, 154], "000014e": 81, "000026": 154, "000048": 154, "000061": 152, "000066": 150, "000068": 153, "000075": 153, "000081": 153, "000097": 154, "000304": 150, "000314": 150, "000359": 150, "000404": 154, "000444": 154, "000515": 150, "000535": 153, "000715": 150, "000760": 153, "000808": 154, "000819": 152, "000846": 153, "000850": 150, "001": [151, 154], "001124": 150, "001146": 153, "001204": 150, "001222": 153, "001305": 153, "001358": 154, "001370": 150, "001455": 154, "001467": 150, "001562": 150, "001575": 150, "001588": 154, "0016": 105, "001623": 153, "001648": 150, "001793": 150, "001822": 152, "001851": 150, "001990": 154, "001993": 154, "002": [85, 88, 89, 151], "002013": 150, "002165": 153, "002199": 153, "002258": 153, "00235677": 101, "002417": 154, "00242257": 101, "002547": 154, "00255895": 101, "002576": 97, "002586": 153, "002592": 97, "002599": 97, "00260115": 146, "00260425": 146, "002611": 97, "00261378": 101, "00262117": 101, "00262928": 146, "002633": 101, "002637": 97, "00264001": 146, "00264072": 146, "00264263": 146, "00264454": 146, "002653": 97, "002656": 97, "0026567": 146, "00265765": 146, "002661": 153, "002662": 101, "00266814": 146, "002669": 97, "00269294": 146, "002696": 154, "00270438": 146, "002707": 150, "00270867": 146, "002714": 101, "00272059": 146, "00272655": 146, "002741": 154, "0027492": 146, "002753": 97, "00277352": 146, "00282311": 146, "00283194": 146, "00283408": 146, "002896": 101, "002917": 150, "003": [28, 76, 84, 87, 88, 89, 151, 152], "003015": 150, "003026": 97, "003118": 154, "003180": 101, "003207": 101, "003211": 101, "003214": 101, "003220": 101, "003234": 153, "003240": 101, "003246": 101, "003250": 101, "003256": 101, "003265": 101, "003277": 101, "003294": 150, "003296": 101, "003299": 101, "003300": 101, "003321": 101, "003326": 101, "003330": 101, "003336": 101, "003347": 101, "003351": 101, "003354": 101, "003360": 101, "003380": 101, "003382": 101, "003383": 101, "003385": 101, "003388": 101, "003390": 156, "003391": 101, "003398": 101, "003402": 101, "003410": 101, "003416": 101, "003417": 101, "003451": 101, "003538": 101, "003545": 150, "003610": 150, "003621": 153, "0036210968": 153, "003775": 150, "003826": 150, "004": 119, "004036": 150, "004274": 153, "004490": 154, "004505": 150, "005": 119, "005112": 153, "005130": 153, "005614": 150, "005790": 150, "006": [94, 119, 123], "006022": 150, "006079": 107, "007510": 150, "00787": 108, "008": [115, 116], "008068": 154, "009": 99, "01": [3, 10, 18, 31, 37, 56, 58, 62, 81, 91, 96, 105, 117, 118, 130, 131, 136, 137, 150, 151, 153, 165, 167, 169, 180], "010": [116, 155], "01004028": 76, "01007438": 76, "0100987": 76, "01022959": 76, "010357": 153, "01043272": 76, "010555": 97, "010738": 97, "010762": 97, "010910": 150, "011375": 97, "011453": 97, "011550": 97, "011775": 154, "011841": 97, "011917": 97, "012251": 91, "012724": 97, "013292": 97, "014": [94, 96], "014456": 153, "014958": 153, "015": 108, "015390e": 91, "0154488709": 153, "015449": 153, "015862": 91, "016": [115, 116], "016736": 153, "017167": 153, "017240": 117, "018": [89, 90], "018640": 117, "019351": 117, "01957631": 84, "019923": 117, "01_tabular_data_explor": 3, "01_tabular_data_exploration_ex_01": 3, "01_tabular_data_exploration_sol_01": 3, "02": [9, 10, 19, 29, 40, 58, 64, 81, 165, 167, 176, 178], "02034998": 84, "02048445": 84, "02055025": 84, "02165532": 84, "023006": 153, "023330": 120, "0234": 105, "023571": 91, "023658": 153, "023810": [101, 107, 108], "024410": 150, "02466226": 85, "02486277": 85, "02490568": 85, "02517319": 85, "02600694": 85, "02605707": 81, "026746": 101, "027": 94, "027523": 117, "027698": 117, "029": 89, "029005": 153, "02_numerical_pipeline_cross_valid": 3, "02_numerical_pipeline_ex_00": 3, "02_numerical_pipeline_ex_01": 3, "02_numerical_pipeline_hands_on": 3, "02_numerical_pipeline_introduct": 3, "02_numerical_pipeline_sc": 3, "02_numerical_pipeline_sol_00": 3, "02_numerical_pipeline_sol_01": 3, "03": [3, 9, 11, 20, 28, 30, 40, 44, 54, 64, 67, 91, 165, 176], "032645": 154, "032908": 105, "03303773": 109, "033223": 105, "033403": 153, "033571": 105, "033870": 105, "034555": 91, "03471139": 81, "035100": 117, "035833": 150, "036232": 154, "036786": 154, "037": 157, "03726708": 157, "038301": 153, "039361": 117, "03_categorical_pipelin": 3, "03_categorical_pipeline_column_transform": 3, "03_categorical_pipeline_ex_01": 3, "03_categorical_pipeline_ex_02": 3, "03_categorical_pipeline_sol_01": 3, "03_categorical_pipeline_sol_02": 3, "03_categorical_pipeline_visu": 3, "04": [11, 29, 44, 67, 81, 91, 109, 117, 133, 150, 165, 168], "040271": 150, "040635": 117, "041": 134, "041687": 150, "042372": 101, "043": 97, "044276": 150, "044408": 153, "045": 87, "045455": 101, "046539": 117, "047293": 117, "047970e": 81, "048780": 107, "05": [28, 30, 67, 91, 94, 98, 99, 100, 105, 107, 110, 115, 118, 121, 131, 133, 136, 137, 142, 152, 157, 161, 163, 165], "050461": 101, "051681": 117, "052069": 153, "052381": 107, "05263158": 109, "053": 81, "053150": 153, "053856e": 133, "054511": 117, "055563": 153, "055758": 120, "05584931": 76, "056048": 153, "056071": 91, "05613256": 76, "05675244": 76, "05680799": 76, "05864198": 109, "059290": 117, "05959249": 76, "06": 3, "060": 116, "060351": 153, "060527": 150, "061034": 117, "06109e": 133, "061283": 153, "061975": 150, "062": 107, "062725": 153, "063036": 154, "064627": 150, "065395": 150, "0659455480": 153, "065946": 153, "066322": 152, "066667": 107, "066917": 152, "067092": 150, "067350": 152, "067503": 117, "067979": 152, "068874": 150, "068985": 152, "07": [91, 108, 133], "070219": 117, "070655": 107, "071": 135, "071197": 117, "073059": [101, 107, 108], "073446": [101, 107, 108], "074142": 153, "074473": 117, "075318": 154, "076563": 153, "076653": 154, "077047": 154, "077721": 81, "07772106": 81, "078186": 153, "079415": 117, "08": [28, 100, 105, 133], "081049": 120, "081081": [101, 107, 108], "083": 118, "083489e": 91, "083745": 117, "084083": 147, "0842": 105, "084639": 91, "085871": 156, "086348": 117, "086764": 120, "08697490026177834": 133, "087": [118, 135], "08787269": 109, "0880": 105, "09": [3, 101, 105, 133], "091079": 154, "092993": 153, "093117": 117, "093878": 154, "095": 115, "095093": 153, "0950934559": 153, "095317": 150, "096371": 141, "096675": 107, "098791": 120, "099526": 107, "0x7f1d0b2a6d30": 154, "0x7f1d0b2bf550": 154, "0x7f1d0b2bf5e0": 154, "0x7f1d0b2c1430": 154, "0x7f1d0b2c1730": 154, "1": [2, 3, 17, 26, 28, 41, 42, 43, 46, 48, 59, 65, 73, 75, 76, 79, 80, 81, 82, 84, 85, 87, 88, 89, 90, 91, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 110, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 133, 134, 135, 136, 137, 139, 140, 141, 142, 143, 145, 146, 147, 148, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 161, 162, 163, 165, 177, 179, 185], "10": [2, 3, 17, 24, 35, 41, 46, 59, 72, 73, 76, 77, 79, 82, 84, 91, 93, 94, 95, 96, 97, 98, 101, 102, 104, 105, 106, 107, 108, 110, 113, 116, 117, 118, 120, 122, 127, 129, 130, 131, 132, 133, 135, 136, 137, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 161, 177, 179, 185], "100": [17, 46, 59, 79, 80, 91, 95, 98, 101, 102, 107, 108, 110, 111, 113, 114, 115, 116, 117, 118, 120, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 133, 135, 136, 137, 139, 142, 145, 149, 154, 155, 156, 185], "1000": [28, 46, 85, 105, 117, 122, 123, 137, 142, 144, 145, 147, 156], "10000": 108, "100000": 127, "1007": 101, "100_000": 110, "100k": 108, "101": [46, 185], "102": [84, 105], "102818": 101, "1029": 94, "103": [84, 105, 147, 157], "104147": 150, "10461772": 81, "105": [84, 105], "106": [84, 105], "1060737427": 153, "106832": 150, "107": 118, "107460": 91, "108": 146, "1087": 81, "108748": 150, "10878": 73, "109": 154, "109842": [101, 107, 108], "10_000": [139, 157], "10e": [92, 97], "10e2": [92, 97], "10th": [73, 84, 136], "11": [73, 84, 91, 101, 104, 117, 118, 120, 133, 150, 156, 161, 177], "110175": 81, "110536": 153, "112105": 91, "11250": [90, 104], "1132": 107, "113367": 147, "113577": 117, "114035": 101, "1144": 105, "11497569953977356": 133, "115": [84, 101], "1157": 94, "116386": 117, "1166": 107, "11687": [73, 83], "118105": 153, "11828e": 133, "11th": [73, 79, 84, 136, 150, 154], "12": [3, 73, 79, 81, 85, 90, 91, 101, 103, 104, 105, 106, 107, 115, 117, 120, 142, 150, 154, 156, 163], "120": 81, "120092": 101, "1201": 104, "120686": 153, "120765": 150, "121": 101, "121819": 153, "122": [84, 101, 107, 108, 154], "12211": [79, 85], "122807": 101, "123": 75, "123209": 101, "124": 142, "124277": 150, "1243": 107, "125": [116, 122], "12500": 106, "125207": 117, "126": 116, "126228": 91, "127": 84, "127201": 156, "1273587578442403": 154, "127359": 154, "127849": 154, "1287": 94, "129412": 142, "12e": 133, "12th": [73, 84, 136], "13": [3, 73, 79, 81, 91, 94, 101, 104, 105, 106, 117, 120, 132, 145, 154, 156], "130": [94, 116], "13000": 140, "13175": 90, "132577": 154, "133042": 153, "133333": 101, "134604": 154, "136": 116, "137": 145, "137484": 153, "1379": 104, "138": 84, "1387": 101, "1389": 73, "14": [3, 73, 91, 94, 101, 104, 105, 116, 117, 120, 132, 142, 145, 154, 156, 163], "140": 153, "14000": 134, "140000": 104, "140009": 91, "140510": 101, "141": 107, "1415": 94, "141873": 101, "142128": 101, "1422": 104, "1423": 104, "1425": 107, "14260": [90, 104], "142949": 101, "143360": 101, "14344": 85, "143535": 101, "143567": 101, "143594": 101, "143666": 101, "143733": 101, "143735": 101, "143857": 101, "143858": 101, "143962": 101, "144": 154, "144197": 101, "144234": 101, "144274": 101, "14450843": 81, "144564": 101, "144638": 101, "144753": 101, "144780": 101, "144796": 101, "144937": 101, "144945": 101, "144957": 101, "1452": 104, "145232": 101, "145264": 101, "145295": 101, "1455": 90, "145558": 101, "1456": 90, "1457": 90, "145718": 101, "1458": 90, "145812": 101, "145861": 101, "1459": [90, 104], "146": 153, "1460": [90, 104], "146006": 101, "146009": 101, "146276": 101, "146424": 101, "146496": 101, "146657": 101, "146978": 101, "147974": 91, "148": [105, 146], "1485": 80, "148986": 101, "149997": 91, "15": [2, 3, 73, 79, 81, 84, 91, 101, 102, 104, 114, 116, 117, 120, 123, 142, 154, 156, 157, 163, 177], "150": [105, 109], "15000": 134, "15015271408292113": 154, "150153": 154, "15024": [73, 79, 80, 130, 136, 150, 151, 154], "151": [75, 84, 153], "1519911082952933": 133, "1545": 94, "154546": 138, "155": [84, 154], "156": 105, "15784": 73, "1590": [73, 80], "16": [3, 73, 81, 84, 89, 91, 94, 101, 104, 106, 116, 117, 120, 139, 147, 153, 154, 156, 157, 163], "160": 116, "1601": 73, "160519": 117, "161": [105, 116, 157], "16192": 73, "162": 116, "162231": 150, "162264": 101, "16273999": 101, "163": 105, "164": 153, "165331": 91, "165524": 150, "166307": 101, "16655684": 84, "16664577": 101, "1667": 94, "16705751": 101, "16733408": 101, "168120": 153, "1692574": 84, "17": [3, 28, 73, 75, 79, 81, 91, 101, 104, 116, 117, 120, 129, 135, 141, 145, 147, 153, 154, 156, 157, 158], "170": [112, 121], "170225": 91, "170mm": 140, "17125511": 101, "1714923": 141, "17177061": 81, "171920": 101, "1725": 107, "173780": 101, "173852e": 81, "174": 146, "175": 164, "176363": 101, "17657471": 84, "176656": 117, "177257": 156, "178499": 120, "1797": 94, "17e": 133, "18": [3, 28, 73, 75, 79, 91, 100, 101, 104, 105, 117, 120, 129, 130, 135, 136, 139, 150, 151, 153, 154, 156, 158], "180": [110, 140], "18029213": 84, "181": [129, 135, 140], "1812": 73, "18139195": 84, "181467": [101, 107, 108], "181500": 104, "182": 84, "182242": 101, "183": 116, "183491": 120, "184": 84, "184716": 150, "185225": 91, "186": [129, 135, 140, 154], "186026": 117, "19": [3, 75, 84, 91, 101, 104, 105, 116, 117, 120, 129, 133, 135, 138, 153, 156, 158], "190": [129, 135, 140], "192": 154, "193": [129, 135, 140], "194585": 101, "19484": 85, "195": [129, 135, 140], "196565e": 81, "198": [129, 135, 140], "19832": 85, "199": 116, "1990": [101, 107], "1994": [73, 94, 103], "1995": 94, "1997": [101, 107], "1998": 94, "1_000": [17, 28, 94, 113, 122], "1_000_000": 133, "1d": [2, 28, 139], "1e": [110, 116, 131, 137, 151, 154], "1e0": 46, "1e3": 154, "1e5": 46, "1e6": [131, 137], "1f": [79, 110, 147], "1st": [48, 73, 84, 135], "1stflrsf": [46, 72, 104, 177], "2": [2, 3, 17, 27, 28, 36, 41, 42, 46, 48, 65, 70, 73, 75, 76, 79, 80, 81, 82, 84, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 110, 113, 115, 116, 117, 118, 119, 120, 122, 123, 125, 126, 127, 129, 130, 132, 133, 134, 135, 136, 139, 140, 141, 142, 144, 145, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 161, 163, 165, 177, 185], "20": [3, 17, 28, 59, 73, 75, 79, 84, 90, 91, 96, 101, 102, 103, 104, 105, 106, 107, 113, 117, 118, 120, 122, 129, 130, 133, 135, 136, 142, 147, 148, 149, 150, 151, 154, 155, 156, 158], "200": [17, 59, 72, 94, 105, 113, 115, 116, 122, 132, 154], "2000": 94, "2005": 94, "2006": [90, 104], "2007": [90, 104], "2008": [90, 104], "200_000": [72, 90], "2010": 90, "202": [129, 135, 140], "2020": [28, 105], "2025": 3, "202513e": 81, "202993": 153, "203": [115, 116], "203750e": 91, "204499": 150, "205543": 101, "206": [84, 162], "2061": 73, "20635": 101, "20636": 101, "20637": 101, "20638": 101, "20639": [101, 107], "2064": 145, "20640": [101, 107], "207": [116, 129, 135, 140], "207156": 154, "208": 154, "208500": 104, "21": [3, 84, 91, 101, 104, 105, 107, 108, 120, 147, 154, 156], "210": [129, 135, 140], "210069": 91, "210599": 150, "211005": 150, "211718": 150, "213912": 91, "215": [133, 156], "215543": 117, "216": 154, "216574": 91, "21e": 133, "22": [3, 73, 79, 91, 101, 104, 105, 107, 108, 120, 130, 136, 145, 147, 150, 151, 153, 154], "22025127": 81, "22041321": 85, "220446049250313e": 139, "222567": 91, "223500": 104, "224702": 154, "225": 147, "227": [153, 154], "228605": 156, "228801": 156, "229": 154, "229129": 107, "23": [3, 84, 91, 101, 104, 105, 106, 107, 108, 116, 120, 133], "230": [112, 121], "230mm": 140, "231": 116, "232": 154, "23357874": 101, "234": 83, "235": [116, 121, 164], "236": 154, "236109": 91, "236325": 154, "237968": 106, "238137": [101, 107, 108], "23881": 85, "24": [3, 85, 91, 101, 104, 105, 106, 107, 108, 120, 142, 147, 154], "2401": [101, 107, 108], "24051595": 85, "241": 154, "241053": 154, "242": 116, "243": 147, "244503": 154, "247": 73, "248": 116, "248343": 120, "248463": 117, "249": 116, "25": [3, 59, 73, 79, 80, 81, 85, 91, 101, 102, 104, 105, 107, 108, 120, 130, 134, 136, 140, 150, 151, 154], "2500": 90, "250000": 104, "251": 146, "253": 116, "25313": 85, "253714": 154, "25410914": 85, "254324": 101, "254717": 101, "255": 154, "25501013": 85, "255219": 153, "255355": 120, "256": [94, 116, 117, 154], "256799": 147, "2574": [101, 107, 108], "258": 142, "25983644": 85, "25e": 133, "26": [3, 91, 101, 104, 105, 120], "261": 121, "26291527": 101, "2657": 73, "266257": 120, "27": [73, 79, 84, 91, 101, 104, 105, 120, 130, 136, 150, 151, 154], "2700": 140, "270381": 120, "271818": 142, "272": 102, "273364e": 81, "274354": 120, "274549e": 133, "27618374": 81, "2764": 134, "279701": 150, "28": [3, 73, 79, 81, 84, 91, 101, 104, 105, 106, 116, 118, 130, 136, 150, 151, 154], "281": 104, "281853": [101, 107, 108], "282261": 107, "283476": 150, "285553": 150, "287251": 150, "287904": 153, "288": 155, "288136": [101, 107, 108], "28845333": 81, "28911": 85, "29": [3, 80, 85, 91, 101, 104, 108, 116, 125, 139, 154], "290": 123, "291": [101, 107], "291096": 150, "291777": 156, "29333333": 146, "295": 84, "297": [101, 107], "297739": 117, "298971": 91, "299": 135, "2d": [2, 73, 132, 139], "2e": 133, "2f": [100, 101, 117, 120, 122, 128, 131, 133, 134, 137, 138, 139, 140, 142, 150, 154, 157, 163], "2ndflrsf": [46, 72, 104, 177], "3": [0, 3, 17, 28, 41, 46, 55, 59, 72, 73, 75, 76, 79, 80, 81, 82, 83, 84, 90, 91, 92, 94, 96, 97, 98, 99, 101, 104, 105, 106, 107, 109, 110, 114, 115, 116, 117, 118, 120, 121, 123, 125, 126, 129, 130, 132, 133, 135, 136, 139, 140, 141, 142, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 160, 161, 162, 163, 164, 165, 170, 177, 182], "30": [3, 73, 79, 84, 85, 91, 92, 94, 95, 97, 101, 102, 104, 106, 107, 110, 116, 117, 118, 120, 130, 135, 136, 148, 150, 151, 152, 154, 155, 161, 164], "300": [46, 81, 105, 110, 115, 117, 121, 128, 134, 138, 140, 161], "301": 135, "3014": [101, 107, 108], "30507": 85, "305162": 101, "305556": 91, "306": 116, "30726": 85, "307961": 91, "309102": 154, "309259": 117, "31": [3, 101, 104, 114, 117, 123, 135, 142, 154], "310130e": 133, "310494": 117, "311": 153, "313": 138, "314865e": 81, "315789": 101, "316393": 120, "318133e": 133, "319": 153, "319038e": 133, "319174": 117, "319824": 101, "32": [3, 85, 101, 104, 117], "320": 102, "321941e": 133, "322": [101, 107, 108], "322007": 117, "32298137": 157, "323": [153, 157], "3236": 135, "323872": 117, "324503e": 133, "325": [95, 105], "3250": [106, 140], "3252": [101, 107, 108], "325285": 101, "325623": 101, "325635": 101, "32650": 73, "327": [145, 154], "3273": 80, "328": 154, "328094": 117, "328652e": 133, "328761e": 133, "328835e": 133, "329513": 101, "32x32": 94, "33": [3, 101, 104, 107, 117, 153], "331799e": 133, "333333": 107, "334185e": 133, "334673": 120, "336": 105, "336956e": 133, "337": [135, 140, 158], "337394e": 133, "338": [134, 140, 158], "338181e": 133, "33822677": 81, "3384": 135, "338739": 150, "338778e": 133, "339": [129, 135, 140, 158], "339232e": 133, "339576e": 133, "34": [3, 85, 101, 104, 107, 117, 118, 153], "340": [129, 135, 140, 158], "3400": 140, "341": [101, 129, 135, 140, 158], "342": [101, 129, 135, 140, 158], "343": [129, 135, 154], "343115e": 133, "344": [147, 154], "344120e": 91, "344629": 154, "3450": 140, "347783e": 133, "35": [73, 101, 104, 106, 117, 136], "3510": 135, "35111917342151344": 133, "351641": 120, "352": 101, "352148": 101, "354": 118, "356": 101, "35682": 107, "357": 154, "358": 101, "359681": 101, "36": [3, 72, 75, 101, 104, 117, 120, 122, 129, 135, 158, 162], "360169": 101, "361854": 150, "364373": 153, "3650": 140, "36631": [79, 81], "369": 121, "3698": 162, "37": [79, 80, 81, 84, 100, 101, 104, 107, 108, 117, 118, 153], "370341": 147, "370977": 117, "37155": [73, 83], "3724": 135, "373503": 120, "374": 142, "3750": 140, "3775": 140, "377560": 91, "38": [73, 79, 81, 84, 101, 104, 118, 130, 136, 150, 151, 154], "3800": 140, "380344": 120, "381": 76, "38253": 105, "38254": 105, "385": [76, 115], "386": 94, "386050": 107, "386320": 91, "386772": 154, "387785": 91, "3886": 101, "389876": 120, "39": [3, 75, 84, 85, 101, 104, 116, 120, 129, 135, 154, 158], "39068": 80, "39069": 80, "39070": 80, "39071": 80, "39072": 80, "39073": 80, "3914": 135, "392071": 120, "394161": 117, "395488": 117, "395787": 117, "3f": [76, 79, 80, 81, 83, 84, 85, 87, 88, 89, 90, 94, 96, 97, 99, 107, 115, 116, 118, 119, 121, 123, 127, 128, 134, 135, 141, 142, 145, 146, 147, 151, 152, 155, 157], "3ssnporch": [46, 72, 104, 177], "4": [2, 3, 17, 28, 72, 73, 75, 79, 80, 81, 82, 83, 84, 87, 89, 90, 91, 94, 97, 98, 99, 101, 102, 104, 105, 106, 107, 108, 110, 115, 116, 117, 120, 123, 125, 129, 130, 131, 132, 133, 135, 136, 137, 139, 140, 142, 147, 150, 151, 152, 153, 154, 156, 158, 161, 163, 165, 177], "40": [28, 73, 75, 79, 80, 81, 85, 101, 104, 117, 120, 129, 130, 134, 135, 136, 140, 149, 150, 151, 154, 156, 158], "400": 105, "4000": [103, 106, 140], "401": 115, "40497076": 146, "406": 145, "407": 81, "407382": 153, "40788361": 101, "408985": 91, "409": 105, "40e": 133, "41": [3, 80, 101, 104, 107, 108, 120], "410": 104, "4100": 140, "411089": 101, "413": [107, 154], "413821": 101, "414": 123, "415111": 101, "41520468": 146, "415551": 152, "416071": 152, "417": 115, "417523": 152, "418715": 120, "41947109": 101, "419578": 152, "42": [72, 79, 81, 83, 85, 104, 117, 118, 127, 132, 148, 149, 150, 152, 154, 155, 156], "420250": 101, "42105263": 146, "421170": 101, "42129": 85, "422": [107, 116, 142], "422695": 120, "423237": 152, "42378265": 81, "423952": 81, "424": 116, "429000": 107, "429741": 107, "43": [3, 85, 91, 94, 101, 104, 105, 116, 120, 129, 135, 154, 158], "43124676": 81, "431247": 81, "431434": 117, "432205": 117, "433377": 101, "4356": 81, "436413": 117, "43832": 84, "44": [73, 79, 91, 101, 104, 130, 135, 136, 150, 151, 154, 156], "440716": 107, "441": 105, "441577": 142, "44492564": 101, "445": 105, "445084e": 81, "445210": 142, "4452178932": 153, "445218": 153, "445564": 117, "447": 123, "447982": 156, "448205": 154, "448279": 147, "45": [3, 73, 80, 81, 84, 91, 101, 104, 106, 116, 134, 140, 141, 150, 156, 157, 163], "451056": 101, "451765": 142, "451797": 150, "452": 101, "452411": 153, "45356037": 146, "454": 115, "456739": 117, "45mm": 157, "46": [3, 80, 84, 85, 91, 101, 104, 115, 116, 120, 156], "460": 127, "460025": 153, "4600250010": 153, "460077": 117, "460185": 117, "462": 89, "462122": 107, "463084": 120, "464": 156, "4641588833612782": 133, "46637427": 146, "467047": 153, "4670474863": 153, "46901998": 109, "47": [80, 91, 101, 104, 120, 156], "471": 115, "471139e": 81, "473911": 107, "474173": 107, "475258": 117, "476744": 107, "477244": 91, "47e": 133, "48": [3, 79, 80, 81, 91, 101, 102, 104, 120, 153], "480": 155, "4813767874": 153, "48319243": 81, "488098": 117, "48837": [73, 79, 84, 130, 136, 150, 151, 154], "48838": [73, 79, 84, 130, 136, 150, 151, 154], "48839": [73, 79, 84, 130, 136, 150, 151, 154], "48840": [73, 79, 84, 130, 136, 150, 151, 154], "48841": [73, 79, 84, 130, 136, 150, 151, 154], "48842": [73, 79, 84, 130, 136, 150, 151, 154], "49": [84, 101, 104, 116, 117, 125, 129, 135, 138, 154, 156, 158], "490": [104, 127], "490778": 117, "493803": 150, "495": 153, "4950": 135, "496": [101, 107, 108, 153, 154], "497": [153, 154], "498": 153, "499": [153, 154], "4th": [73, 84], "4x4": 94, "5": [2, 3, 17, 28, 59, 65, 72, 73, 75, 76, 80, 81, 82, 83, 84, 85, 90, 91, 94, 95, 96, 97, 98, 99, 101, 102, 104, 105, 106, 107, 108, 109, 110, 111, 113, 114, 115, 117, 118, 119, 120, 121, 122, 123, 126, 129, 131, 132, 133, 134, 135, 137, 138, 139, 140, 141, 142, 145, 147, 150, 152, 153, 154, 156, 157, 158, 161, 162, 163, 164, 165, 181, 182, 185], "50": [17, 28, 32, 59, 73, 77, 79, 81, 82, 85, 91, 98, 99, 101, 102, 104, 105, 106, 107, 113, 115, 116, 117, 118, 119, 122, 129, 130, 133, 135, 136, 145, 150, 151, 152, 153, 154, 158], "500": [17, 59, 84, 85, 88, 101, 105, 107, 145, 153, 154], "5000": [73, 106, 125, 126, 134, 136, 140], "503": 116, "503017": 91, "5032": 162, "505": 142, "5050": 135, "50666667": 146, "506933": 117, "507198": 117, "508787": 156, "508904": 156, "509": 73, "509348": 154, "50e": 133, "50k": [73, 78, 79, 80, 81, 82, 83, 85, 103, 150, 154], "51": [3, 104, 120, 185], "510": 146, "51186791": 146, "516": 94, "518682": 120, "52": [73, 79, 101, 104, 105, 107, 108, 117, 120, 125, 130, 136, 150, 151, 154, 156, 157], "520": 127, "521": [107, 142], "522054": 101, "523": [118, 134], "524239": 117, "526": 107, "528": 123, "5281": 35, "53": [104, 105, 116, 125, 154, 156], "530310e": 81, "53605445": 81, "538870": 117, "539353": 91, "539567": 91, "54": [104, 105, 117, 154], "540456": 153, "542490": 91, "543652": 91, "544371": 91, "544430": 154, "5469": 94, "547140": 91, "547945": [101, 107, 108], "548235": 142, "548284": 91, "549338": 150, "549508": 91, "549777": 91, "55": [3, 80, 95, 104, 105, 129, 135, 158], "550075": 153, "55116959": 146, "553": [107, 115, 142], "554790": 142, "555556": [101, 107, 108], "5568": 101, "556965": 91, "558": [101, 107, 108], "558423": 142, "55e": 133, "56": [3, 85, 104, 105, 120, 154, 156], "5603": 101, "560606": 101, "564633e": 133, "565": [101, 107, 108], "56e": 133, "57": [3, 104, 105, 120], "570102": 156, "571093": 154, "572610e": 91, "572767": 101, "573": 134, "573103": 91, "57333333": 146, "574": 145, "576": 123, "576068": 91, "576792e": 81, "5769786920519312": 108, "5780": 138, "579032": 91, "58": [3, 73, 79, 104, 105, 130, 135, 136, 150, 151, 154], "58351894": 109, "585": 107, "58666667": 146, "588": 104, "5899811014945939": 108, "59": [3, 84, 104, 105], "590236": 101, "59025606": 81, "592996": 120, "594": 73, "594783": 154, "59553e": 133, "59556e": 133, "59564e": 133, "59566e": 133, "5957e": 133, "59593e": 133, "5959e": 133, "59609e": 133, "59686e": 133, "5972410717953709": 108, "59735e": 133, "5975758025570992": 108, "597833": 120, "59824e": 133, "59923e": 133, "5_000": 136, "5th": [73, 84, 85, 135, 136], "6": [3, 12, 21, 38, 46, 70, 72, 73, 75, 81, 82, 90, 91, 94, 97, 98, 99, 101, 102, 104, 105, 107, 108, 116, 117, 118, 120, 129, 131, 133, 135, 137, 142, 145, 147, 150, 152, 153, 154, 156, 157, 158, 161, 162, 165, 177], "60": [28, 73, 80, 81, 90, 104, 105, 108, 110, 120, 153, 161], "600": [101, 107], "6000": 106, "60047e": 133, "60121e": 133, "601315755610292": 108, "6013465986314437": 108, "601679": 154, "601784": 91, "60243e": 133, "603530": 117, "607357": 154, "60737e": 133, "607875": 101, "608": 145, "61": [104, 117, 120, 133, 154], "611280": 91, "6135907273413176": 133, "615": 142, "61694e": 133, "616981": 101, "618080": 150, "618773": 117, "62": [3, 90, 104, 120, 156], "620057": 120, "620318": 91, "626372": 91, "627": 96, "629": 146, "63": [3, 104, 110], "6300": 140, "63304094": 146, "635020": 117, "63513514": 146, "636768": 156, "637473": 147, "639": 155, "63975155": 157, "64": [3, 94, 104, 105, 146, 154], "640": [107, 157], "640x480": 109, "642": [85, 118], "642296": 154, "642352": 81, "64235211": 81, "6431": [101, 107, 108], "643585": 79, "646": 94, "646988": 120, "647": 85, "648": 85, "648786": 101, "65": [3, 84, 85, 90, 104, 122, 154], "653002": 101, "654769": 117, "657": 73, "66": [90, 100, 104, 105], "662623": 101, "663053": 117, "663100e": 81, "665": [108, 142], "6653108": 81, "665311": 81, "665618": 91, "666425": 156, "67": [84, 104, 105, 108, 154], "676": 108, "677425e": 81, "68": [3, 75, 90, 104, 154], "680000": 97, "68556640610011": 138, "686482": 117, "686645": 117, "687": 135, "6872520581075443": 145, "688": 142, "68e": 133, "69": [104, 109], "69090725": 81, "692308": 107, "692939": 81, "6929824561403509": 109, "69333333": 146, "693453": 117, "694": 116, "69e": 133, "6th": [73, 84, 85, 136], "7": [3, 17, 46, 72, 73, 75, 81, 82, 84, 91, 94, 97, 98, 99, 101, 104, 105, 107, 108, 115, 116, 117, 118, 120, 129, 133, 135, 142, 147, 150, 153, 154, 156, 158, 161, 163, 165, 177, 185], "70": [28, 73, 90, 104], "7000": 101, "7077": 135, "708": 135, "7083": 135, "71": [3, 104, 150, 162], "710510": 79, "71188483": 81, "713153": 91, "714245e": 81, "72": [3, 104, 154], "7200": 140, "7220306": 35, "722368e": 133, "72304e": 133, "7240": 140, "724791": 141, "725": 135, "72556083": 81, "725748": 81, "727109": 101, "727998": 101, "728182": 142, "73": [3, 80, 104], "731": 135, "733333": 97, "733808": 154, "7347": 135, "736": 145, "73684211": 146, "74": [3, 104, 154], "741": 101, "741752e": 81, "742356": 150, "743": 142, "745925": 120, "746667": 97, "7467": 135, "747": 106, "747528e": 133, "748": 106, "749092": 154, "75": [3, 59, 79, 81, 84, 90, 91, 98, 100, 104, 107], "7522": 81, "753534e": 91, "753674e": 81, "753988": 91, "755": 88, "756": 73, "75675676": 146, "756808": 98, "757176": 98, "757366": 154, "757463": 98, "757566": 91, "758": 116, "758941": 153, "758947": [153, 154], "758974": 153, "759142": 98, "759937": 150, "76": [3, 83, 97, 104, 106, 142, 146], "760329": 98, "7607182343065395": 83, "760739": 98, "761": 88, "761681": 98, "761885": 98, "762": 142, "762032": 106, "763114": 98, "764569": 153, "764947": 153, "765": 97, "765083": 153, "765281": 153, "765581": 120, "765902": 153, "766": 83, "767": 116, "767244": 117, "768721": 91, "7688": [73, 79, 130, 136, 150, 151, 154], "77": [3, 101, 104, 106, 116, 142], "770": 104, "77019645": 81, "771341": 153, "77333333": 146, "774303": 117, "7744": 85, "775": 95, "77536738": 81, "7755366885": 153, "776": 94, "7762": 85, "776413": 153, "777311": 117, "778": 142, "7780748663101604": 142, "779143": 153, "78": [101, 104, 142, 157], "783267": 153, "784404": 156, "7858": 135, "786667": 97, "786716": 91, "787": [107, 151], "789": 155, "78e": 133, "79": [72, 104], "790": 135, "791101": 117, "7917": 90, "79283521": 136, "794": 147, "795099": 156, "795231": 101, "79557785": 76, "796451": 150, "79668305": 136, "797131e": 91, "797166": 150, "79750205": 136, "797882": 150, "79856704": 136, "79873055": 76, "799": 151, "79914005": 136, "799211": 91, "7993448": 136, "79965192": 76, "7th": [73, 84, 85, 136], "8": [3, 59, 73, 75, 79, 82, 84, 90, 91, 94, 97, 98, 99, 100, 101, 103, 104, 107, 108, 110, 114, 115, 116, 117, 118, 120, 121, 123, 129, 131, 132, 133, 135, 136, 137, 139, 142, 147, 148, 150, 153, 154, 155, 156, 157, 158, 162, 163, 177, 179, 185], "80": [28, 72, 90, 91, 104, 148, 155], "800": [76, 151], "800000": 97, "80036855": 136, "80049135": 76, "800495": 117, "802260": [101, 107, 108], "802335": 120, "8025": 73, "802785": 117, "804": 80, "80405405": 136, "80456593": 76, "80487305": 136, "805255": 91, "80528256": 136, "807": [79, 81], "809": 135, "80k": 108, "81": [3, 28, 72, 78, 83, 116, 117], "810146": 120, "810368": 98, "810982": 98, "811604": 120, "812375": 98, "813": 155, "813112": 98, "813513": 147, "813849": 98, "814138": 147, "814452e": 133, "814831": 98, "815036": 98, "815569": 98, "815660": 91, "815937": 98, "816306": 98, "816374": 147, "816708": 150, "817352": [101, 107, 108], "817680e": 81, "8177909714402702": 82, "817832": 150, "818116": 107, "818956": 150, "82": [3, 78, 80, 83, 116, 117, 154], "820": 119, "820219": 91, "821140": 91, "821818": 154, "822215": 120, "822284": 91, "82241954": 17, "823774": 153, "824232": 120, "8242776341719346": 80, "824352e": 133, "826667": 97, "828": 155, "82831695": 84, "8285077": 141, "828574": 154, "829014": 101, "8290379545978042": 82, "83": [3, 73, 100, 116], "83120264": 17, "831358077066": 138, "831866e": 133, "83195043": 17, "83232675": 84, "83292383": 84, "833": [84, 88, 155], "83309064": 17, "834": 73, "83497133": 84, "83570478": 84, "837563e": 133, "838880": 154, "839": 123, "839907": 153, "83e": 133, "84": [90, 101, 104, 135], "840413": 150, "840667e": 81, "841881e": 133, "842": 155, "843330": 150, "844390": 153, "844684e": 81, "845": 101, "8450": [90, 104], "845377": 153, "846": 119, "846028": 153, "846154": 107, "8462": [101, 107, 108], "846246": 150, "8465505278706927": 108, "84684685": 136, "846847": 153, "847": 155, "8474611": 136, "84791111": 17, "84807535": 136, "8481712992211636": 108, "8482801": 85, "848321": 150, "84834224": 17, "848721": 147, "848736": 120, "84930385": 136, "849713": 153, "84971335": 136, "84993346": 85, "85": [3, 84, 90, 101, 107, 108, 156, 179], "8503276": 136, "85045978": 17, "85056295": 136, "8507371": 136, "851": [85, 119], "851028": 150, "85116184": 85, "8515561": 136, "85176085": 136, "85183089": 17, "852": 155, "8523751": 136, "852502": 154, "85257985": 85, "852752": 150, "8527846": 136, "85281474": 136, "85298935": 136, "853266": 150, "853403": 154, "853734": 150, "853781": 150, "85383828": 136, "853844": 150, "854": 134, "854491": 153, "85462735": 136, "854659": 154, "854826": 150, "85544636": 85, "85565111": 136, "856558": 153, "856675": 153, "857": [84, 155], "857389": 150, "8575055278028008": 85, "857552": 153, "857767": 153, "85790323": 17, "858332": 153, "85872236": 136, "858863": 150, "859": [90, 155], "859951": 153, "85e": 133, "86": [84, 90, 101, 107, 108, 117, 153], "860360": [152, 153], "860770": 153, "860784": 152, "860934": 150, "861429": 153, "861862": 153, "861881": 153, "862221": 153, "862271": 153, "86235297": 17, "862408": [152, 153], "862681": 153, "862899": 150, "863": 152, "863241": 152, "864195": 152, "864559": 101, "865001": 153, "865260": 117, "866015": 154, "866066": 150, "866365": 152, "866425": 150, "866563": 156, "866691": 120, "866783": 150, "866912": 152, "8672": 101, "867213": 150, "868": 135, "868063": 150, "868912": 150, "869457": 152, "869673": 154, "869837": 154, "87": [3, 84, 117, 150, 156], "870": 155, "870588": 142, "870793": 154, "870910": 152, "871": 152, "871339": 154, "871393": 154, "873": [87, 89], "873310": 120, "874": 135, "874470": 117, "875165": 91, "877": 152, "877676": 154, "88": [3, 84, 100, 101, 107, 108, 117, 150, 154], "8801899926295963": 85, "884016": 154, "89": [3, 81, 91, 101, 125, 156], "892": 147, "89722222": 94, "899316": 101, "8th": [73, 84, 85, 136], "8x8": 94, "9": [3, 28, 72, 73, 81, 82, 90, 91, 94, 97, 98, 99, 101, 104, 107, 108, 116, 117, 118, 120, 133, 135, 145, 147, 150, 154, 156, 158, 185], "90": [3, 28, 73, 79, 81, 91, 94, 99, 103, 134, 156], "900": 145, "901": 104, "901300": 101, "902": 145, "90250696": 94, "9042": 90, "906226": 117, "907397": 154, "908": 115, "909091": 107, "909797": 101, "90e": 133, "91": [3, 91, 104, 154, 156], "911828": 153, "913": 116, "915": 94, "92": [3, 80, 84, 91, 101, 145, 153, 185], "920918": 117, "923": 147, "929": 94, "929802": 117, "93": [3, 116], "931": 94, "934878": 91, "935108": 153, "935582": 91, "935672514619883": 109, "94": [76, 85, 100, 109, 154], "940": 127, "94166667": 94, "941912": 91, "943183": 117, "94707521": 94, "947952": 91, "95": [3, 107, 154], "951": 84, "951540": 91, "952839": 117, "953": 99, "953314": 91, "955": 73, "9550": [90, 104], "959243": 91, "96": 156, "9600": [90, 104], "96657382": 94, "967": 99, "968": 94, "968612": 101, "97": [3, 153, 163, 185], "9717": 90, "971880": [101, 107, 108], "972057": 120, "972476": 117, "972679": 120, "976": 115, "97630": 48, "976823": 154, "9769": 80, "979007": 91, "9794315178246614": 108, "98": [3, 106, 108, 157], "9804590975502424": 108, "981744": 91, "984127": [101, 107, 108], "984471": 91, "986667": 120, "99": [3, 81], "991373": 91, "992587": 117, "992936": 117, "9937": 90, "994918": 154, "996": 145, "99999": 81, "9th": [73, 84, 136], "A": [2, 5, 28, 43, 50, 52, 60, 63, 65, 73, 81, 85, 91, 94, 96, 101, 102, 107, 116, 117, 133, 136, 138, 139, 142, 145, 156, 162, 172, 173, 177, 183], "AND": [73, 136], "And": [5, 100], "As": [5, 28, 46, 73, 76, 79, 80, 81, 84, 85, 87, 89, 92, 96, 97, 98, 99, 100, 103, 104, 106, 107, 108, 110, 115, 116, 132, 133, 137, 141, 142, 144, 145, 147, 151, 152, 153, 156, 157, 161, 164], "At": [79, 96, 99, 101, 110, 142, 159, 163], "BY": [0, 35], "Be": [28, 73, 84, 85, 86, 88, 102, 117, 151, 177], "But": [5, 90, 91, 97, 101, 102, 107, 132, 133, 152, 153], "By": [14, 17, 28, 43, 46, 73, 81, 84, 96, 99, 102, 105, 110, 119, 127, 130, 133, 136, 142, 143, 145, 146, 147, 160, 164, 177], "For": [2, 5, 35, 41, 52, 57, 70, 73, 76, 79, 80, 81, 84, 85, 90, 91, 94, 96, 98, 99, 100, 101, 102, 105, 107, 108, 112, 113, 115, 117, 119, 121, 122, 124, 125, 126, 127, 129, 130, 131, 132, 133, 135, 136, 137, 139, 140, 141, 142, 145, 147, 150, 151, 152, 154, 157, 161, 163, 164, 173, 177, 182], "If": [2, 5, 27, 28, 36, 41, 52, 65, 73, 76, 80, 84, 91, 92, 94, 95, 96, 97, 100, 101, 102, 107, 108, 109, 111, 112, 113, 116, 117, 118, 119, 120, 121, 122, 125, 128, 129, 131, 132, 133, 134, 135, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 151, 152, 156, 157, 159, 160, 161, 162, 163, 164, 175, 185], "In": [2, 5, 14, 21, 22, 28, 33, 35, 36, 38, 39, 41, 43, 46, 57, 63, 71, 73, 76, 77, 79, 80, 81, 82, 83, 84, 85, 86, 88, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 114, 115, 116, 117, 118, 119, 121, 122, 123, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 161, 162, 163, 164, 171, 177, 179, 182, 183], "It": [2, 5, 28, 35, 57, 72, 73, 77, 80, 81, 82, 84, 85, 91, 94, 95, 96, 97, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 110, 115, 116, 118, 119, 123, 124, 127, 132, 133, 137, 140, 142, 143, 145, 146, 147, 152, 153, 154, 157, 161, 162, 163], "Its": 108, "No": [5, 23], "Not": [5, 85, 86, 88, 96, 136], "OR": 132, "Of": 125, "On": [2, 5, 28, 79, 81, 82, 85, 90, 96, 101, 108, 109, 110, 117, 122, 124, 127, 132, 133, 135, 136, 137, 138, 139, 142, 145, 150, 152, 154, 157, 162, 163, 164], "One": [2, 48, 59, 68, 72, 76, 77, 82, 84, 99, 100, 102, 106, 108, 115, 116, 142, 145, 152], "Such": [91, 110, 133], "That": [102, 142, 147, 150, 152], "The": [0, 1, 2, 12, 17, 21, 32, 35, 36, 38, 41, 43, 46, 48, 55, 57, 59, 65, 70, 72, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 108, 109, 110, 111, 112, 113, 114, 116, 117, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 144, 145, 147, 148, 149, 152, 153, 154, 155, 156, 157, 159, 160, 161, 162, 163, 164, 170, 172, 174, 177, 182, 183, 185], "Their": [94, 141], "Then": [55, 70, 76, 81, 85, 91, 94, 101, 110, 114, 118, 123, 124, 125, 126, 127, 130, 131, 132, 136, 137, 139, 141, 143, 144, 145, 146, 147, 150, 152, 154, 157], "There": [75, 81, 101, 102, 103, 106], "These": [2, 5, 22, 73, 76, 80, 84, 104, 110, 119, 142, 151, 162, 182], "To": [2, 7, 8, 28, 36, 45, 46, 47, 49, 51, 53, 61, 66, 72, 73, 79, 80, 81, 85, 86, 88, 92, 94, 95, 96, 97, 99, 100, 101, 102, 108, 109, 110, 113, 115, 116, 117, 122, 125, 129, 132, 133, 135, 137, 138, 139, 142, 145, 150, 154, 156, 162, 166, 177], "With": [5, 36, 88, 101, 105, 117, 125, 126, 142, 154, 161], "_": [2, 73, 75, 80, 81, 85, 91, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 115, 120, 121, 122, 123, 125, 126, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 145, 146, 151, 153, 157, 158, 161, 162, 163, 164], "__": [150, 151, 153, 154, 179], "__class__": [80, 81], "__init__": 154, "__main__": 154, "__name__": [80, 81], "_california_housing_dataset": [101, 107], "_digits_dataset": 94, "_discret": 116, "_distn_infrastructur": 154, "_distribut": 154, "_fig_": 115, "_i": 91, "_ridg": 133, "ab": [103, 134, 136, 139], "abil": [24, 94, 99, 100, 102, 142, 148, 155], "abl": [84, 85, 97, 102, 103, 105, 110, 111, 115, 116, 119, 120, 130, 136, 142, 153, 157, 160, 162, 164, 171, 182, 185], "abnorml": [90, 104], "about": [2, 12, 16, 21, 23, 28, 32, 35, 38, 55, 59, 63, 70, 71, 73, 76, 80, 81, 84, 93, 94, 95, 98, 99, 101, 102, 104, 106, 108, 118, 131, 132, 137, 141, 145, 158, 162, 170, 179, 182, 185], "abov": [2, 5, 17, 28, 46, 72, 73, 77, 80, 81, 82, 90, 96, 99, 100, 101, 102, 103, 107, 108, 110, 113, 115, 116, 122, 128, 130, 133, 134, 135, 136, 140, 141, 142, 143, 145, 146, 150, 152, 158, 160, 162, 163, 164, 177, 179, 185], "absenc": 102, "absent": 136, "absolut": [27, 28, 46, 91, 95, 101, 102, 111, 112, 113, 115, 116, 120, 121, 122, 129, 130, 134, 135, 136, 138, 139, 144, 145, 147, 156], "absolute_error": 147, "abstract": [2, 105], "acceler": [15, 28, 105, 116], "accept": [2, 84, 101], "access": [2, 14, 28, 35, 46, 57, 77, 82, 92, 97, 100, 101, 108, 110, 112, 121, 131, 133, 137, 141, 150, 185], "accomplish": [92, 97], "accord": [2, 73, 137], "accordingli": 150, "account": [28, 80, 91, 94, 104, 108, 132, 136, 142, 177], "accumul": 5, "accur": [28, 42, 109, 133, 142], "accuraci": [5, 17, 26, 59, 65, 72, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 87, 88, 89, 90, 92, 94, 96, 97, 98, 99, 106, 109, 115, 125, 126, 127, 128, 130, 131, 134, 136, 137, 141, 143, 146, 147, 150, 151, 152, 153, 154, 155, 157, 159, 163, 185], "accuracy_scor": 142, "acdm": [73, 79, 84, 136, 150, 154], "achiev": [5, 17, 73, 78, 81, 83, 95, 97, 106, 117, 132, 133, 161, 163], "acknowledg": 165, "acquir": [5, 12, 21, 32, 38, 55, 102, 105, 170, 182], "acquisit": [5, 102], "acronym": [2, 100], "across": [5, 41, 46, 76, 81, 91, 108, 110, 133, 141, 150, 154, 185], "act": 136, "activ": [28, 68, 136, 153, 179], "actual": [5, 73, 77, 80, 82, 91, 94, 100, 101, 102, 117, 132, 142, 145, 152], "actual_vs_predict": 145, "ad": [2, 17, 39, 46, 52, 92, 95, 97, 108, 113, 117, 119, 122, 139, 150, 154], "adaboost": [9, 12, 115, 165], "adaboostclassifi": 109, "adaboostclassifierifittedadaboostclassifi": 109, "adapt": [9, 99, 118, 139, 145, 154, 165], "add": [59, 105, 108, 115, 116, 125, 129, 133, 135, 139, 140, 142, 143, 146, 160, 164], "addit": [21, 28, 42, 68, 76, 80, 84, 91, 101, 105, 107, 113, 114, 118, 119, 122, 123, 133, 135, 139, 140, 142, 145, 150, 151, 152, 154, 156, 177], "addition": [24, 80, 81, 103, 133], "address": [81, 115], "adeli": [75, 131, 137, 141, 157, 158, 163], "adelie_proba": 157, "adequ": 161, "adjust": [39, 46, 57, 84, 117, 129, 135, 150, 153, 157], "adm": [73, 79, 84, 136, 150, 154], "adopt": [2, 132, 137], "adult": [1, 63, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 93, 98, 119, 130, 136, 148, 150, 151, 152, 154, 155, 165], "adult_censu": [63, 73, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 93, 98, 103, 119, 130, 136, 148, 150, 151, 152, 153, 154, 155], "adult_census_test": [80, 82], "advanc": [2, 5, 35, 70, 87, 89], "advantag": [34, 76, 125, 132], "adventur": 5, "advertis": 5, "advis": [5, 108], "advocaci": 5, "affair": 5, "affect": [5, 133, 137, 141, 150, 157, 159, 162, 163], "affirm": 59, "afford": 5, "after": [2, 17, 27, 46, 76, 84, 101, 104, 110, 113, 116, 117, 120, 122, 138, 151, 155, 156], "afterward": [73, 80, 108], "ag": [2, 73, 76, 78, 79, 80, 81, 83, 84, 85, 101, 103, 107, 108, 130, 133, 136, 150, 151, 152, 154], "again": [17, 92, 97, 103, 108, 110, 113, 122, 137, 139, 142, 153], "against": 145, "age_limit": 73, "aggreg": [76, 107, 114, 123, 133, 142], "agnost": 119, "ago": 142, "agre": [80, 122, 141], "ahead": 133, "ai": 5, "aim": [2, 35, 79, 92, 97, 100, 101, 109, 111, 112, 113, 114, 118, 120, 121, 122, 123, 124, 125, 127, 128, 129, 134, 135, 140, 142, 143, 146, 159, 160, 163, 164], "air": 28, "algorithm": [2, 12, 15, 17, 23, 34, 73, 80, 81, 84, 91, 94, 108, 109, 113, 115, 116, 117, 119, 122, 124, 125, 127, 129, 135, 139, 141, 157, 158], "alia": 27, "alien": 125, "align": [28, 103, 106, 132, 137, 139, 154], "all": [2, 14, 15, 16, 17, 23, 25, 26, 27, 28, 36, 41, 42, 43, 46, 48, 50, 52, 59, 60, 63, 65, 68, 72, 76, 77, 79, 80, 81, 82, 84, 87, 89, 91, 93, 94, 95, 96, 98, 99, 100, 101, 102, 104, 105, 106, 107, 108, 109, 110, 115, 116, 118, 119, 125, 130, 132, 133, 134, 136, 137, 139, 140, 142, 145, 147, 149, 150, 151, 152, 153, 156, 157, 160, 162, 163, 164, 172, 173, 175, 177, 179, 181, 185], "all_column": 73, "all_error": 91, "all_preprocessor": 185, "all_scor": [94, 96], "all_test_scor": 98, "allei": [90, 104], "allevi": 154, "alloc": 5, "allow": [12, 13, 28, 43, 65, 73, 76, 79, 81, 84, 87, 89, 90, 92, 97, 101, 104, 107, 108, 115, 124, 127, 128, 133, 134, 139, 140, 141, 150, 152, 154, 157, 161, 162, 170, 179, 182, 183], "allpub": [90, 104], "almost": [17, 46, 72, 80, 89, 94, 95, 101, 108, 137, 140, 145, 152, 163, 177], "alon": [95, 132], "along": [2, 73, 76, 84, 86, 88, 100, 104, 128, 134, 145, 157], "alpaydin": 94, "alpha": [28, 43, 46, 73, 94, 97, 107, 108, 109, 110, 115, 121, 131, 132, 133, 134, 137, 138, 139, 140, 141, 145, 157, 161, 162, 163, 164], "alpha_": [46, 133], "alphabet": 84, "alreadi": [63, 73, 79, 80, 81, 85, 97, 108, 117, 119, 132, 133, 149, 154, 156], "also": [2, 12, 21, 24, 26, 28, 32, 35, 43, 57, 59, 73, 79, 80, 81, 84, 85, 86, 87, 88, 89, 92, 95, 96, 97, 101, 102, 103, 104, 105, 108, 109, 110, 113, 114, 116, 117, 119, 122, 123, 125, 130, 132, 133, 136, 137, 139, 140, 141, 142, 145, 151, 152, 153, 154, 156, 157], "altern": [73, 84, 90, 94, 100, 129, 135, 139, 141, 154, 161], "although": [2, 185], "altogeth": 117, "alwai": [5, 14, 17, 21, 23, 41, 43, 52, 73, 78, 79, 80, 83, 85, 87, 89, 91, 94, 95, 96, 97, 98, 99, 100, 101, 102, 106, 107, 119, 125, 142, 151, 152, 153, 164, 177, 179, 185], "am": [1, 133, 144, 145, 147, 165], "ambigu": [143, 146], "ames_h": [46, 72, 90, 104, 133, 144, 145, 147, 177], "ames_housing_no_miss": [46, 72, 104, 133, 177], "ames_housing_preprocess": 104, "among": [72, 84, 85, 99, 102, 125, 156], "amount": [76, 101, 106, 119, 133], "amp": 84, "amplifi": 5, "an": [2, 15, 21, 22, 25, 27, 34, 35, 39, 41, 42, 46, 48, 50, 52, 55, 57, 68, 70, 71, 72, 73, 76, 80, 81, 83, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 99, 100, 101, 105, 106, 107, 108, 110, 113, 115, 116, 117, 118, 119, 122, 124, 125, 127, 129, 131, 132, 133, 134, 135, 137, 138, 139, 141, 142, 143, 145, 146, 147, 148, 150, 151, 152, 155, 156, 157, 161, 162, 163, 170, 173, 177, 179, 181, 182, 183, 185], "analys": [28, 79, 108], "analysi": [5, 70, 79, 92, 97, 100, 106, 107, 113, 122, 133, 145, 154, 156, 157, 165, 178], "analyz": [125, 133, 179], "anatom": 158, "andrea": 5, "angel": 107, "angl": 28, "ani": [5, 14, 17, 28, 35, 42, 72, 73, 76, 79, 81, 83, 85, 88, 89, 91, 94, 97, 99, 100, 101, 102, 103, 105, 106, 107, 108, 110, 117, 118, 119, 123, 124, 125, 126, 127, 132, 133, 138, 141, 145, 150, 152, 153, 156, 161, 174, 185], "anim": 185, "annoi": 5, "annot": [73, 150], "annual": 133, "anoth": [2, 5, 17, 57, 72, 73, 76, 96, 98, 100, 110, 119, 125, 126, 128, 133, 134, 139, 142, 145, 161], "anova": [124, 125, 127], "answer": [14, 15, 16, 17, 23, 24, 25, 26, 27, 28, 34, 41, 42, 43, 46, 48, 50, 52, 59, 60, 63, 65, 68, 72, 84, 104, 129, 135, 137, 156, 172, 173, 174, 175, 177, 179, 181, 185], "anti": 103, "anymor": [95, 98, 100, 101, 117], "anyth": [88, 100, 124, 127, 142], "anywher": 137, "ap": 142, "apart": [48, 100, 102, 108], "api": [0, 5, 71, 77, 80, 82, 85, 142], "appear": [83, 94, 102, 110], "append": [96, 99, 108, 110, 121, 123, 137, 147], "appendix": [28, 91, 92, 95, 97, 101, 102, 109, 111, 112, 113, 116, 118, 119, 120, 121, 122, 128, 129, 131, 133, 134, 135, 137, 138, 140, 141, 142, 143, 144, 145, 146, 147, 157, 159, 160, 161, 162, 163, 164], "appli": [2, 5, 14, 15, 16, 17, 23, 25, 26, 28, 38, 42, 43, 46, 48, 50, 52, 59, 60, 63, 65, 68, 72, 81, 84, 85, 90, 99, 102, 104, 105, 107, 132, 140, 147, 150, 151, 153, 156, 161, 163, 172, 173, 175, 179, 181, 185], "applic": [2, 5, 80, 94, 100, 142, 145, 161], "appreci": 110, "apprehend": 152, "approach": [2, 5, 13, 22, 33, 39, 46, 57, 71, 72, 84, 99, 103, 116, 132, 138, 141, 150, 152, 154, 156, 171, 183], "appropri": [21, 73, 103, 163], "approxim": [39, 59, 76, 80, 81, 89, 91, 94, 98, 110, 122, 132, 133, 135, 137, 156, 185], "ar": [2, 12, 13, 14, 15, 16, 17, 21, 22, 24, 26, 27, 28, 32, 33, 35, 36, 38, 39, 42, 43, 46, 48, 52, 55, 57, 59, 65, 68, 70, 71, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 84, 85, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 115, 116, 117, 118, 119, 120, 123, 125, 126, 127, 129, 130, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 145, 147, 149, 150, 151, 152, 153, 154, 155, 156, 157, 161, 162, 163, 164, 170, 171, 173, 177, 179, 181, 182, 183, 185], "arang": [103, 105, 107, 108, 110, 136, 161, 162, 164], "arbitrari": [73, 84, 85, 86, 88, 89, 90, 108, 140], "arc": 28, "archiv": [35, 90, 94], "arctan": 28, "area": [101, 102, 104, 137, 142], "arg": 154, "argsort": [108, 125], "argu": 98, "argument": [2, 46, 59, 77, 81, 82, 84, 86, 88, 105, 185], "aris": [55, 73, 100], "arithmet": 84, "around": [12, 21, 32, 38, 73, 79, 90, 94, 97, 100, 101, 102, 105, 108, 135, 137, 142, 145, 170], "arrai": [17, 28, 41, 42, 59, 65, 76, 79, 80, 81, 82, 84, 85, 94, 95, 101, 102, 103, 108, 109, 110, 113, 116, 121, 122, 129, 133, 135, 136, 141, 142, 146, 150, 157], "arrow": [7, 8, 45, 47, 49, 51, 53, 61, 66, 166], "art": 13, "articl": [2, 5, 57, 110], "artifici": 35, "as_fram": [91, 95, 99, 101, 102, 107, 108, 111, 113, 114, 115, 116, 117, 118, 120, 122, 123, 149, 156], "ascend": [100, 150, 154], "asian": 136, "ask": [5, 124, 127, 128, 131, 134, 137, 142], "aspect": [2, 5, 21, 33, 76, 81, 95, 126, 133, 172], "assembl": [86, 88, 115, 117], "assert": [108, 117, 185], "assess": [5, 24, 46, 76, 80, 93, 94, 95, 96, 98, 102, 113, 122, 128, 133, 134, 142, 145, 152, 156], "assign": [73, 79, 81, 84, 98, 104, 109, 115, 132, 139, 141, 144, 147], "assoc": [73, 79, 84, 136, 150, 154], "associ": [85, 94, 105, 107, 133, 142, 145, 150], "assum": [26, 28, 42, 43, 52, 68, 88, 99, 100, 108, 133, 139, 141, 177], "assume_a": 133, "assumpt": [81, 84, 88, 100, 126, 161, 162], "astyp": [72, 90, 104, 132, 149, 154, 156, 161], "asymmetr": 161, "asymmetri": 161, "asymptot": 141, "attempt": [132, 133, 137], "attent": [70, 109], "attract": 183, "attribut": [5, 36, 73, 81, 84, 94, 96, 101, 107, 108, 110, 112, 121, 131, 137, 138, 139, 141, 149, 150, 154, 156, 179, 185], "auc": [26, 142], "augment": [28, 132, 133], "august": 105, "aur\u00e9lien": 5, "author": 177, "autom": [5, 73, 165, 183], "automat": [17, 65, 73, 79, 81, 84, 86, 88, 90, 101, 151, 152, 177, 182], "autoregress": [101, 107], "avail": [2, 5, 28, 35, 48, 72, 73, 74, 75, 76, 80, 90, 91, 94, 95, 104, 106, 107, 112, 121, 125, 127, 133, 138, 140, 142, 151, 154, 177], "avebedrm": [101, 107, 108], "aveoccup": [101, 107, 108], "averag": [17, 28, 59, 80, 91, 94, 98, 99, 101, 105, 107, 108, 110, 115, 116, 117, 118, 119, 122, 125, 133, 137, 138, 142, 145, 147, 150, 185], "averoom": [101, 107, 108, 156], "avoid": [2, 5, 39, 46, 72, 73, 96, 102, 108, 109, 110, 113, 115, 121, 122, 127, 129, 130, 135, 136, 137, 138, 139, 150, 153, 154, 156], "awai": [86, 88, 89, 137, 145, 165], "awar": [28, 32, 33, 73, 80, 84, 85, 86, 88, 90, 94, 102, 104, 117, 136, 151, 177], "ax": [73, 81, 103, 104, 105, 108, 109, 115, 119, 131, 132, 133, 134, 136, 137, 139, 140, 142, 145, 150, 153, 157, 161, 162, 163], "ax1": 103, "ax2": 103, "ax_": [95, 97, 102, 122, 131, 135, 137, 142], "axhlin": 73, "axi": [2, 17, 28, 75, 91, 98, 99, 103, 104, 107, 108, 110, 123, 125, 126, 132, 133, 137, 139, 140, 142, 145, 150, 153, 154, 156, 157, 161, 179], "axvlin": [73, 107, 108], "b": [2, 14, 15, 16, 17, 23, 24, 25, 26, 27, 28, 34, 36, 41, 42, 43, 46, 48, 50, 52, 59, 60, 63, 65, 68, 72, 128, 134, 140, 154, 156, 172, 173, 174, 175, 177, 179, 181, 185], "bachelor": [73, 84, 85, 136], "back": [141, 142, 153, 157], "background": 35, "backyard": 48, "bad": [5, 88, 97, 127, 133, 142, 179], "badli": 179, "bag": [10, 12, 14, 15, 109, 111, 115, 117, 118, 119, 120, 165], "bag_of_tre": 110, "bag_predict": 110, "bagged_tre": [110, 119], "bagged_trees_predict": 110, "bagging_predict": 110, "bagging_regressor": 118, "baggingclassifi": [14, 119], "baggingregressor": [14, 110, 111, 118, 119, 120], "balanc": [26, 59, 102, 117, 133, 142, 143, 145, 146, 185], "balanced_accuraci": [59, 142, 143, 146, 185], "balanced_accuracy_scor": 142, "banana": 145, "band": 153, "bar": [76, 91, 99, 104, 108, 133, 141, 157], "barh": [104, 106, 108, 137, 141, 142], "barplot": 141, "barri": [101, 107], "base": [5, 12, 13, 14, 15, 17, 28, 33, 35, 41, 48, 59, 73, 74, 75, 81, 87, 89, 94, 101, 102, 103, 105, 110, 115, 118, 119, 125, 126, 129, 130, 135, 136, 137, 139, 141, 142, 147, 150, 152, 157, 158, 165, 185], "base_estim": 14, "base_model_lin": 110, "baselin": [5, 21, 23, 78, 83, 85, 86, 88, 93, 97, 98, 118, 165], "baseline_score_train": 108, "basi": 132, "basic": [12, 21, 32, 35, 38, 49, 55, 70, 99, 101, 108, 120, 145, 170, 182], "bathroom": 48, "bay": 95, "bbox_to_anchor": [91, 94, 98, 99, 100, 105, 107, 109, 110, 115, 121, 131, 134, 136, 137, 142, 153, 157, 161, 163], "beat": [28, 105], "becam": 84, "becaus": [2, 5, 17, 72, 73, 76, 77, 80, 81, 82, 84, 85, 86, 88, 91, 98, 99, 100, 101, 102, 108, 110, 119, 122, 125, 132, 133, 139, 143, 145, 146, 147, 149, 150, 151, 152, 154, 156, 158, 162], "becom": [17, 92, 95, 97, 102, 108, 116, 117, 150, 154, 163, 179, 183], "bedroom": [48, 101, 107, 108], "bedroomabvgr": [46, 72, 104, 177], "been": [2, 73, 76, 79, 84, 91, 99, 101, 108, 109, 116, 122, 125, 135, 141, 151, 152, 157], "befor": [2, 43, 46, 73, 77, 79, 81, 82, 84, 99, 101, 103, 104, 108, 116, 119, 124, 127, 133, 140, 142, 143, 145, 146], "beforehand": 133, "begin": [17, 80, 113, 122, 132, 133, 152], "beginn": 35, "behav": [17, 92, 97, 98, 131, 133, 137, 177], "behavior": [78, 83, 131, 133, 137, 162], "behaviour": [81, 100, 126], "behind": [2, 5, 12, 21, 118], "being": [5, 41, 84, 91, 141, 156], "belong": [12, 17, 46, 79, 83, 84, 94, 97, 106, 141, 147, 157, 163, 177], "below": [2, 17, 46, 59, 76, 83, 84, 90, 91, 102, 107, 108, 116, 122, 128, 134, 140, 142, 145, 154, 156, 158, 160, 163, 164, 177, 179, 181, 185], "benchmark": 116, "benefici": [5, 32, 81, 117, 125, 152, 154, 161], "benefit": [5, 16, 23, 32, 81, 84, 95, 115, 118, 119, 133, 145, 165], "besid": [21, 22, 32, 38, 79, 85, 87, 89, 94, 95, 102, 114, 123, 125, 157, 170], "best": [2, 12, 17, 21, 32, 38, 52, 57, 59, 73, 90, 96, 97, 99, 102, 111, 113, 114, 116, 117, 119, 120, 122, 123, 128, 133, 134, 137, 138, 139, 142, 145, 148, 149, 150, 151, 152, 154, 155, 156, 157, 170, 179, 182, 183, 185], "best_alpha": 133, "best_estimator_": [96, 123, 150, 152, 154], "best_lr": 155, "best_mln": 155, "best_param": [150, 155], "best_params_": [96, 123, 149, 150, 152, 154, 156, 161, 179, 185], "best_scor": 155, "best_score_": 96, "beta": 28, "beta_": 28, "better": [5, 16, 17, 27, 46, 72, 76, 80, 83, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 102, 110, 117, 118, 119, 124, 127, 128, 130, 133, 134, 136, 137, 138, 143, 146, 147, 149, 150, 151, 152, 154, 156, 161, 177, 185], "between": [2, 5, 12, 14, 17, 21, 28, 39, 46, 52, 55, 57, 59, 73, 75, 76, 79, 81, 84, 92, 94, 96, 97, 100, 102, 103, 105, 106, 108, 112, 115, 117, 121, 124, 125, 127, 129, 131, 132, 133, 134, 135, 137, 139, 140, 141, 142, 145, 153, 154, 156, 158, 161, 162, 170, 185], "beyond": [73, 87, 89, 103, 139, 141, 145, 147], "bia": [5, 36, 50, 55, 57, 108, 129, 130, 132, 135, 136, 145, 165], "bias": [24, 132], "bicycl": 28, "big": [107, 145], "biggest": 5, "bike": [1, 28, 165], "bike_rid": [28, 105], "bin": [15, 73, 81, 91, 94, 98, 101, 102, 104, 105, 106, 107, 116, 132, 139, 153, 154], "bin_var": 108, "binari": [2, 5, 41, 59, 73, 90, 98, 141, 142, 172, 185], "binder": 35, "binned_regress": 139, "bioinformat": [124, 127], "bit": [5, 28, 83, 101, 102, 108, 133, 139, 143, 146, 152], "bitmap": 94, "black": [2, 73, 79, 84, 85, 91, 94, 96, 98, 101, 102, 104, 105, 106, 107, 109, 110, 115, 121, 123, 125, 126, 133, 134, 136, 138, 139, 140, 146, 150, 154, 161, 162, 164], "bldgtype": 104, "blend": [105, 132], "blender": 105, "blindli": 28, "blob": [137, 161], "block": [94, 100, 101, 107, 108, 116, 143, 146], "blood": [1, 92, 97, 142, 143, 146, 165], "blood_transfus": [59, 92, 97, 106, 142, 143, 146], "blue": [2, 73, 76, 94, 101, 103, 109, 110, 131, 132, 136, 137, 141, 142, 150, 152, 157, 161, 163], "bodi": [17, 74, 75, 105, 112, 121, 128, 129, 134, 135, 138, 140, 158, 160, 161, 162, 164, 185], "body_mass": [128, 134, 140], "body_mass_180": 140, "body_mass_181": 140, "bogazici": 94, "boil": 2, "boilerpl": 127, "bool": [80, 104, 142], "boolean": 156, "boost": [5, 12, 13, 15, 16, 17, 28, 85, 113, 114, 118, 122, 123, 150, 154, 165], "boosting_round": 109, "bootstrap": [12, 13, 17, 117, 118, 119, 120, 165], "bootstrap_featur": 120, "bootstrap_idx": 110, "bootstrap_indic": 110, "bootstrap_sampl": 110, "border": 94, "both": [2, 14, 15, 17, 21, 28, 38, 39, 46, 52, 63, 72, 73, 75, 80, 81, 84, 85, 86, 87, 88, 89, 93, 94, 95, 96, 97, 98, 99, 100, 102, 104, 108, 109, 113, 115, 116, 119, 122, 124, 125, 127, 128, 130, 132, 133, 134, 136, 137, 139, 141, 142, 143, 145, 146, 150, 152, 156, 157, 158, 160, 161, 164, 170, 171, 177, 179, 185], "bottom": [2, 73, 142, 163], "boun": 94, "bound": [94, 142, 145], "boundari": [41, 73, 132, 141, 157, 159, 161, 162, 163], "box": [46, 96, 107, 114, 123, 125, 126, 133, 136, 143, 146, 185], "boxplot": [108, 130, 136], "brain": 5, "brake": 28, "branch": [117, 161], "break": [17, 94, 100, 133], "breakout": 105, "breast": 96, "briefli": 5, "bring": 35, "brittl": 5, "broad": 2, "broader": [5, 160, 164], "brought": 108, "brows": 2, "brute": [116, 138], "bsmtcond": 104, "bsmtexposur": 104, "bsmtfinsf1": [46, 72, 104, 177], "bsmtfinsf2": [46, 72, 104, 177], "bsmtfintype1": 104, "bsmtfintype2": 104, "bsmtfullbath": 104, "bsmthalfbath": 104, "bsmtqual": 104, "bsmtunfsf": [46, 72, 104, 177], "budget": [179, 183], "build": [5, 17, 28, 46, 70, 72, 73, 80, 90, 92, 97, 110, 115, 116, 126, 129, 132, 135, 150, 161, 165, 167], "built": [5, 36, 93, 98, 108, 110, 139, 142, 161, 163, 173], "bureau": [101, 107], "busi": 35, "buyer": 48, "bypass": 84, "c": [5, 14, 15, 16, 17, 23, 24, 25, 26, 27, 28, 34, 36, 41, 42, 43, 46, 48, 50, 52, 59, 60, 63, 65, 68, 72, 94, 96, 130, 132, 136, 141, 151, 163, 172, 173, 174, 175, 177, 179, 181, 185], "c_r": 28, "c_x": 28, "cach": 3, "cadenc": [28, 105], "cal_hous": [101, 107], "calcul": [108, 109, 142], "calibr": 105, "california": [1, 91, 101, 104, 108, 111, 113, 114, 115, 116, 117, 118, 120, 122, 123, 165], "california_h": 107, "call": [2, 12, 13, 28, 43, 57, 59, 65, 68, 72, 73, 79, 80, 81, 84, 85, 86, 88, 91, 92, 95, 97, 98, 101, 102, 106, 109, 115, 116, 117, 118, 119, 127, 129, 132, 133, 135, 138, 139, 140, 141, 142, 143, 145, 146, 147, 148, 152, 154, 155, 163, 182], "cambodia": 84, "can": [2, 5, 13, 14, 17, 21, 22, 27, 28, 33, 35, 38, 39, 43, 46, 52, 55, 57, 59, 65, 68, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 125, 126, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 156, 157, 158, 159, 160, 161, 162, 163, 164, 171, 172, 174, 177, 179, 182, 183, 185], "canada": 84, "cancer": 96, "candela": 94, "candid": [131, 137, 154, 156, 157], "cannot": [5, 17, 28, 57, 73, 91, 99, 100, 102, 108, 117, 124, 127, 132, 139, 142, 145, 147, 150, 153, 164, 182], "cap": [73, 96, 107, 123, 125, 126, 133, 136, 146], "capabl": [94, 110, 115, 133, 142, 160, 162, 164], "capac": [28, 95], "capit": [2, 73, 76, 78, 79, 80, 81, 83, 84, 85, 103, 130, 136, 150, 151, 152, 154], "captur": [57, 102, 108, 132, 133], "cardin": [36, 84, 87, 89, 108], "care": [5, 33, 46, 73, 84, 96, 99, 106, 107, 124, 127, 133], "carefulli": [5, 94], "carelessli": 100, "carpentri": 5, "carri": [12, 21, 32, 38, 46, 55, 70, 84, 95, 107, 126, 150, 170, 182], "cascad": 94, "case": [2, 28, 32, 48, 73, 76, 79, 81, 84, 85, 87, 89, 90, 91, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 108, 115, 117, 119, 124, 127, 129, 130, 132, 133, 135, 136, 137, 140, 141, 142, 143, 145, 146, 147, 149, 150, 151, 152, 154, 156, 157, 183], "cat": [2, 90], "catastroph": [28, 81], "catch": [143, 146], "categor": [5, 36, 46, 63, 68, 70, 71, 72, 73, 74, 75, 86, 88, 102, 103, 104, 119, 130, 132, 133, 136, 141, 142, 150, 158, 165, 177], "categori": [2, 68, 72, 73, 79, 85, 86, 88, 104, 106, 119, 130, 133, 136, 150, 158], "categorical_column": [73, 84, 85, 86, 87, 88, 89, 130, 136, 150, 152, 154], "categorical_columns_selector": [84, 85, 86, 87, 88, 89, 150, 152, 154], "categorical_encod": 119, "categorical_featur": [46, 90, 104], "categorical_preprocessor": [85, 87, 89, 148, 150, 152, 154, 155], "categorical_transform": 90, "categories_": 84, "caus": [5, 17, 50, 57, 73, 84, 88, 89, 117, 133, 150, 152, 179], "causal": 136, "caution": [5, 87, 89, 108], "caveat": [32, 124, 127, 152, 165], "cax": 163, "cbar_kw": 150, "cc": [0, 35], "cc0": 0, "ceil": 104, "cell": [35, 73, 77, 79, 81, 82, 85, 90, 96, 101, 109, 115, 116, 132, 133, 136, 138, 139, 142, 150, 152, 154, 157, 163, 164], "censu": [1, 63, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 93, 98, 101, 107, 119, 130, 136, 148, 150, 151, 152, 154, 155, 165], "center": [94, 109, 132, 134, 153, 156, 161, 163], "central": 35, "centralair": 104, "certain": [94, 103, 123, 163], "certainli": [113, 122], "certainti": [137, 163], "certif": 5, "chain": [5, 76, 81, 85], "challeng": [5, 35, 106, 139], "chanc": [72, 98, 101, 110, 124, 127, 142], "chance_level_kw": 142, "chang": [5, 27, 28, 36, 46, 81, 90, 93, 98, 102, 108, 109, 113, 122, 125, 133, 145, 148, 151, 155, 157, 182, 185], "channel": 35, "chapter": [57, 185], "charact": 104, "character": [55, 108, 142], "characterist": [50, 94, 101, 107, 142], "charg": 150, "cheap": [28, 48, 85], "cheaper": [105, 108], "cheatsheet": 2, "check": [2, 5, 28, 46, 59, 73, 76, 79, 80, 81, 83, 84, 85, 92, 94, 95, 97, 99, 100, 101, 102, 103, 105, 107, 109, 110, 111, 112, 114, 115, 116, 118, 119, 120, 121, 123, 124, 125, 126, 127, 129, 133, 135, 138, 139, 140, 142, 143, 146, 150, 152, 153, 157, 158, 160, 161, 162, 164, 170, 177], "cherri": 90, "chevron": 100, "child": [73, 79, 84, 85, 136, 150, 154], "children": 172, "china": 84, "chinstrap": [75, 131, 137, 141, 157, 158], "chinstrap_proba": 157, "choic": [2, 28, 35, 43, 52, 55, 57, 84, 97, 98, 101, 105, 107, 108, 110, 133, 138, 147, 150, 154, 156, 165, 185], "choleski": 133, "choos": [5, 17, 35, 73, 86, 88, 101, 108, 124, 125, 127, 132, 133, 139, 141, 152, 175, 185], "chose": [5, 52, 73, 115, 133], "chosen": [23, 84, 88, 91, 106, 113, 122, 145, 183], "circl": [2, 73, 107, 110, 132], "circular": 179, "cite": 35, "citi": [107, 145], "civ": [73, 79, 84, 85, 136, 150, 154], "cla": 154, "clariti": 117, "class": [2, 14, 24, 26, 41, 59, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 91, 92, 93, 94, 97, 98, 99, 103, 104, 105, 106, 107, 108, 109, 110, 116, 119, 130, 131, 132, 133, 136, 137, 139, 140, 141, 143, 146, 147, 148, 150, 151, 152, 154, 155, 157, 159, 161, 173, 185], "class_nam": [157, 163], "class_of_interest": 163, "class_sep": 126, "class_to_predict": 83, "classes_": [103, 141, 142, 157, 163, 181], "classic": 28, "classif": [5, 14, 21, 37, 38, 41, 48, 59, 63, 72, 73, 79, 80, 86, 88, 90, 92, 93, 94, 97, 98, 101, 106, 109, 129, 130, 132, 135, 136, 140, 143, 144, 145, 146, 147, 161, 162, 163, 165, 170, 171, 172, 185], "classifi": [5, 14, 23, 26, 41, 68, 74, 75, 78, 79, 80, 81, 83, 84, 85, 86, 87, 88, 89, 90, 92, 93, 94, 97, 98, 99, 106, 109, 119, 125, 130, 131, 132, 136, 137, 141, 143, 146, 147, 148, 150, 151, 152, 154, 155, 157, 159, 161, 163, 171, 172, 179, 181, 185], "classifier_": 150, "classifier__": 150, "classifier__c": [151, 179, 181], "classifier__class_weight": 151, "classifier__du": 151, "classifier__fit_intercept": 151, "classifier__intercept_sc": 151, "classifier__l1_ratio": 151, "classifier__l2_regular": [153, 154], "classifier__learning_r": [150, 152, 154, 155], "classifier__max_bin": 154, "classifier__max_it": 151, "classifier__max_leaf_nod": [150, 152, 154, 155], "classifier__min_samples_leaf": 154, "classifier__multi_class": 151, "classifier__n_job": 151, "classifier__n_neighbor": 185, "classifier__penalti": 151, "classifier__random_st": 151, "classifier__solv": 151, "classifier__tol": 151, "classifier__verbos": 151, "classifier__warm_start": 151, "claudio": 94, "cleaner": [85, 136], "clear": [2, 73, 97, 102, 161], "clearer": 81, "clearli": [59, 83, 95, 97], "cleric": [73, 79, 84, 136, 150, 154], "clf": 2, "click": [7, 8, 14, 45, 47, 49, 51, 53, 61, 66, 103, 153, 156, 166, 179], "client": 2, "clip": 28, "clone": [76, 110], "close": [0, 2, 28, 43, 48, 73, 81, 96, 99, 100, 101, 107, 108, 115, 122, 123, 132, 133, 137, 138, 139, 141, 145, 150, 152, 154, 157], "closer": [28, 43, 99, 105, 131, 133, 137, 141], "closest": [28, 80, 132], "cloud": 145, "clue": 145, "cluster": [2, 101, 103, 172], "cm": 163, "cmap": [103, 109, 131, 132, 137, 141, 150, 157, 161, 163], "cm\u00b3": [106, 142], "co": 28, "coal": 105, "coars": 90, "coast": 107, "code": [5, 28, 35, 48, 68, 74, 77, 78, 86, 92, 93, 103, 110, 111, 112, 113, 114, 115, 116, 124, 127, 128, 129, 130, 131, 133, 134, 139, 141, 143, 144, 146, 148, 149, 150, 152, 153, 155, 156, 159, 160, 179], "coef": [107, 108, 133, 136, 137, 141], "coef0": [132, 141], "coef1": 141, "coef_": [41, 42, 46, 107, 108, 126, 131, 133, 136, 137, 138, 139, 141, 151, 182], "coeffici": [28, 36, 39, 42, 46, 107, 130, 131, 133, 135, 136, 137, 139, 140, 141, 145, 172], "coincid": [123, 142], "col": [103, 116], "col_idx": 108, "collaps": 84, "collect": [2, 5, 28, 73, 76, 93, 98, 101, 102, 103, 118, 125, 147], "collector": 102, "colleg": [73, 79, 84, 85, 136, 150, 154], "color": [2, 73, 90, 96, 103, 107, 108, 109, 110, 115, 121, 123, 125, 126, 131, 132, 133, 134, 136, 137, 138, 139, 140, 141, 142, 146, 150, 153, 156, 157, 161, 162, 163, 164, 179], "color_continuous_scal": [153, 156, 179], "colorbar": 163, "colormap": [103, 131, 132, 137, 141, 163], "colorscal": 103, "columbia": 84, "column": [2, 5, 17, 28, 36, 41, 42, 46, 59, 63, 68, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 92, 93, 97, 98, 100, 101, 103, 104, 105, 106, 107, 108, 110, 115, 117, 119, 120, 121, 123, 125, 126, 129, 130, 132, 133, 135, 136, 139, 140, 142, 143, 144, 145, 146, 147, 148, 150, 151, 152, 153, 154, 155, 156, 158, 161, 162, 164, 177, 185], "column_name_map": 156, "column_result": [150, 154], "column_scal": 156, "columns_drop": 107, "columns_to_plot": 103, "columntransfom": 85, "columntransform": [71, 85, 90, 132, 136, 150, 152, 154], "columntransformercolumntransform": [85, 90, 136, 150, 152, 154], "com": [2, 5, 35], "combin": [5, 12, 13, 15, 41, 42, 65, 73, 79, 81, 85, 94, 96, 100, 106, 107, 108, 109, 110, 114, 115, 116, 117, 118, 119, 123, 126, 132, 133, 135, 138, 139, 147, 148, 149, 150, 152, 154, 155, 156, 157, 172, 173, 179, 182, 183], "come": [2, 5, 28, 55, 72, 73, 79, 91, 99, 115, 118, 125, 128, 134, 140, 150, 151, 152, 162], "comma": [63, 73, 104, 105, 106], "command": [28, 46, 59, 72, 177, 185], "comment": 73, "commerc": 2, "commit": 145, "common": [2, 57, 76, 84, 100, 102, 110, 133, 136, 145, 163, 185], "common_scatter_plot_param": 132, "commonli": [2, 73, 79, 80], "compani": [68, 100], "compar": [5, 12, 16, 17, 21, 28, 36, 46, 48, 58, 59, 72, 73, 76, 78, 80, 81, 83, 84, 85, 86, 88, 94, 96, 97, 98, 102, 107, 108, 110, 113, 115, 116, 120, 122, 125, 126, 127, 129, 130, 133, 135, 136, 142, 145, 150, 154, 156, 157, 165, 177, 185], "comparison": [22, 39, 96, 118, 142], "compat": [76, 152], "complet": [5, 14, 28, 35, 46, 96, 98, 124, 127, 132, 133, 149, 152, 156, 179], "complex": [28, 43, 60, 79, 85, 86, 88, 90, 104, 115, 137, 143, 146, 161, 162], "complic": [73, 107, 125], "compon": [73, 129, 135, 139, 157, 181], "compos": [2, 72, 73, 80, 84, 85, 86, 87, 88, 89, 90, 93, 98, 104, 119, 130, 136, 145, 148, 150, 152, 154, 155, 161], "compound": 85, "compris": [5, 142], "compromis": [102, 142], "comput": [2, 5, 28, 46, 59, 65, 73, 76, 77, 80, 81, 82, 84, 85, 87, 89, 91, 92, 93, 95, 96, 97, 98, 99, 101, 102, 108, 109, 110, 113, 115, 116, 117, 118, 122, 124, 125, 127, 129, 132, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 149, 150, 152, 153, 154, 156, 157, 159, 162, 163, 179], "computation": [76, 116, 139, 150, 179, 183], "concat": [91, 98, 99, 108, 123, 125, 126, 137], "concaten": [85, 93, 94, 98, 99, 132, 139, 150, 152, 161], "concentr": 132, "concept": [5, 12, 13, 21, 22, 32, 33, 38, 39, 55, 57, 71, 94, 99, 101, 147, 170, 171, 183], "concern": [130, 136, 152], "conclud": [80, 86, 88, 98, 110, 137, 157], "conclus": [5, 73, 91, 93, 94, 96, 98, 100, 102, 103, 106, 117, 125, 136], "concret": [2, 28, 57], "condit": [2, 5, 108, 133, 141], "condition1": 104, "condition2": 104, "conduct": [113, 122, 156], "confid": [43, 131, 132, 137, 141, 142], "configur": [2, 81, 114, 123, 139, 150], "confirm": [5, 91, 99, 105, 107, 110, 124, 127, 130, 132, 136, 137, 141, 145, 152, 153], "confus": [2, 115, 151, 162, 182], "confusionmatrixdisplai": 142, "congratul": 5, "conjunct": 137, "connect": 2, "conocophillip": 100, "consecut": 154, "consequ": [28, 99, 110, 116, 119], "consid": [17, 42, 59, 68, 72, 84, 100, 102, 104, 107, 108, 109, 116, 117, 119, 132, 133, 139, 143, 146, 157, 163, 172, 181, 185], "consider": [28, 94, 117, 140], "consist": [2, 39, 46, 72, 76, 80, 94, 101, 129, 130, 135, 136, 139, 147], "constant": [2, 23, 83, 108, 129, 132, 135, 137, 139, 162, 174], "constantli": [88, 93, 98], "constrain": [2, 39, 43, 60, 102, 132], "constrained_layout": 132, "constraint": [117, 133, 161], "construct": [68, 119, 154], "constructor": 84, "contain": [2, 17, 28, 42, 46, 59, 63, 65, 72, 73, 76, 79, 80, 84, 85, 86, 88, 94, 99, 102, 103, 104, 105, 106, 107, 110, 112, 119, 121, 124, 126, 127, 133, 139, 140, 141, 142, 150, 154, 160, 162, 164, 177, 185], "content": [2, 79, 85, 104, 147, 150], "context": [2, 5, 14, 141, 142], "continu": [2, 5, 35, 36, 41, 42, 63, 73, 79, 101, 103, 104, 108, 110, 140, 141, 142, 145, 147, 158, 161], "contour": [131, 132, 137], "contrari": [2, 84, 101, 108, 162], "contrast": [55, 73, 84, 101, 140, 143, 146, 162], "contribut": [5, 59, 81, 94, 117, 133, 156, 179], "control": [5, 43, 77, 81, 82, 92, 96, 97, 102, 117, 119, 125, 131, 132, 137, 139, 148, 149, 151, 153, 155, 156, 157, 161, 172, 182], "conveni": [77, 82, 133], "convent": [2, 73, 80, 81, 139], "converg": [2, 65, 81, 84], "convergencewarn": 84, "convert": [72, 101, 103, 109, 110, 121, 156], "cook": 133, "cookbook": 35, "coolwarm": 103, "coordin": [103, 141, 152, 153, 156, 179], "cop": 100, "copi": [94, 101, 105, 108, 128, 130, 134, 136, 179], "core": [104, 105, 106, 107, 115, 116], "corner": [104, 142], "corr": 103, "correct": [15, 16, 17, 28, 59, 72, 80, 82, 100, 109, 115, 117, 129, 135, 142, 148, 154, 155, 161, 185], "correctli": [79, 89, 90, 99, 109, 142, 149, 156], "correl": [36, 46, 73, 103, 106, 108, 119, 125, 133, 139], "correspond": [2, 17, 28, 46, 73, 76, 79, 84, 85, 86, 88, 91, 94, 96, 101, 102, 104, 105, 106, 107, 108, 110, 112, 121, 135, 140, 141, 142, 150, 151, 153, 154, 162, 174, 177], "cost": [5, 80, 101, 105, 118, 136, 137, 153, 154], "costli": [105, 137, 149, 150, 156], "could": [36, 59, 72, 73, 79, 80, 81, 83, 84, 85, 91, 92, 96, 97, 98, 100, 101, 102, 104, 105, 106, 107, 108, 109, 116, 118, 119, 123, 125, 127, 128, 132, 133, 134, 136, 138, 139, 140, 141, 142, 145, 151, 152, 153, 156, 161], "count": [17, 46, 59, 72, 73, 75, 79, 81, 83, 84, 91, 94, 99, 104, 105, 106, 107, 130, 136, 157, 177, 185], "countri": [73, 79, 84, 85, 136, 150, 152, 154], "country_": [84, 136], "country_infrequent_sklearn": 136, "coupl": [104, 105, 107, 116, 117, 133, 154], "cours": [79, 84, 87, 89, 94, 104, 125, 128, 134, 139, 143, 146, 159, 163], "court": 5, "covari": 73, "cover": [2, 49, 73, 76, 79, 84, 85, 137], "coverag": 5, "cox": 185, "cpu": [76, 85, 102, 115, 118, 150, 154], "craft": 136, "creat": [2, 28, 43, 46, 59, 68, 70, 71, 72, 76, 77, 79, 81, 82, 84, 85, 89, 92, 93, 94, 95, 97, 98, 99, 100, 102, 104, 105, 106, 107, 109, 110, 111, 112, 113, 114, 115, 118, 120, 121, 122, 123, 124, 125, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 139, 141, 142, 143, 144, 146, 147, 150, 151, 152, 154, 157, 159, 160, 162, 163, 164, 173, 177, 182], "creator": 94, "crescent": 132, "criteria": [2, 117], "criterion": [104, 108, 157], "critic": [5, 35, 104], "cross": [5, 12, 17, 21, 22, 23, 24, 25, 28, 32, 33, 38, 39, 43, 46, 55, 56, 58, 59, 64, 65, 72, 84, 86, 87, 88, 89, 91, 92, 93, 94, 97, 98, 99, 100, 102, 103, 107, 108, 114, 115, 116, 117, 118, 119, 122, 123, 124, 125, 126, 127, 129, 130, 133, 135, 136, 142, 143, 144, 146, 147, 148, 150, 151, 152, 153, 154, 155, 156, 161, 165, 170, 175, 177, 179, 182, 183, 185], "cross_val_scor": [27, 59, 94, 96, 100, 101, 119, 127, 143, 144, 146, 147, 148, 150, 155], "cross_valid": [17, 28, 46, 59, 65, 72, 76, 84, 85, 86, 87, 88, 89, 90, 91, 92, 97, 98, 99, 102, 107, 108, 114, 115, 116, 118, 123, 125, 126, 133, 135, 136, 143, 144, 146, 147, 151, 152, 177, 185], "cross_validation_baselin": 3, "cross_validation_ex_01": 3, "cross_validation_ex_02": 3, "cross_validation_group": 3, "cross_validation_learning_curv": 3, "cross_validation_nest": 3, "cross_validation_sol_01": 3, "cross_validation_sol_02": 3, "cross_validation_stratif": 3, "cross_validation_tim": 3, "cross_validation_train_test": 3, "cross_validation_validation_curv": 3, "crosstab": 73, "crucial": [76, 94, 108, 117], "cruis": 105, "csv": [17, 28, 46, 59, 63, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 93, 97, 98, 100, 103, 104, 105, 106, 109, 112, 119, 121, 128, 129, 130, 131, 133, 134, 135, 136, 137, 138, 140, 141, 142, 143, 144, 145, 146, 147, 148, 150, 151, 152, 153, 154, 155, 157, 158, 159, 160, 161, 162, 163, 164, 177, 179, 185], "cuba": 84, "cube": 28, "cubic": [139, 174], "culmen": [17, 74, 75, 109, 129, 131, 135, 137, 141, 157, 158, 159, 161, 163, 185], "culmen_column": [109, 131, 137, 141, 157, 158, 159, 163], "culmen_depth_first_sampl": 135, "curat": 5, "curiou": [107, 185], "curr_feat": 108, "current": [89, 122, 177], "curv": [17, 41, 57, 59, 92, 97, 113, 122, 135, 140, 141, 142, 165], "curvi": [132, 137], "custom": [2, 90, 115, 139], "cut": [153, 161], "cv": [17, 28, 46, 65, 72, 76, 85, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 108, 114, 118, 123, 126, 133, 135, 136, 143, 146, 147, 150, 152, 154, 155, 161, 177, 179, 185], "cv_alpha": 133, "cv_fold": 152, "cv_idx": 123, "cv_inner": 152, "cv_model": 108, "cv_outer": 152, "cv_result": [76, 84, 85, 87, 88, 89, 90, 97, 101, 102, 107, 117, 118, 120, 125, 126, 133, 135, 147, 150, 151, 152, 153, 154, 156, 179], "cv_results_": [117, 120, 123, 133, 150, 152, 154, 156, 179], "cv_results_complex_lr": 136, "cv_results_gbdt": [115, 116], "cv_results_hgbdt": 116, "cv_results_interact": 136, "cv_results_logistic_regress": 98, "cv_results_lr": 136, "cv_results_most_frequ": 98, "cv_results_rf": 115, "cv_results_stratifi": 98, "cv_results_tre": 17, "cv_results_tree_regressor": 91, "cv_results_uniform": 98, "cv_results_with_select": [125, 126], "cv_results_without_select": [125, 126], "cv_test_scor": 152, "cvx": 100, "cyan": 108, "cycl": [28, 105], "cycling_rid": 105, "cyclist": [28, 105], "d": [14, 15, 16, 17, 19, 24, 26, 28, 36, 41, 42, 43, 46, 48, 50, 52, 59, 60, 63, 65, 68, 72, 73, 80, 94, 165, 172, 177, 179, 181, 185], "dai": 28, "danger": [5, 133, 154], "dark": [103, 137], "darker": [110, 131, 137, 141, 150, 163], "dash": 142, "data": [17, 19, 21, 22, 23, 24, 27, 28, 35, 38, 39, 42, 43, 46, 50, 57, 59, 65, 68, 70, 71, 72, 74, 75, 77, 78, 82, 83, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 133, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 173, 177, 179, 182, 185], "data_bootstrap": 110, "data_bootstrap_sampl": 110, "data_categor": [84, 86, 88], "data_clf": 161, "data_clf_column": 161, "data_encod": 84, "data_expand": 139, "data_gauss": 132, "data_linear_model": 28, "data_linear_model_test": 28, "data_linear_model_train": 28, "data_max": 139, "data_min": 139, "data_misclassifi": 109, "data_moon": 132, "data_numer": [46, 76, 78, 79, 81, 83, 177], "data_numeric_test": 83, "data_numeric_train": 83, "data_random": 99, "data_rang": 121, "data_reg": 161, "data_reg_column": 161, "data_rid": 105, "data_subset": 127, "data_test": [28, 79, 80, 81, 82, 85, 100, 101, 110, 111, 112, 113, 115, 117, 120, 121, 122, 127, 131, 137, 141, 142, 145, 148, 149, 150, 152, 154, 155, 156, 157, 159, 161, 162, 163, 164], "data_test_hug": 110, "data_test_linear_model_subset": 28, "data_test_subset": [28, 127], "data_train": [28, 79, 81, 85, 100, 101, 110, 111, 112, 113, 115, 117, 120, 121, 122, 127, 131, 137, 141, 142, 145, 148, 149, 150, 152, 154, 155, 156, 157, 159, 160, 162, 163, 164], "data_train_hug": 110, "data_train_sc": 81, "data_train_subset": 127, "data_tran": 116, "data_xor": 132, "databas": [104, 156], "datafram": [28, 46, 59, 72, 73, 74, 75, 79, 80, 81, 84, 85, 90, 91, 93, 94, 96, 97, 98, 100, 101, 102, 104, 105, 106, 107, 108, 110, 115, 117, 118, 120, 121, 123, 125, 126, 132, 133, 136, 139, 140, 141, 142, 146, 147, 150, 152, 153, 154, 156, 157, 161, 162, 164, 177, 185], "datapoint": [107, 137], "datasci": 85, "dataset": [2, 15, 17, 24, 28, 34, 36, 38, 39, 41, 42, 46, 52, 55, 59, 62, 63, 65, 68, 70, 72, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 153, 154, 155, 156, 157, 159, 160, 161, 162, 163, 164, 165, 177, 179, 185], "datasets_adult_censu": 3, "datasets_ames_h": 3, "datasets_bike_rid": 3, "datasets_blood_transfus": 3, "datasets_california_h": 3, "date": [28, 85, 94, 104, 105], "date_first_rid": 105, "datetim": 105, "datetime64": 105, "datetimeindex": [28, 100, 105], "dbd": 157, "dcc": [101, 107], "deal": [46, 80, 84, 85, 87, 89, 103, 106, 124, 127, 132, 135, 139, 141, 147, 158, 177], "dealt": 150, "debt": 5, "debug": 5, "decad": 108, "decent": [118, 153, 154], "decid": [14, 21, 73, 79, 101, 102, 107, 124, 127, 129, 135, 152], "decim": 76, "decis": [2, 5, 9, 12, 14, 15, 17, 26, 34, 41, 57, 81, 85, 87, 89, 91, 95, 100, 101, 102, 109, 110, 112, 113, 116, 118, 119, 120, 121, 122, 126, 132, 139, 141, 142, 143, 146, 158, 159, 160, 163, 164, 170, 171, 172, 173, 174, 175, 177], "decision_funct": 81, "decisionboundarydisplai": [109, 131, 132, 137, 141, 157, 159, 161, 163], "decisiontreeclassifi": [109, 119, 143, 146, 157, 161, 163], "decisiontreeclassifierdecisiontreeclassifi": 109, "decisiontreeclassifierifitteddecisiontreeclassifi": [109, 157, 163], "decisiontreeregressor": [2, 17, 91, 95, 100, 101, 102, 110, 111, 115, 118, 119, 120, 139, 161, 162, 164, 177], "decisiontreeregressorifitteddecisiontreeregressor": [101, 139, 164], "declar": 96, "decompos": 57, "decorrel": 119, "decoupl": [85, 142], "decreas": [17, 28, 43, 46, 52, 57, 81, 108, 116, 125, 133, 145, 150, 156], "dedic": [96, 145], "deduc": [137, 158], "deep": [5, 87, 89, 115, 117, 118, 161], "deeper": [16, 91, 92, 95, 97, 101, 102, 109, 111, 112, 113, 116, 117, 118, 119, 120, 121, 122, 128, 129, 131, 133, 134, 135, 137, 138, 140, 141, 142, 143, 144, 145, 146, 147, 157, 159, 160, 161, 162, 163, 164], "def": [103, 108, 110, 115, 128, 131, 132, 134, 137, 139, 140, 150, 153, 154, 161, 179], "default": [14, 17, 26, 27, 43, 46, 59, 65, 73, 76, 77, 81, 82, 84, 92, 97, 99, 105, 111, 117, 118, 120, 130, 132, 133, 136, 142, 143, 144, 145, 146, 147, 151, 152, 177, 182], "default_rng": [110, 115], "defaultdict": 147, "defin": [2, 17, 28, 35, 46, 55, 65, 72, 76, 81, 84, 85, 86, 88, 90, 91, 93, 94, 98, 99, 100, 108, 109, 114, 115, 123, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 139, 140, 142, 148, 149, 150, 153, 154, 155, 156, 157, 177, 179, 181, 182, 185], "definit": [2, 80, 91, 139, 151], "degrad": 150, "degre": [2, 42, 46, 52, 102, 108, 110, 129, 130, 132, 133, 135, 136, 139], "delai": 5, "deliv": [5, 105], "demograph": [5, 103, 118], "demographi": 107, "demonstr": [5, 85, 91, 100, 109, 115, 116, 133, 139, 161], "dendro": 103, "dendro_idx": 103, "dendrogram": 103, "denomin": 80, "denot": 2, "dens": [87, 89], "densiti": [28, 132], "depart": 68, "depend": [2, 17, 21, 26, 27, 28, 36, 39, 76, 81, 84, 85, 95, 96, 98, 99, 107, 108, 110, 117, 125, 129, 132, 133, 135, 137, 141, 142, 147, 150, 154, 157, 158], "depict": 158, "deploi": [73, 85, 101, 152, 162], "deploy": [73, 152], "deprec": 150, "depth": [16, 17, 35, 74, 75, 87, 89, 102, 107, 109, 115, 116, 117, 118, 129, 131, 135, 137, 141, 148, 155, 157, 158, 159, 160, 161, 162, 163, 164, 172, 175, 177], "der": 5, "deriv": [101, 105, 107, 129, 135, 139], "descent": [81, 105, 147], "descr": [94, 101, 107], "describ": [2, 68, 79, 80, 81, 91, 107, 185], "descript": [2, 28, 48, 72, 73, 77, 82, 91, 92, 94, 95, 97, 101, 102, 107, 109, 111, 112, 113, 116, 118, 119, 120, 121, 122, 128, 129, 131, 133, 134, 135, 137, 138, 140, 141, 142, 143, 144, 145, 146, 147, 157, 159, 160, 161, 162, 163, 164, 165], "design": [0, 2, 5, 35, 79, 104, 110, 139, 145], "desir": [5, 78, 83, 119, 133], "despit": [95, 132], "detail": [2, 12, 17, 28, 38, 60, 73, 76, 77, 79, 80, 82, 84, 85, 86, 88, 92, 94, 97, 103, 105, 107, 108, 117, 118, 132, 139, 141, 142, 147, 158, 170, 171, 185], "detect": 5, "deterior": [113, 122], "determin": [57, 84, 92, 97, 142, 145], "determinist": [79, 91, 98, 139], "detriment": [87, 89, 117, 119, 122, 133], "dev": 108, "dev_features_import": 3, "develop": [2, 5, 35, 70, 73, 86, 88, 105, 129, 135, 139], "deviat": [2, 28, 46, 76, 81, 96, 101, 102, 108, 114, 123, 133, 150, 152], "devic": [24, 28], "df": [28, 72, 153], "diag_kind": [73, 108], "diag_kw": 73, "diagnos": 57, "diagon": [73, 103, 106, 132, 142, 145, 150, 158], "diagram": [0, 2, 81, 90], "dict": [81, 103, 132, 151], "dictionari": [28, 65, 76, 101], "did": [72, 73, 81, 84, 93, 98, 101, 104, 105, 117, 125, 127, 132, 137, 139, 142, 143, 146, 150, 151, 152, 154, 156, 159, 163, 182], "didact": [5, 35, 84, 85, 104], "die": 5, "diego": 107, "differ": [2, 5, 14, 15, 17, 21, 24, 28, 36, 41, 43, 46, 52, 57, 59, 63, 65, 68, 72, 73, 74, 75, 76, 80, 81, 84, 85, 89, 91, 92, 94, 95, 96, 97, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 115, 116, 117, 119, 121, 127, 128, 131, 132, 133, 134, 135, 137, 138, 139, 143, 145, 146, 147, 149, 150, 151, 152, 153, 154, 156, 157, 158, 161, 162, 177, 185], "differenti": [28, 70, 73, 150, 152, 158], "difficult": [5, 105, 117, 138, 145, 150], "digit": 94, "dimens": [2, 42, 73, 103, 132, 136, 156], "dimension": [38, 65, 70, 73, 94, 103, 124, 127, 129, 132, 135, 137, 141, 150, 153, 157], "dimensionalityreduct": 94, "dimmick": 94, "direct": [105, 132, 137, 154], "directli": [77, 80, 82, 85, 108, 115, 136, 139, 147, 157], "directori": [104, 105, 106], "disabl": 119, "disadvantag": 145, "disapprov": 145, "discard": [76, 101, 107, 109, 157], "discontinu": 147, "discourag": 72, "discov": 72, "discoveri": 35, "discret": [2, 84, 101, 102, 106, 116, 141, 145], "discrimin": 142, "discuss": [5, 13, 17, 35, 46, 73, 76, 80, 109, 115, 117, 118, 131, 133, 137, 157], "diseas": [2, 5, 24, 73, 129, 135], "disp": [97, 102, 122, 131, 135, 137, 142], "dispers": [102, 116], "displai": [95, 103, 112, 115, 121, 141, 142, 153, 179], "disproport": 73, "disproportion": 133, "disregard": 157, "dist_linkag": 103, "distanc": [59, 81, 103, 137, 156], "distance_matrix": 103, "distinct": [76, 79, 96, 99], "distinguish": 107, "distort": 94, "distribut": [2, 5, 42, 73, 74, 75, 81, 93, 94, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 116, 132, 133, 141, 145, 147, 149, 152, 154, 156, 158, 162, 179, 183], "district": [101, 107, 108, 118], "dive": 101, "diverg": [103, 131, 137, 141, 156, 163], "divers": [5, 157], "divid": [36, 81, 91, 94, 102, 134, 142, 152, 154], "divis": [94, 133], "divorc": [85, 136], "do": [2, 5, 16, 17, 28, 57, 65, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86, 88, 90, 93, 96, 98, 100, 101, 102, 104, 107, 116, 117, 119, 124, 125, 127, 129, 132, 133, 135, 138, 139, 142, 143, 146, 150, 151, 152, 153, 162, 163, 179, 181, 185], "doc": 5, "docstr": [2, 145], "doctor": [73, 84, 136], "document": [2, 3, 5, 14, 59, 77, 79, 80, 81, 82, 84, 85, 86, 88, 90, 91, 93, 94, 96, 98, 100, 101, 109, 132, 133, 136, 138, 139, 142, 143, 144, 146, 147, 150, 151, 152, 154, 157, 163, 164], "doe": [2, 5, 17, 23, 28, 35, 46, 73, 76, 81, 84, 85, 87, 88, 89, 91, 96, 97, 99, 100, 105, 108, 117, 120, 131, 132, 133, 137, 141, 145, 147, 150, 154, 156, 157, 162, 179], "doesn": 100, "dog": 2, "doi": 35, "dollar": [90, 91, 101, 107, 133], "domain": 5, "domin": 102, "dominican": 84, "don": [2, 5, 72, 77, 82, 92, 97, 101, 106, 124, 127, 133, 150], "donat": [97, 106, 142, 143, 146], "done": [2, 5, 72, 80, 88, 116, 117, 119, 127, 129, 132, 133, 135, 150, 151, 152, 154, 157, 161], "donor": 142, "dot": 2, "doubl": [117, 150, 151], "dovetail": 35, "down": [2, 5, 117], "download": [73, 101, 107], "downsid": [76, 135], "downstream": [84, 135], "drag": 28, "drastic": 109, "draw": [14, 28, 73, 93, 98, 103, 110, 117, 125, 136, 154, 157, 179], "drawback": 132, "drawn": 132, "dream": 5, "drift": 5, "drive": 5, "driven": [5, 137], "drop": [17, 28, 36, 46, 59, 72, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 93, 97, 98, 100, 103, 104, 105, 106, 107, 108, 119, 130, 136, 142, 143, 144, 145, 146, 147, 148, 150, 152, 154, 155, 177], "dropdown": [91, 94], "dropna": [17, 129, 135, 185], "dtype": [73, 75, 79, 80, 81, 82, 83, 84, 85, 86, 88, 91, 98, 101, 103, 104, 105, 106, 107, 109, 133, 141, 142, 150, 154, 156, 157], "dtype_exclud": [85, 87, 89, 136], "dtype_includ": [84, 85, 86, 87, 88, 89, 119, 136, 148, 150, 152, 154, 155], "due": [24, 28, 83, 85, 91, 94, 95, 99, 119, 147, 157], "dummi": [23, 59, 78, 83, 88, 91, 93, 98, 142, 145], "dummy_classifi": 142, "dummy_regressor": 145, "dummyclassifi": [59, 78, 83, 88, 93, 98, 142], "dummyregressor": [91, 145], "duplic": [79, 84, 85, 110, 150, 154], "dure": [2, 12, 21, 24, 28, 32, 33, 38, 39, 55, 57, 71, 72, 73, 79, 84, 86, 88, 99, 100, 101, 102, 105, 109, 118, 125, 133, 138, 143, 146, 150, 152, 154, 157, 160, 162, 164, 170, 171, 182, 183], "dynam": 39, "e": [2, 5, 17, 28, 46, 59, 63, 65, 68, 70, 71, 72, 73, 79, 80, 81, 83, 84, 85, 88, 91, 94, 95, 96, 99, 100, 101, 102, 103, 108, 115, 116, 117, 118, 121, 125, 129, 133, 135, 137, 138, 141, 142, 145, 147, 148, 150, 154, 155, 156, 157, 177, 179, 181, 185], "each": [2, 5, 17, 24, 28, 35, 39, 46, 48, 65, 68, 73, 74, 75, 76, 79, 80, 81, 84, 85, 89, 91, 93, 94, 96, 98, 99, 100, 101, 104, 105, 106, 107, 108, 109, 110, 112, 113, 114, 115, 116, 117, 118, 119, 121, 122, 123, 125, 126, 128, 132, 133, 134, 139, 140, 141, 142, 143, 146, 148, 150, 151, 152, 154, 155, 156, 157, 158, 160, 161, 162, 163, 164, 173, 177, 179, 185], "earli": [13, 28, 113, 114, 122, 123], "earlier": [76, 85, 107, 118, 142, 157], "early_stop": [28, 117, 123], "earn": [73, 103, 150], "eas": [101, 103, 132, 139], "easi": [5, 72, 85, 100, 101, 105, 132, 161], "easier": [5, 81, 84, 91, 94, 139, 145], "easiest": 101, "easili": [35, 72, 73, 80, 81, 84, 132, 135, 136, 161], "ecosystem": 35, "ecuador": 84, "edg": 132, "edgecolor": [91, 94, 98, 101, 102, 104, 105, 106, 107, 132, 153, 163], "edit": 115, "edu": 94, "educ": [35, 73, 79, 84, 85, 86, 87, 88, 89, 103, 119, 122, 130, 136, 141, 148, 150, 152, 154, 155], "education_": [84, 136], "education_column": 84, "education_doctor": 136, "education_encod": 84, "education_infrequent_sklearn": 136, "effect": [38, 43, 46, 58, 59, 81, 91, 92, 96, 97, 99, 100, 107, 108, 110, 117, 118, 119, 121, 131, 136, 137, 159, 162, 163, 165], "effici": [84, 109, 116, 119, 133, 139], "effort": [5, 105], "eg": 5, "either": [27, 68, 80, 94, 97, 99, 102, 124, 125, 127, 134, 141, 142, 145, 183], "el": 84, "elaps": [85, 125], "elapsed_tim": [81, 87, 89], "electr": [94, 104], "electron": 94, "element": [5, 21, 80, 84, 94, 101, 133, 134, 140, 142], "els": [103, 110, 139, 161], "email": 2, "emb": [96, 152], "embed": 152, "emp": [73, 79, 84, 85, 136, 150, 154], "emphas": [55, 118], "empir": [87, 89, 101], "employ": 73, "employe": 68, "empow": 5, "empti": [101, 107], "en": 2, "enabl": [84, 185], "enclosedporch": [46, 72, 104, 177], "encod": [2, 5, 28, 46, 67, 68, 79, 80, 85, 86, 88, 103, 104, 105, 107, 116, 119, 130, 132, 133, 136, 141, 165, 177], "encount": [22, 72, 84, 86, 88, 99, 143, 146], "encourag": [87, 89, 139, 145], "end": [2, 81, 84, 91, 92, 95, 97, 98, 99, 101, 102, 103, 105, 109, 111, 112, 113, 116, 118, 119, 120, 121, 122, 128, 129, 131, 133, 134, 135, 137, 138, 140, 141, 142, 143, 144, 145, 146, 147, 155, 157, 159, 160, 161, 162, 163, 164], "endpoint": 95, "energi": [68, 100, 105], "enforc": [5, 100, 105, 117, 133, 137], "engin": [5, 39, 42, 46, 94, 100, 105, 129, 130, 133, 135, 136, 165], "england": [84, 85], "enough": [2, 5, 87, 88, 89, 91, 98, 101, 102, 115, 117, 130, 136, 137, 145, 151, 157, 161, 163, 177], "enrich": 135, "enrol": 35, "ensembl": [12, 13, 16, 17, 28, 57, 85, 87, 89, 108, 109, 110, 113, 115, 116, 117, 119, 120, 121, 122, 123, 125, 126, 147, 148, 150, 152, 154, 155], "ensemble_adaboost": 3, "ensemble_bag": 3, "ensemble_ex_01": 3, "ensemble_ex_02": 3, "ensemble_ex_03": 3, "ensemble_ex_04": 3, "ensemble_gradient_boost": 3, "ensemble_hist_gradient_boost": 3, "ensemble_hyperparamet": 3, "ensemble_introduct": 3, "ensemble_random_forest": 3, "ensemble_sol_01": 3, "ensemble_sol_02": 3, "ensemble_sol_03": 3, "ensemble_sol_04": 3, "ensemble_weight": 109, "ensur": [42, 43, 94, 96, 99, 100, 103, 110, 117, 131, 132, 137, 143, 146], "entertain": 48, "entir": [2, 34, 76, 84, 85, 99, 106, 107, 117, 122, 124, 127, 137, 177, 185], "entri": [73, 76, 101, 104, 105, 106, 107, 114, 123, 128, 134, 141], "enumer": [99, 109, 110, 121, 123, 125, 128, 134, 152, 163], "environ": [79, 81, 82, 85, 90, 96, 101, 109, 132, 133, 136, 138, 139, 142, 150, 152, 154, 157, 163, 164], "equal": [2, 5, 28, 41, 43, 59, 81, 99, 110, 115, 133, 150, 151, 162], "equat": [28, 138, 141], "equival": [2, 17, 28, 46, 72, 73, 80, 92, 97, 101, 117, 131, 137, 139, 141, 142, 156, 177], "equivalence_pred_proba": 142, "erron": 142, "error": [2, 5, 15, 27, 28, 39, 41, 50, 52, 55, 57, 58, 60, 73, 76, 80, 86, 88, 91, 95, 96, 102, 109, 111, 112, 113, 115, 116, 117, 119, 120, 121, 122, 128, 129, 133, 134, 135, 138, 139, 142, 143, 144, 145, 146, 147, 148, 149, 155, 156, 165, 173], "error_scor": [86, 88, 143, 146], "errorbar": [95, 97, 102, 122, 133, 135], "errorbar_kw": 97, "errors_dummy_regressor": 91, "errors_tree_regressor": 91, "especi": [95, 133, 136], "est": [107, 133], "estat": [48, 101], "estim": [5, 12, 16, 21, 28, 32, 38, 43, 46, 55, 59, 65, 70, 76, 80, 81, 84, 85, 90, 94, 96, 98, 100, 107, 108, 109, 110, 111, 113, 114, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 127, 129, 130, 133, 135, 136, 139, 144, 145, 147, 150, 151, 152, 154, 170, 177, 179, 182, 185], "estimator__ccp_alpha": 120, "estimator__criterion": 120, "estimator__max_depth": 120, "estimator__max_featur": 120, "estimator__max_leaf_nod": 120, "estimator__min_impurity_decreas": 120, "estimator__min_samples_leaf": 120, "estimator__min_samples_split": 120, "estimator__min_weight_fraction_leaf": 120, "estimator__monotonic_cst": 120, "estimator__random_st": 120, "estimator__splitt": 120, "estimator_errors_": 109, "estimator_in_fold": 152, "estimator_weights_": 109, "estimators_": [109, 110, 112, 121], "etc": [2, 24, 28, 35, 73, 84, 96, 105, 118, 129, 135], "evalu": [2, 17, 21, 22, 25, 28, 36, 43, 46, 55, 59, 64, 65, 78, 79, 80, 83, 86, 87, 88, 89, 90, 91, 92, 94, 96, 97, 99, 100, 101, 106, 108, 110, 111, 113, 115, 116, 120, 122, 126, 127, 130, 133, 136, 138, 143, 144, 145, 146, 147, 148, 149, 150, 151, 154, 155, 156, 162, 177, 178, 182, 185], "even": [5, 35, 38, 60, 75, 76, 80, 84, 85, 87, 88, 89, 94, 98, 100, 101, 102, 103, 109, 110, 115, 116, 117, 122, 125, 132, 133, 137, 139, 141, 142, 143, 144, 146, 147, 150, 156, 182, 183], "evenli": [112, 121], "event": 90, "eventu": 2, "everi": [5, 73, 76, 101, 103, 105, 108, 138, 152], "everyon": 5, "everyth": 73, "everywher": [132, 137], "evolv": 147, "exact": [17, 28, 102, 115], "exactli": [2, 43, 55, 59, 79, 89, 91, 94, 102, 185], "examin": [72, 133], "exampl": [2, 5, 10, 13, 22, 33, 39, 48, 57, 70, 71, 73, 76, 78, 80, 81, 83, 84, 85, 90, 91, 94, 99, 100, 102, 103, 105, 109, 110, 116, 119, 125, 133, 134, 136, 139, 140, 145, 147, 151, 161, 163, 165, 171, 182, 183], "exc": [143, 146], "except": [17, 86, 88, 130, 136, 143, 146, 157, 163], "exclud": [72, 73, 125], "exclus": [79, 132], "exec": [73, 79, 84, 85, 136, 150, 154], "execut": [35, 129, 135, 148, 154, 155, 179], "exercis": [9, 10, 11, 18, 28, 29, 30, 31, 35, 37, 40, 44, 46, 58, 62, 64, 67, 72, 79, 84, 115, 117, 132, 138, 139, 150, 152, 157, 165, 167, 176, 177, 178, 180], "exhaust": [148, 155, 182], "exhibit": [50, 145], "exist": [16, 73, 76, 94], "expand": 139, "expans": [132, 139], "expect": [17, 28, 65, 72, 73, 75, 79, 84, 85, 96, 99, 100, 101, 107, 110, 117, 127, 129, 133, 135, 136, 145, 147, 149, 152, 156, 161], "expens": [28, 72, 116, 150, 183], "experi": [17, 35, 46, 59, 70, 72, 78, 83, 92, 94, 95, 96, 97, 101, 102, 107, 113, 114, 116, 122, 123, 131, 137, 143, 146, 156, 159, 162, 163], "expert": [101, 139], "explain": [5, 17, 28, 38, 57, 80, 85, 91, 102, 108, 115, 116, 122, 126, 137, 145, 150, 170], "explan": [105, 158], "explanatori": [79, 100], "explicit": [79, 80, 101, 139, 144, 147], "explicitli": [84, 143, 146, 154, 156], "explor": [5, 46, 74, 75, 79, 81, 84, 87, 89, 94, 97, 103, 112, 114, 115, 121, 123, 130, 132, 133, 136, 139, 149, 150, 152, 153, 154, 156, 165, 183, 185], "expos": [2, 81, 84, 119, 142], "express": [42, 46, 60, 72, 89, 92, 95, 97, 101, 107, 108, 115, 129, 132, 135, 137, 139, 153, 156, 161, 179], "extend": [70, 79], "extens": 101, "extercond": 104, "exterior1st": 104, "exterior2nd": 104, "extern": [91, 95, 145, 152], "exterqu": 104, "extra": [39, 72, 105, 137, 143, 146, 152], "extract": [42, 76, 94, 105, 108, 110, 133, 150, 154, 185], "extrapol": [42, 132, 160, 164, 171], "extrem": [91, 96, 102, 106, 107, 124, 127, 132, 133], "extremum": 105, "exxon": 100, "f": [2, 7, 8, 17, 28, 45, 47, 49, 51, 53, 61, 66, 73, 76, 79, 80, 81, 82, 83, 84, 85, 87, 88, 89, 90, 94, 96, 97, 99, 100, 101, 107, 108, 109, 110, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 127, 128, 131, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 145, 146, 147, 150, 151, 152, 154, 155, 157, 161, 163, 166, 179], "f_classif": [124, 125, 127], "facecolor": 110, "fact": [2, 17, 81, 83, 89, 102, 103, 105, 133, 154, 164, 177], "factor": [24, 28, 91, 95, 137, 145], "fail": [5, 100, 132], "failur": [35, 55, 136], "fair": [73, 96, 118], "fairlearn": [5, 73], "fairli": 157, "fall": [46, 101, 145, 154], "fallaci": 5, "fals": [5, 15, 52, 80, 84, 87, 89, 94, 96, 100, 105, 107, 110, 123, 125, 126, 132, 133, 135, 136, 139, 142, 146, 149, 150, 152, 154, 156, 157, 163, 175, 177], "famili": [12, 13, 38, 55, 57, 73, 79, 81, 85, 118, 136, 151], "familiar": [77, 82, 107, 114, 123, 144, 147], "fanci": 5, "fantast": [5, 127], "far": [5, 46, 90, 91, 100, 102, 115, 122, 126, 133], "farm": [73, 79, 84, 136, 150, 154], "fashion": [81, 163], "fast": [85, 89], "faster": [59, 65, 73, 81, 115, 117, 148, 155], "favor": 132, "fc": [101, 107], "feat_nam": 108, "featur": [5, 14, 15, 16, 17, 23, 28, 32, 33, 34, 36, 39, 41, 42, 46, 48, 57, 59, 63, 64, 65, 68, 70, 71, 72, 73, 74, 75, 76, 79, 80, 85, 86, 88, 90, 91, 93, 94, 95, 97, 98, 100, 101, 102, 103, 104, 105, 106, 107, 109, 110, 115, 116, 117, 119, 124, 127, 129, 130, 135, 136, 140, 141, 145, 150, 151, 152, 156, 157, 158, 159, 160, 161, 162, 163, 164, 172, 173, 177, 185], "feature_import": 108, "feature_importances_": 126, "feature_nam": [17, 104, 107, 112, 121, 128, 130, 132, 133, 134, 136, 138, 140, 141, 157, 158, 160, 161, 162, 163, 164], "feature_names_in_": 133, "feature_select": [124, 125, 126, 127], "feature_selection_ex_01": 3, "feature_selection_introduct": 3, "feature_selection_limitation_model": 3, "feature_selection_sol_01": 3, "feature_selector": [126, 127], "features_of_interest": [107, 133], "fed": 80, "feder": [84, 136], "feed": 110, "feedback": 5, "feel": [74, 75, 84, 114, 117, 123, 133, 147, 185], "femal": [73, 79, 84, 85, 136, 150, 154], "fenc": [90, 104], "fetch": 107, "fetch_california_h": [91, 95, 101, 102, 107, 108, 111, 113, 114, 115, 116, 117, 118, 120, 122, 123, 149, 156], "few": [5, 73, 74, 75, 78, 79, 83, 101, 104, 106, 107, 117, 119, 129, 132, 135, 137, 152, 158], "fewer": [65, 117, 145], "field": [35, 73, 185], "fig": [103, 104, 108, 132, 133, 142, 145, 153, 156, 157, 158, 179], "fight": [28, 38], "figsiz": [73, 75, 103, 104, 105, 106, 107, 108, 132, 133, 136, 142, 145, 157, 161, 162, 163], "figur": [2, 26, 76, 79, 99, 101, 103, 108, 109, 110, 115, 141, 150, 152, 153, 154, 157, 163, 179], "file": [63, 73, 79, 80, 104, 105, 106, 185], "fill_diagon": 103, "filter": [73, 84, 105, 150], "final": [2, 12, 22, 38, 43, 55, 59, 70, 73, 80, 81, 84, 85, 91, 96, 101, 105, 107, 110, 114, 115, 117, 122, 123, 124, 126, 127, 130, 133, 136, 137, 143, 144, 146, 147, 150, 151, 152, 160, 164, 182], "financi": 100, "find": [2, 5, 48, 57, 73, 79, 80, 81, 84, 92, 93, 96, 97, 98, 100, 102, 111, 112, 114, 120, 121, 123, 126, 127, 129, 131, 132, 133, 135, 137, 138, 148, 149, 150, 152, 154, 155, 156, 157, 159, 160, 163, 164, 170, 179], "fine": [34, 39, 85, 87, 89, 152, 156, 182], "finer": 142, "finish": [2, 133], "finit": [2, 63, 68, 73, 84], "fireplac": [46, 72, 104, 177], "fireplacequ": 104, "first": [7, 8, 28, 45, 47, 49, 51, 53, 55, 61, 62, 63, 64, 66, 70, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86, 87, 88, 89, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 112, 114, 116, 117, 118, 119, 121, 123, 124, 125, 126, 127, 129, 132, 133, 135, 138, 139, 141, 142, 143, 144, 145, 146, 147, 150, 152, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 182], "first_data_valu": 82, "first_predict": 82, "first_target_valu": 82, "fish": [73, 79, 84, 136, 150, 154], "fit": [5, 13, 23, 25, 28, 38, 39, 41, 42, 43, 46, 63, 65, 76, 77, 79, 82, 83, 84, 87, 89, 90, 92, 96, 97, 98, 100, 101, 102, 108, 109, 110, 112, 113, 115, 116, 117, 118, 119, 120, 121, 122, 124, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 141, 142, 145, 147, 150, 151, 152, 153, 154, 155, 156, 157, 159, 160, 161, 162, 163, 164, 165, 170, 174, 179], "fit_and_plot_classif": 161, "fit_and_plot_regress": 161, "fit_predict": 2, "fit_score_plot_regress": 139, "fit_tim": [76, 84, 85, 97, 101, 115, 116, 125, 146, 152], "fit_transform": [2, 65, 81, 84, 85, 103, 104, 116, 127, 139], "fittedlinearregress": 139, "fittedpipelin": [81, 85, 90, 132, 136, 139, 150, 152, 154], "fittedsvr": 139, "five": [65, 80, 85, 101], "fix": [2, 28, 46, 52, 57, 84, 92, 97, 117, 133, 149, 150, 156, 161, 177, 179, 183], "flatnonzero": 109, "flavor": 28, "flexibl": [2, 5, 52, 55, 57, 60, 92, 97, 102, 119, 157], "flipper": [17, 112, 121, 128, 129, 134, 135, 138, 140, 158, 160, 161, 162, 164, 185], "flipper_length": [128, 134, 140], "flipper_length_first_sampl": 135, "flipper_length_rang": [128, 134, 138, 140], "float": [42, 105, 107, 116, 130, 136, 154], "float64": [79, 80, 83, 91, 98, 101, 104, 105, 106, 107, 133, 138, 139, 141, 142, 154], "floor": 28, "flower": 2, "fluctuat": [102, 139, 150], "fn": 142, "fnlwgt": 103, "focu": [2, 17, 73, 79, 80, 84, 101, 104, 107, 109, 115, 133, 142, 145, 150, 151, 157], "focus": [5, 13, 60, 107, 109, 133, 142, 152], "fold": [17, 24, 28, 46, 59, 72, 76, 94, 99, 101, 108, 114, 123, 125, 126, 127, 128, 129, 130, 133, 134, 135, 136, 144, 147, 150, 152, 154, 156, 177, 185], "fold_idx": 99, "follow": [0, 2, 12, 13, 16, 17, 21, 22, 26, 28, 32, 33, 39, 41, 43, 46, 48, 55, 57, 59, 65, 68, 70, 71, 72, 73, 77, 80, 81, 82, 84, 85, 92, 93, 94, 96, 97, 98, 100, 102, 108, 110, 114, 115, 117, 119, 123, 130, 131, 132, 133, 136, 137, 138, 139, 140, 141, 142, 145, 148, 149, 150, 151, 152, 154, 155, 156, 158, 162, 170, 171, 177, 179, 182, 183, 185], "fontsiz": 73, "food": 105, "footprint": 135, "forc": [43, 81, 87, 89, 109, 116, 119, 133, 138], "force_int_remainder_col": [150, 152, 154], "forecast": 100, "forest": [10, 12, 13, 14, 16, 17, 36, 108, 112, 113, 116, 118, 121, 122, 125, 126, 165], "forest_predict": 121, "forget": 96, "forgot": 185, "form": [92, 94, 96, 97, 100, 108, 128, 129, 134, 135, 138, 140, 145, 151], "formal": 108, "format": [73, 85, 100, 105, 134, 140, 150], "former": [12, 73, 81, 102, 115, 125, 142], "formula": 140, "fortun": 133, "forum": [5, 35], "forward": [28, 105], "found": [5, 59, 96, 106, 107, 114, 117, 123, 133, 138, 148, 150, 152, 155, 156, 157, 161, 162, 179, 185], "foundat": 104, "four": [2, 105, 106, 142], "fourth": 28, "fp": 142, "fr2": 104, "frac": [17, 28, 91], "fraction": [79, 102, 142, 145], "frame": [2, 104, 105, 106, 107], "framework": [21, 22, 33, 55, 56, 95, 102, 114, 123, 144, 147, 152, 165, 182], "franc": 84, "francisco": 107, "free": [0, 5, 35, 84, 114, 117, 123, 133, 185], "freq": 105, "frequenc": [99, 106, 142], "frequent": [83, 88, 93, 98, 104, 154, 174], "from": [2, 5, 13, 17, 23, 24, 28, 35, 36, 43, 46, 48, 55, 57, 59, 73, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 130, 131, 132, 133, 135, 136, 137, 138, 139, 140, 141, 142, 143, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 172, 174, 177, 179, 181, 182, 183, 185], "from_estim": [95, 97, 102, 109, 122, 131, 132, 135, 137, 141, 142, 157, 161, 163], "from_predict": 145, "front": 101, "frontal": 28, "frontier": 137, "fulfil": 2, "full": [5, 7, 8, 17, 35, 45, 47, 48, 49, 51, 53, 61, 65, 66, 76, 77, 81, 82, 84, 96, 101, 117, 122, 124, 126, 127, 130, 136, 140, 148, 152, 155, 166, 179, 182], "full_data": 139, "fullbath": [90, 104], "fulli": [57, 84, 95, 101, 113, 117, 122], "fun": 35, "func": [73, 101, 107], "function": [2, 5, 17, 28, 35, 42, 50, 57, 59, 65, 72, 76, 79, 80, 81, 84, 85, 91, 93, 98, 101, 102, 104, 107, 108, 109, 110, 114, 115, 123, 124, 127, 128, 131, 132, 134, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 149, 152, 153, 156, 157, 159, 162, 163, 174, 185], "fundament": [35, 55, 91, 97, 145], "furthemor": 132, "further": [2, 38, 73, 94, 95, 121, 132, 139, 140, 150, 152, 161, 163], "furthermor": [2, 46, 101, 132, 133, 135, 141], "futur": [5, 73, 76, 85, 90, 99, 100, 101, 136, 159, 163], "g": [2, 5, 17, 28, 72, 73, 84, 85, 94, 96, 100, 102, 108, 112, 118, 121, 125, 128, 129, 133, 134, 135, 137, 138, 140, 141, 148, 154, 155, 156, 158, 160, 161, 162, 164, 177, 185], "ga": 105, "gain": [33, 35, 63, 73, 76, 78, 79, 80, 81, 83, 84, 85, 95, 100, 103, 111, 120, 124, 125, 127, 130, 136, 141, 145, 150, 151, 152, 154], "gamma": [92, 96, 97, 131, 132, 137], "gap": [17, 102, 133], "garag": 46, "garagearea": [46, 72, 104, 177], "garagecar": [46, 72, 104, 177], "garagecond": 104, "garagefinish": 104, "garagequ": 104, "garagetyp": 104, "garageyrblt": 104, "garbag": 73, "garri": 94, "gaug": 108, "gauss": 132, "gaussian": 132, "gave": [5, 142], "gbdt": [109, 122], "gd": 104, "gdprv": 90, "gear": 0, "geist": 94, "gender": 73, "gener": [5, 16, 17, 21, 22, 27, 28, 46, 49, 55, 57, 58, 59, 65, 76, 78, 79, 80, 81, 83, 84, 85, 86, 88, 89, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 106, 107, 109, 110, 111, 112, 113, 115, 116, 117, 118, 119, 120, 121, 122, 124, 125, 126, 127, 129, 130, 133, 135, 136, 137, 139, 141, 142, 144, 145, 147, 148, 149, 150, 151, 152, 154, 155, 156, 162, 165, 168, 177, 179, 185], "generate_data": [110, 115], "generate_dict": 103, "gentil": 94, "gentoo": [75, 157, 158, 163], "gentoo_proba": 157, "geograph": [101, 107, 118], "germani": 84, "get": [2, 17, 33, 41, 46, 52, 59, 65, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 85, 86, 87, 88, 89, 94, 95, 96, 99, 100, 101, 102, 105, 107, 109, 110, 113, 114, 115, 122, 123, 124, 125, 127, 130, 133, 136, 137, 139, 142, 143, 144, 145, 146, 147, 150, 152, 153, 154, 156, 157, 160, 161, 162, 164, 165, 179, 180, 181], "get_feature_import": 108, "get_feature_names_out": [130, 136], "get_param": [59, 92, 97, 111, 120, 131, 137, 151, 181, 185], "get_paramet": 181, "get_score_after_permut": 108, "git": 5, "github": [5, 35, 79, 81, 82, 85, 90, 96, 101, 109, 132, 133, 136, 138, 139, 142, 150, 152, 154, 157, 163, 164], "give": [2, 5, 13, 21, 23, 28, 32, 52, 55, 57, 70, 73, 76, 81, 83, 84, 85, 94, 95, 96, 99, 101, 105, 106, 108, 109, 110, 115, 116, 117, 118, 119, 125, 131, 133, 137, 138, 139, 141, 142, 145, 152, 156, 157], "given": [2, 5, 17, 28, 41, 48, 57, 65, 79, 81, 84, 91, 93, 96, 98, 99, 101, 104, 106, 107, 108, 110, 115, 119, 126, 131, 132, 133, 135, 136, 137, 138, 140, 141, 142, 147, 150, 152, 154, 163, 172, 179, 181, 182, 183], "glanc": 132, "global": 139, "glossari": [80, 165], "go": [7, 8, 12, 21, 28, 32, 35, 38, 45, 47, 49, 51, 53, 55, 61, 66, 70, 76, 77, 82, 84, 92, 94, 96, 97, 101, 102, 103, 105, 107, 108, 118, 125, 141, 142, 147, 157, 166, 170, 182], "goal": [2, 24, 35, 73, 76, 77, 78, 82, 83, 86, 87, 88, 89, 90, 94, 117, 118, 119, 125, 145, 148, 149, 155, 156, 185], "goe": [72, 140, 142], "good": [5, 21, 24, 46, 73, 76, 78, 79, 80, 83, 84, 85, 87, 89, 100, 101, 102, 106, 108, 117, 118, 119, 126, 128, 129, 133, 134, 135, 137, 138, 141, 142, 150, 152, 153, 154, 156, 157, 179], "goodness_fit_measur": [128, 134], "googl": 2, "got": [79, 114, 123, 164], "goto": 5, "gov": [73, 79, 84, 85, 136, 150, 154], "gp": [28, 105], "grad": [73, 79, 84, 85, 136, 150, 154], "gradient": [5, 9, 12, 13, 15, 16, 17, 28, 81, 85, 109, 113, 114, 118, 122, 123, 147, 150, 154, 163, 165], "gradient_boost": [115, 116], "gradientboostingregressor": [115, 116, 122], "graduat": 94, "grai": 94, "gram": [121, 129, 135, 138], "granular": [107, 142], "graph": 142, "graph_object": 103, "graphic": [2, 65, 85, 99, 107, 140], "graviti": 28, "great": [2, 5, 48], "greater": [5, 28, 50, 72, 132], "greec": 84, "green": [150, 152, 157, 163], "grid": [2, 96, 105, 114, 117, 118, 123, 133, 149, 152, 154, 156, 161, 165, 175, 177, 178, 179, 183, 185], "gridsearchcv": [2, 5, 96, 118, 123, 149, 150, 152, 154, 156, 161, 177, 179, 185], "gridsearchcvifittedgridsearchcv": [96, 150, 152], "grlivarea": [46, 72, 104, 177], "grother": 94, "ground": [5, 115, 142], "group": [2, 5, 19, 24, 28, 73, 99, 100, 101, 107, 130, 136, 165], "group_id": 94, "groupbi": 141, "groupkfold": 94, "grow": [87, 89, 117, 118, 161, 179], "grown": [101, 113, 117, 122], "grvl": 104, "gt": [73, 79, 80, 154], "guam": 84, "guarante": [27, 108], "guatemala": 84, "guid": [5, 94, 99, 132, 141, 145, 185], "guido": 5, "g\u00e9ron": 5, "h": [28, 73, 79, 84, 85, 108, 136, 150, 154], "ha": [2, 24, 36, 42, 43, 46, 63, 70, 72, 73, 79, 80, 81, 83, 84, 85, 87, 89, 91, 95, 98, 99, 100, 101, 102, 104, 107, 108, 109, 110, 116, 117, 122, 129, 130, 132, 133, 135, 136, 137, 138, 140, 142, 143, 145, 146, 149, 150, 151, 154, 156, 157, 163, 177, 179, 185], "habit": 108, "had": [28, 101, 108, 135, 139, 141], "haiti": 84, "half": [93, 98, 142], "halfbath": [90, 104], "hand": [5, 28, 90, 94, 109, 122, 133, 135, 139, 150, 152], "handbook": 5, "handi": [86, 88, 101, 105], "handl": [2, 70, 71, 79, 84, 85, 89, 104, 115, 139, 150, 165], "handle_unknown": [72, 84, 85, 86, 87, 88, 89, 90, 119, 136, 148, 150, 152, 154, 155], "handler": [85, 136], "handprint": 94, "handwrit": 94, "handwritten": 94, "happen": [57, 65, 73, 86, 88, 91, 102, 119, 135, 157], "hard": [5, 72, 73, 74, 75, 102, 124, 127, 141, 142, 147, 150], "harder": [80, 125], "harm": 102, "harmless": 5, "hassl": 118, "hasti": 5, "hat": 91, "have": [2, 24, 28, 41, 46, 48, 52, 57, 59, 65, 68, 73, 76, 78, 79, 80, 81, 83, 84, 88, 91, 94, 95, 96, 99, 100, 101, 102, 104, 105, 106, 107, 108, 109, 112, 113, 115, 116, 117, 118, 119, 121, 122, 124, 125, 127, 129, 132, 133, 134, 135, 136, 139, 140, 141, 142, 145, 148, 150, 151, 152, 153, 154, 155, 156, 157, 158, 161, 162, 163, 170, 177, 179, 182, 183, 185], "haven": 142, "head": [68, 72, 73, 75, 101, 104, 105, 106, 107, 108], "health": [5, 35], "healthi": 73, "heart": [5, 28, 105, 129, 135], "heat": 104, "heatingqc": 104, "heatmap": [150, 153], "heavier": [140, 158], "heavili": 108, "height": [73, 75, 129, 135], "held": [80, 91, 113, 117, 122, 152], "help": [5, 59, 65, 81, 83, 87, 89, 91, 92, 93, 95, 97, 98, 102, 104, 105, 106, 107, 108, 117, 124, 127, 131, 132, 133, 137, 141, 142, 145, 185], "helper": [79, 81, 84, 85, 115, 128, 134, 139, 143, 146], "henc": [98, 108, 115, 133, 154, 163], "her": 106, "here": [2, 28, 35, 59, 73, 74, 76, 77, 78, 79, 80, 81, 84, 85, 86, 87, 88, 91, 92, 93, 94, 97, 100, 102, 103, 105, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 124, 126, 127, 128, 129, 130, 131, 133, 134, 136, 137, 139, 140, 141, 142, 143, 144, 146, 148, 149, 150, 151, 152, 155, 158, 159, 160, 162], "hesit": 28, "heterogen": [73, 79, 85, 104, 133], "hgbt": 117, "hi": [28, 106], "hierarch": 103, "hierarchi": 103, "high": [17, 28, 36, 50, 52, 57, 63, 73, 75, 79, 83, 84, 87, 89, 94, 97, 102, 103, 104, 105, 106, 107, 108, 132, 137, 141, 150, 151, 154], "high_revenue_clf": 83, "higher": [2, 5, 26, 38, 42, 82, 83, 85, 91, 94, 96, 101, 104, 105, 108, 109, 117, 118, 132, 133, 136, 137, 138, 147, 149, 152, 156, 179], "highest": [96, 109, 124, 125, 127, 142, 145, 153, 154], "highli": [60, 73, 132], "highlight": [73, 80, 84, 94, 96, 109, 115, 124, 125, 127, 133, 142, 150, 152, 157, 162], "hill": 28, "hint": [14, 27, 28, 46, 59, 72, 74, 75, 77, 78, 82, 83, 87, 89, 129, 131, 133, 135, 137, 159, 163, 185], "hist": [73, 75, 91, 94, 98, 101, 102, 104, 105, 106, 107, 141], "hist_gbdt": 123, "histgradientboostingclassifi": [2, 85, 87, 89, 116, 148, 150, 152, 153, 154, 155, 181], "histgradientboostingclassifierhistgradientboostingclassifi": [150, 152, 154], "histgradientboostingregressor": [5, 17, 28, 116, 117, 123, 147], "histogram": [15, 28, 65, 73, 74, 75, 81, 93, 98, 103, 104, 107, 114, 116, 123, 141, 150, 154], "histogram_gradient_boost": 116, "histor": 2, "holand": 84, "hold": [103, 145, 153, 156, 179, 185], "home": [101, 107], "homogen": 133, "hondura": 84, "hong": 84, "hope": [5, 90, 132, 145], "hopefulli": [102, 117, 138], "horizont": [73, 141, 161, 163], "hospit": [5, 24], "host": [35, 154], "hostedtoolcach": [116, 133], "hot": [68, 84, 85, 130, 132, 136], "hour": [12, 21, 38, 55, 70, 73, 76, 78, 79, 80, 81, 83, 84, 85, 103, 130, 136, 150, 151, 152, 154, 170, 182], "hours_per_week_limit": 73, "hous": [1, 2, 48, 72, 90, 91, 95, 101, 102, 108, 111, 113, 114, 115, 116, 117, 118, 120, 122, 123, 133, 144, 145, 147, 165], "house_pric": [90, 104, 144, 145, 147], "houseag": [101, 107, 108], "household": [101, 107], "housestyl": [90, 104], "how": [2, 17, 21, 22, 25, 28, 32, 33, 38, 41, 46, 55, 57, 63, 68, 72, 73, 74, 75, 76, 78, 79, 80, 81, 83, 84, 85, 90, 91, 94, 95, 97, 99, 100, 102, 103, 108, 109, 110, 115, 117, 119, 121, 129, 131, 132, 133, 135, 137, 138, 142, 143, 146, 150, 151, 152, 153, 154, 156, 157, 159, 161, 162, 163, 170, 175, 179, 181, 182], "howev": [2, 5, 21, 35, 79, 80, 81, 84, 85, 87, 89, 91, 94, 96, 98, 99, 101, 102, 104, 105, 106, 108, 109, 110, 113, 115, 116, 117, 118, 119, 122, 124, 125, 127, 132, 133, 134, 138, 139, 141, 142, 143, 145, 146, 147, 150, 152, 154, 156, 157, 161, 162, 182], "hspace": [104, 105, 107], "html": [2, 79, 81, 82, 85, 90, 96, 101, 107, 109, 132, 133, 136, 138, 139, 142, 150, 152, 154, 157, 163, 164], "http": [2, 5, 35, 73, 80, 94, 101, 107], "hue": [73, 75, 105, 106, 107, 109, 131, 137, 141, 153, 157, 158, 161, 163], "huge": 107, "human": [5, 73, 102, 105, 132], "hundr": [101, 107, 125], "hungari": 84, "husband": [73, 79, 84, 85, 136, 150, 154], "hyper": [5, 17, 36, 96, 98, 102, 152, 156, 179], "hyperparamet": [12, 16, 25, 39, 42, 46, 59, 92, 95, 96, 97, 102, 103, 111, 118, 120, 121, 122, 127, 131, 132, 133, 137, 147, 149, 156, 157, 170, 171, 178, 179, 180, 181, 182, 183], "hyperparameter_nam": 151, "hyperplan": 2, "hypothesi": [94, 133], "hypothet": 141, "i": [0, 2, 12, 14, 15, 16, 17, 19, 21, 24, 26, 27, 28, 32, 33, 34, 35, 36, 38, 39, 41, 42, 43, 46, 48, 49, 50, 52, 55, 57, 59, 60, 63, 65, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 88, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 101, 103, 104, 105, 106, 107, 108, 109, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 170, 172, 173, 175, 177, 179, 182, 185], "ic": 94, "icon": [0, 35], "id": [90, 94, 104], "idea": [2, 73, 90, 94, 97, 108, 115, 152], "ideal": [2, 101, 142, 145], "ident": [100, 101, 116, 142, 152], "identifi": [5, 12, 70, 85, 94, 97, 102, 142, 179], "idx": [99, 125, 163], "idxmax": 142, "ignor": [28, 72, 84, 85, 87, 88, 89, 90, 91, 108, 136, 156], "ii": [12, 21, 76, 80, 138], "iii": 76, "ill": [73, 133], "illustr": [2, 32, 57, 76, 80, 81, 84, 93, 96, 98, 99, 101, 108, 119, 132, 139, 142, 152, 157, 158, 161, 162], "iloc": [28, 82, 99, 105, 107, 108, 109, 110, 115, 150, 161], "imag": [5, 94], "imagin": [74, 75, 102, 108], "imbal": [73, 106, 137], "imbalanc": [24, 59, 73, 98, 133, 142, 185], "immedi": [143, 146], "impact": [2, 17, 43, 46, 55, 84, 86, 87, 88, 89, 94, 95, 96, 102, 103, 113, 117, 122, 130, 133, 136, 145, 153, 156, 161, 179, 181, 182, 183, 185], "imperfect": [5, 108], "implement": [2, 89, 99, 109, 110, 116, 138, 141, 143, 146, 150, 153], "impli": [2, 161], "implic": 38, "import": [5, 12, 17, 21, 28, 35, 36, 46, 57, 59, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 162, 163, 164, 165, 168, 170, 177, 179, 181, 185], "importance_permut": 36, "importances_mean": 108, "importances_std": 108, "importantli": 55, "impos": [117, 154], "imposs": 97, "imprecis": 102, "impress": 100, "improv": [5, 35, 65, 89, 90, 94, 95, 97, 98, 102, 111, 113, 117, 120, 122, 139, 145, 153, 179, 182], "impur": [157, 163], "imput": [90, 104], "imshow": 103, "inc": [73, 79, 84, 85, 136, 150, 154], "inclin": 141, "includ": [2, 28, 33, 35, 63, 84, 96, 104, 125, 126, 129, 130, 133, 135, 136, 137, 139, 151, 174], "include_bia": [110, 132, 133, 135, 136, 139], "inclus": 5, "incom": [63, 73, 79, 83, 91, 101, 103, 107, 108], "incomplet": 89, "incorpor": [94, 125], "incorrect": 142, "increas": [5, 15, 17, 27, 28, 42, 43, 46, 52, 57, 75, 81, 84, 97, 101, 102, 108, 113, 116, 117, 121, 122, 130, 133, 136, 137, 140, 141, 150, 154, 156, 157, 159, 161, 162, 163, 175, 183], "increasingli": 5, "increment": 173, "inde": [2, 17, 28, 73, 79, 80, 81, 85, 89, 92, 94, 96, 97, 101, 103, 104, 105, 106, 107, 108, 109, 110, 115, 116, 117, 118, 124, 125, 126, 127, 131, 132, 133, 135, 137, 138, 139, 141, 142, 143, 145, 146, 147, 151, 152, 154, 157, 158, 161, 162, 163], "independ": [36, 84, 91, 96, 100, 110, 116, 117, 132, 133, 142, 145, 151, 154], "index": [28, 46, 73, 80, 94, 99, 100, 105, 108, 110, 116, 125, 129, 133, 135, 137, 141, 146, 150, 152, 157], "index_col": [28, 100, 105, 153, 154, 179], "index_column": 123, "india": 84, "indic": [28, 73, 85, 94, 101, 105, 107, 108, 110, 125, 128, 133, 134, 136, 150, 156, 163], "individu": [5, 16, 28, 73, 81, 85, 103, 106, 110, 112, 115, 119, 121, 133, 141, 157, 185], "induc": [5, 108, 110, 130, 136, 137], "induct": 132, "industri": 35, "ineffici": 84, "inequ": 5, "infer": [36, 105, 125, 136, 140, 182], "inferior": 157, "inferred_body_mass": 138, "infin": 2, "infinit": [131, 137], "influenc": [46, 95, 100, 102, 108, 133, 139, 145, 154, 170], "info": [72, 80, 94, 104, 105, 106, 107], "inform": [5, 23, 28, 46, 72, 73, 76, 80, 84, 85, 91, 94, 95, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 117, 118, 125, 126, 133, 139, 141, 142, 143, 146, 150, 151, 154, 156, 158], "infrequent_if_exist": 84, "inher": [42, 95, 100, 108], "initi": [73, 115, 150, 152, 182], "inject": 119, "inner": [25, 96, 114, 123, 133, 152, 177], "inner_cv": 96, "inner_cv_result": 123, "inpati": 5, "inplac": 100, "input": [5, 23, 39, 41, 57, 72, 73, 78, 80, 81, 83, 84, 87, 88, 89, 91, 94, 97, 98, 102, 108, 110, 128, 134, 139, 140, 141, 152, 157, 158, 172, 185], "input_featur": 139, "inria": 35, "insert": [108, 131, 137], "insid": [77, 82, 84, 104, 143, 146, 177], "insight": [21, 32, 35, 46, 63, 73, 102, 124, 127, 140, 141, 145, 154], "insist": 141, "inspct": [73, 79, 84, 136, 150, 154], "inspect": [2, 5, 28, 59, 63, 81, 90, 101, 103, 109, 114, 117, 123, 126, 130, 131, 132, 136, 137, 139, 141, 150, 153, 154, 157, 159, 161, 163, 185], "instabl": 108, "instanc": [5, 46, 63, 73, 79, 80, 81, 84, 94, 100, 101, 104, 105, 107, 108, 111, 114, 120, 123, 124, 125, 126, 127, 129, 132, 133, 135, 138, 140, 142, 144, 145, 147, 151, 154, 175, 181, 182], "instantan": 105, "instead": [17, 28, 46, 59, 72, 76, 80, 81, 84, 86, 87, 88, 89, 90, 91, 92, 94, 95, 97, 98, 101, 110, 115, 117, 118, 119, 129, 130, 133, 135, 136, 139, 142, 143, 144, 145, 146, 147, 148, 149, 150, 152, 153, 154, 155, 156, 157, 158, 159, 162, 163, 177], "institut": 94, "instruct": 84, "instructor": 5, "insur": 68, "int": [72, 76, 90, 109, 154], "int32": [132, 149, 156, 161], "int64": [73, 75, 79, 83, 84, 104, 106, 154, 156], "integ": [28, 68, 76, 79, 84, 86, 88, 94, 104, 106, 108, 110, 130, 136, 154, 156, 185], "integr": [84, 116, 133], "intellig": 35, "intend": [100, 101, 106, 177], "intens": [76, 98, 163, 179], "intention": 110, "inter": 117, "interact": [35, 46, 73, 103, 105, 129, 130, 133, 135, 136, 139, 153, 154, 156, 179], "interaction_onli": [129, 130, 135, 136], "intercept": [2, 39, 128, 129, 130, 134, 135, 136, 137, 138, 139, 140, 141, 157], "intercept_": [41, 42, 138, 139], "intercept_body_mass": [128, 134, 138, 140], "interest": [5, 48, 73, 74, 75, 87, 89, 91, 93, 94, 98, 99, 101, 102, 103, 105, 107, 110, 115, 121, 129, 132, 135, 136, 138, 139, 141, 142, 145, 147, 150, 151, 152, 153, 154, 156, 163, 185], "interfac": [2, 80], "interlac": 161, "intermedi": [129, 135, 139, 150, 152, 153], "intern": [13, 43, 65, 81, 85, 96, 101, 109, 110, 113, 114, 122, 123, 133, 145, 147, 150, 152, 179], "internet": 107, "interplai": 132, "interpol": [100, 164], "interpret": [5, 16, 35, 73, 103, 105, 107, 108, 131, 135, 136, 137, 138, 139, 145, 153, 157, 163], "intersect": [5, 103, 153, 156], "intersect1d": 109, "interv": [105, 107, 110, 112, 115, 121, 160, 162, 164], "intervent": [5, 73], "intract": 132, "intro": [145, 165], "introduc": [28, 35, 80, 81, 91, 101, 102, 108, 110, 125, 129, 130, 133, 135, 136, 139, 140, 145, 147, 165], "introduct": [5, 55, 70, 165], "introductori": [10, 141, 165], "introspect": [85, 185], "intuit": [5, 9, 10, 12, 13, 21, 38, 44, 55, 70, 79, 80, 84, 103, 105, 107, 109, 110, 115, 123, 131, 132, 137, 138, 139, 141, 157, 158, 162, 163, 165], "invari": [94, 132], "invers": [131, 137], "invert": 153, "invert_yaxi": 150, "investig": [28, 81, 100, 108, 111, 114, 118, 120, 123, 125, 157], "involv": [2, 5, 73, 98, 101, 110, 151], "io": [2, 35], "iowa": 145, "ir1": [90, 104], "iran": 84, "ireland": 84, "iri": [2, 99], "irreduc": 57, "irrelev": 59, "irrespect": [78, 83, 98, 132, 179, 185], "island": 136, "isol": [85, 101, 117], "issu": [72, 73, 84, 85, 99, 100, 101, 105, 106, 116, 132, 133, 139], "itali": 84, "iter": [2, 28, 65, 76, 81, 84, 99, 100, 101, 113, 117, 122, 133, 136, 150, 152, 153, 154], "itertool": [94, 104], "its": [2, 5, 17, 22, 28, 57, 60, 73, 78, 80, 81, 83, 84, 85, 91, 92, 95, 96, 97, 98, 100, 101, 102, 103, 108, 111, 115, 117, 119, 120, 125, 130, 131, 132, 133, 136, 137, 138, 140, 141, 142, 143, 145, 146, 149, 156, 157, 179], "itself": [2, 17, 41, 81, 91, 94, 96, 97, 101, 103, 106, 110, 121, 145, 152, 182], "ivl": 103, "j": 94, "jake": 5, "jamaica": [84, 85], "jame": 5, "janet": 94, "japan": 84, "jargon": [2, 80, 81], "job": [5, 81], "jockei": 2, "join": 90, "jointplot": 81, "jose": 107, "juli": 94, "jupyt": [67, 79, 81, 82, 85, 96, 101, 109, 132, 133, 136, 138, 139, 142, 150, 152, 154, 157, 163, 164, 165], "just": [46, 88, 99, 101, 102, 105, 108, 109, 110, 113, 115, 117, 119, 122, 133, 136, 139, 141, 142], "justifi": 145, "k": [2, 24, 59, 76, 79, 80, 81, 91, 95, 101, 102, 109, 111, 113, 114, 115, 116, 117, 118, 120, 122, 123, 125, 126, 127, 145, 147, 149, 150, 152, 153, 156, 163, 182], "kaggl": 5, "kai": 94, "kaynak": 94, "kb": [104, 106], "kbinsdiscret": [116, 132, 139], "kbinsdiscretizerkbinsdiscret": [132, 139], "kde": 108, "keep": [21, 28, 73, 90, 94, 101, 103, 104, 105, 107, 108, 125, 126, 127, 129, 131, 135, 136, 137, 139, 141, 152, 154], "kei": [5, 7, 8, 45, 47, 49, 51, 53, 61, 65, 66, 92, 97, 99, 102, 117, 120, 123, 125, 126, 137, 150, 151, 154, 161, 166], "kellei": [101, 107], "ken": 94, "kera": 5, "kernel": [46, 92, 97, 129, 131, 132, 135, 137, 139], "kernel_approxim": [131, 132, 135, 137, 139], "keyword": 84, "kfold": [94, 96, 99, 123, 144, 147, 152], "kg": [28, 158], "kill": 5, "kilomet": 105, "kind": [48, 73, 85, 89, 102, 108, 122, 139, 145, 172, 185], "kitchenabvgr": [46, 72, 104, 177], "kitchenqu": 104, "kneighbor": 156, "kneighborsclassifi": [2, 59, 77, 80, 82, 185], "kneighborsclassifierifittedkneighborsclassifi": 82, "kneighborsregressor": [149, 156], "kneighborsregressor__n_neighbor": 156, "knot": 132, "know": [2, 5, 85, 92, 94, 97, 101, 102, 104, 110, 115, 127, 133, 139, 142], "knowledg": [5, 35, 70, 96, 102, 110, 127, 139, 152], "known": [32, 73, 106, 107, 133, 134, 141, 142, 145, 150, 157], "kwarg": 154, "kybernetika": 94, "l": [84, 94], "l1": 108, "l2": 43, "l2_regular": [153, 154, 179], "lab": [5, 35], "label": [52, 57, 68, 75, 79, 84, 85, 88, 98, 99, 100, 102, 103, 109, 110, 115, 121, 134, 136, 140, 141, 142, 143, 146, 150, 162, 164], "labelencod": 103, "lack": 57, "lai": 137, "lakeview": 48, "lambda": [105, 107, 153, 179], "landcontour": [90, 104], "landslop": 104, "languag": 35, "lao": 84, "larg": [5, 17, 46, 68, 85, 96, 101, 105, 107, 113, 114, 116, 117, 122, 123, 126, 127, 130, 131, 133, 135, 136, 137, 145, 149, 152, 153, 154, 156, 179], "larger": [46, 81, 84, 102, 117, 122, 135, 137, 139, 140, 148, 149, 152, 155, 156, 157, 174], "largest": [46, 107], "last": [28, 46, 68, 84, 99, 104, 106, 130, 132, 133, 136, 139, 141, 142], "later": [2, 12, 17, 73, 79, 80, 90, 97, 101, 115, 116, 131, 135, 137, 139, 142, 149, 150, 152, 156], "latest": [2, 27, 35], "latitud": [101, 107, 108], "latter": [73, 81, 96, 102, 125, 142], "law": [5, 28], "layout": 104, "lb": 94, "lda": 94, "le": 103, "lead": [26, 57, 73, 84, 85, 88, 94, 99, 100, 103, 115, 117, 118, 119, 133, 135, 136, 137, 142, 149, 150, 152, 153, 154, 156], "leaf": [101, 117, 150, 154, 155, 157, 161, 163, 173, 174], "leaf_rot": 103, "leak": 127, "leakag": 100, "learn": [13, 15, 16, 22, 23, 26, 27, 28, 33, 36, 37, 39, 41, 43, 50, 57, 59, 63, 65, 67, 68, 71, 73, 76, 77, 79, 81, 82, 83, 84, 85, 86, 87, 88, 89, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 102, 106, 107, 109, 112, 114, 116, 117, 119, 121, 123, 124, 125, 126, 127, 130, 132, 133, 135, 136, 139, 141, 142, 143, 145, 146, 147, 150, 152, 153, 154, 155, 158, 159, 162, 163, 171, 172, 173, 174, 180, 181, 183, 185], "learner": [5, 13, 109, 110, 115, 117, 118], "learning_r": [113, 114, 117, 122, 123, 148, 150, 152, 153, 154, 155, 179, 181], "learningcurvedisplai": [92, 95, 97], "learnt": [2, 57, 95, 108, 139, 157], "least": [2, 99, 117, 137, 161, 185], "leav": [24, 80, 92, 97, 103, 117, 154, 161, 163], "leaveonegroupout": [28, 100], "left": [2, 72, 73, 80, 91, 94, 98, 99, 100, 105, 107, 108, 109, 110, 115, 117, 118, 121, 127, 131, 134, 136, 137, 142, 150, 152, 153, 157, 161, 163], "legend": [91, 94, 98, 99, 100, 105, 107, 109, 110, 115, 121, 131, 134, 136, 137, 141, 142, 153, 157, 161, 162, 163, 164], "legend_label": 115, "legit": 127, "len": [82, 101, 103, 108, 109, 116, 117, 132, 136], "len_data": 139, "length": [2, 5, 17, 28, 74, 75, 79, 80, 101, 104, 105, 109, 112, 121, 128, 129, 131, 134, 135, 137, 138, 140, 141, 142, 150, 154, 157, 158, 159, 160, 161, 162, 163, 164, 185], "less": [5, 28, 43, 81, 85, 91, 94, 100, 103, 107, 108, 109, 110, 117, 118, 126, 130, 132, 133, 135, 136, 137, 139, 150, 156, 183], "lesson": [35, 151], "let": [2, 5, 17, 24, 42, 46, 72, 73, 76, 77, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 104, 105, 106, 107, 108, 110, 115, 117, 125, 126, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 142, 145, 147, 150, 151, 154, 157, 158, 161, 162, 185], "letter": [2, 101, 107], "level": [90, 96, 98, 117, 119, 124, 125, 127, 131, 132, 136, 137, 142, 159, 160, 161, 163, 164, 177], "leverag": 132, "lexicograph": 84, "li": 84, "lib": [116, 133], "librari": [2, 35, 70, 73, 100, 156], "licens": [0, 35], "lie": 65, "life": [5, 73, 102, 124, 127, 145], "lift": 5, "light": 137, "like": [2, 5, 17, 24, 28, 43, 52, 73, 79, 80, 84, 85, 86, 88, 96, 98, 101, 104, 106, 108, 109, 110, 125, 133, 134, 139, 140, 141, 142, 143, 146, 147, 149, 151, 156], "limit": [2, 17, 28, 31, 33, 41, 55, 57, 60, 79, 85, 91, 101, 102, 103, 110, 132, 133, 139, 145, 154, 158, 160, 164, 165], "linalg": 133, "linalgwarn": 133, "line": [2, 28, 42, 68, 73, 79, 103, 108, 110, 115, 116, 117, 129, 132, 135, 137, 139, 141, 142, 145, 153, 154, 157, 161, 162, 179, 185], "line_predict": 115, "linear": [2, 5, 12, 14, 15, 28, 36, 38, 39, 41, 42, 43, 46, 52, 57, 79, 81, 84, 85, 86, 87, 88, 89, 92, 94, 97, 103, 107, 110, 118, 126, 128, 129, 130, 134, 135, 136, 144, 145, 147, 151, 157, 158, 160, 162, 164, 172, 174, 177, 182], "linear_model": [28, 43, 46, 72, 76, 79, 81, 84, 85, 86, 88, 90, 94, 98, 99, 107, 108, 110, 127, 131, 132, 133, 135, 136, 137, 138, 139, 141, 142, 145, 147, 151, 157, 162, 164, 177, 179, 181], "linear_model_flipper_mass": [128, 134, 140], "linear_models_ex_01": 3, "linear_models_ex_02": 3, "linear_models_ex_03": 3, "linear_models_ex_04": 3, "linear_models_feature_engineering_classif": 3, "linear_models_regular": 3, "linear_models_sol_01": 3, "linear_models_sol_02": 3, "linear_models_sol_03": 3, "linear_models_sol_04": 3, "linear_regress": [133, 135, 138, 139, 164], "linear_regression_in_sklearn": 3, "linear_regression_interact": 135, "linear_regression_non_linear_link": 3, "linear_regression_without_sklearn": 3, "linearli": [39, 41, 43, 79, 103, 132, 139], "linearregress": [41, 43, 133, 135, 138, 139, 145, 147, 162, 164, 177], "linearregressionifittedlinearregress": 138, "linearregressioninot": 139, "linearregressionlinearregress": [133, 139], "lines_residu": 115, "linestyl": [73, 107, 110, 121, 131, 132, 137, 142, 162], "linewidth": [110, 131, 132, 137, 161], "link": [5, 14, 78, 79, 83, 94, 104, 105, 106, 107, 108, 117, 124, 127, 142, 145], "linkag": 103, "linspac": [91, 95, 97, 98, 110, 115, 121, 128, 134, 138, 140, 153], "list": [2, 28, 35, 46, 59, 84, 87, 89, 99, 103, 108, 110, 111, 113, 120, 122, 133, 143, 144, 146, 147, 151, 185], "list_feature_import": 108, "listedcolormap": 132, "literatur": 119, "littl": [52, 79, 94, 99, 153], "live": [5, 133], "ll": 73, "lo": 107, "load": [5, 17, 63, 74, 75, 76, 77, 81, 82, 84, 85, 86, 88, 92, 94, 95, 96, 97, 99, 100, 101, 102, 103, 106, 107, 109, 111, 116, 117, 118, 120, 129, 130, 131, 132, 133, 135, 136, 137, 138, 139, 142, 150, 151, 153, 154, 157, 158, 159, 160, 161, 162, 163, 164, 179, 185], "load_breast_canc": 96, "load_digit": 94, "load_iri": [99, 179], "loan": 5, "loc": [91, 94, 98, 99, 100, 105, 107, 109, 110, 115, 121, 131, 134, 136, 137, 141, 142, 153, 157, 161, 163], "local": [73, 79, 84, 85, 90, 106, 116, 117, 123, 136, 139, 150, 154], "locat": [2, 74, 75, 91, 104, 105, 107, 141, 153], "log": [95, 133, 145, 153, 154], "log10": [153, 179], "log2": [153, 179], "log_loss": 147, "logarithm": [92, 97], "logic": [5, 136, 150, 163], "logical_xor": 132, "logist": [26, 40, 41, 43, 65, 79, 81, 84, 85, 86, 87, 88, 89, 92, 93, 94, 97, 98, 99, 124, 127, 130, 131, 136, 137, 141, 142, 151, 157, 165], "logistic_regress": [3, 131, 132, 137, 141], "logisticregress": [2, 43, 72, 76, 78, 79, 81, 83, 84, 85, 86, 88, 90, 94, 98, 99, 127, 130, 131, 132, 136, 137, 141, 142, 147, 151, 157, 179, 181], "logisticregression__c": [131, 137, 179, 181], "logisticregressionifittedlogisticregress": [79, 142, 157], "logisticregressionlogisticregress": [81, 85, 90, 132, 136], "logspac": [46, 92, 97, 107, 133, 149, 156], "loguniform": [117, 154], "loguniform_int": 154, "long": [46, 87, 89, 104, 105, 107, 129, 135, 151], "longer": [72, 89, 133, 137, 140, 151, 158], "longest": 164, "longitud": [101, 107, 108], "look": [5, 28, 62, 63, 65, 74, 75, 77, 79, 80, 82, 84, 94, 95, 97, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 115, 117, 129, 131, 133, 135, 137, 139, 140, 142, 157, 158, 165, 179, 185], "loop": [5, 96, 108, 148, 150, 152, 155, 185], "lose": 73, "loss": [73, 76, 78, 79, 80, 81, 83, 84, 85, 103, 130, 136, 145, 147, 150, 151, 152, 154], "loss_func": 147, "loss_funct": 147, "lot": [5, 73, 107, 108, 127, 133, 142], "lotarea": [46, 72, 90, 104, 133, 177], "lotconfig": 104, "lotfrontag": [46, 72, 90, 104, 133, 177], "lotshap": [90, 104], "low": [52, 63, 73, 75, 79, 83, 94, 103, 105, 107, 108, 110, 117, 132, 137, 142, 157, 161, 179], "low_revenue_clf": 83, "lower": [26, 27, 28, 46, 52, 88, 91, 94, 95, 98, 101, 103, 105, 108, 117, 122, 133, 135, 137, 142, 145, 147, 149, 152, 156, 161], "lower_bound": 94, "lowercas": 90, "lowest": [137, 138, 145], "lowqualfinsf": [46, 72, 104, 177], "lr": 155, "lr_weight": 137, "lsqr": 133, "lt": [73, 79, 80, 154], "ltorgo": [101, 107], "lucki": 101, "lure": 96, "lvl": [90, 104], "ly": [2, 133], "m": [28, 76, 84, 85, 94, 102, 118, 150, 154], "m1": [62, 64, 67, 165], "m2": [54, 56, 58, 165], "m3": [150, 165, 178, 180], "m4": [37, 40, 44, 165], "m44koooi7x8tu85wr4ez4f": 35, "m5": [165, 167, 168, 169, 176], "m6": [9, 10, 11, 117, 165], "m7": [18, 19, 20, 29, 30, 165], "ma": 28, "machin": [2, 22, 28, 33, 35, 39, 55, 57, 63, 68, 71, 73, 79, 80, 81, 83, 84, 85, 86, 88, 90, 91, 92, 93, 96, 97, 98, 99, 100, 101, 110, 119, 124, 125, 126, 127, 132, 133, 136, 138, 139, 141, 142, 145, 150, 154], "machineri": [13, 109, 115], "made": [28, 34, 41, 73, 92, 94, 97, 100, 101, 102, 106, 115, 142, 147, 151], "mae": [28, 91, 122, 129, 134, 135, 144, 147], "magic": [33, 100], "magnitud": [43, 108, 130, 131, 133, 136, 137, 172], "mai": [5, 24, 42, 43, 46, 48, 73, 79, 81, 94, 95, 100, 101, 107, 110, 117, 121, 129, 130, 133, 135, 136, 137, 138, 141, 142, 144, 147, 154, 156, 161, 163], "main": [34, 46, 76, 81, 87, 89, 117, 125, 135, 139, 156, 165], "mainli": [12, 13, 21, 32, 38, 73, 104, 157, 170], "maintain": 5, "mainten": 5, "major": [80, 83, 98, 137], "make": [2, 5, 23, 24, 28, 43, 46, 50, 55, 59, 60, 70, 72, 73, 76, 77, 79, 81, 82, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 96, 97, 98, 99, 100, 101, 102, 104, 105, 107, 108, 109, 112, 113, 114, 115, 116, 117, 118, 119, 121, 122, 123, 124, 125, 126, 127, 129, 130, 132, 133, 135, 136, 137, 138, 139, 140, 141, 142, 144, 145, 147, 148, 150, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 172], "make_blob": 161, "make_classif": [125, 126], "make_column_selector": [72, 84, 85, 86, 87, 88, 89, 119, 136, 148, 150, 152, 154, 155], "make_column_transform": [85, 87, 89, 90, 104, 119, 136, 148, 150, 152, 154, 155], "make_gaussian_quantil": 132, "make_moon": 132, "make_pipelin": [59, 65, 76, 81, 84, 85, 86, 87, 88, 89, 90, 94, 97, 98, 99, 104, 107, 108, 110, 116, 119, 124, 125, 126, 127, 129, 131, 132, 133, 135, 136, 137, 139, 141, 156], "make_scor": [143, 146], "male": [73, 79, 84, 85, 136, 150, 154], "malfunct": 73, "manag": 115, "manageri": [73, 79, 84, 85, 136, 150, 154], "mani": [2, 5, 28, 41, 59, 72, 73, 74, 75, 80, 81, 84, 85, 96, 98, 99, 101, 102, 104, 107, 110, 117, 119, 129, 130, 132, 133, 135, 136, 142, 151, 156, 185], "manipul": [73, 77, 82, 92, 97, 101], "manner": [2, 13, 28, 84, 94, 133, 147], "manual": [80, 84, 85, 110, 115, 124, 127, 139, 142, 151, 157, 165, 182], "map": [84, 88, 103, 131, 132, 133, 136, 137, 141, 156], "margin": [33, 94, 108], "marginal_kw": 81, "marit": [73, 79, 84, 85, 136, 150, 152, 154], "marker": [104, 109, 142, 163], "market": [48, 102], "marri": [73, 79, 84, 85, 136, 150, 154], "mass": [17, 28, 112, 121, 128, 129, 134, 135, 138, 140, 158, 160, 161, 162, 164, 185], "master": [73, 84, 136], "masvnrarea": [46, 72, 104, 177], "masvnrtyp": 104, "match": [76, 77, 82, 96, 141], "materi": 5, "math": [2, 104], "mathemat": [2, 57, 92, 97, 132, 139, 140, 141, 145], "matplotlib": [35, 70, 73, 81, 91, 94, 96, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 115, 121, 123, 125, 126, 131, 132, 133, 134, 136, 137, 138, 140, 141, 142, 145, 146, 157, 161, 162, 163, 164], "matric": [2, 84], "matrix": [5, 28, 81, 84, 94, 103, 119, 124, 127, 133, 139], "matter": [102, 126, 152], "max": [79, 81, 91, 105, 107, 110, 128, 133, 134, 138, 139, 140, 155, 161, 162, 164, 174, 179], "max_bin": [153, 154, 179], "max_depth": [17, 102, 109, 110, 113, 114, 115, 117, 118, 122, 123, 139, 157, 162, 163, 164, 177], "max_featur": [117, 119, 120], "max_it": [17, 28, 81, 84, 85, 88, 94, 116, 117, 123, 130, 136, 137], "max_leaf_nod": [114, 117, 123, 148, 150, 152, 153, 154, 155, 161, 179, 181], "max_sampl": 120, "maxim": [2, 46, 94, 96, 133, 147, 148, 149, 150, 154, 155, 156, 157, 175, 182], "maximum": [16, 28, 81, 84, 96, 102, 139, 142, 150, 154, 157, 159, 160, 162, 163, 164, 172, 175], "mayb": [73, 94, 102, 107], "mb": [105, 107], "mea": 156, "mean": [2, 5, 27, 28, 46, 59, 72, 73, 76, 77, 79, 80, 81, 82, 83, 84, 85, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 127, 129, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 150, 151, 152, 154, 155, 156, 157, 159, 162, 163, 174, 185], "mean_": 81, "mean_absolute_error": [101, 113, 120, 121, 122, 138, 145, 149, 156], "mean_absolute_percentage_error": 145, "mean_fit_tim": [150, 153], "mean_imput": 104, "mean_scor": 155, "mean_score_tim": [150, 153], "mean_squared_error": [138, 139, 145], "mean_test_error": [117, 120], "mean_test_scor": [117, 120, 123, 150, 152, 153, 154, 156, 179], "meaning": [68, 84, 87, 89, 100, 124, 126, 127, 136, 140, 145, 150, 163], "meaningless": [84, 110, 162], "meant": 103, "measur": [2, 5, 17, 28, 65, 72, 74, 75, 76, 79, 80, 91, 99, 102, 105, 108, 122, 125, 128, 134, 140, 142, 152, 157, 158], "meca": 28, "mechan": [5, 28, 72, 80, 81, 152], "medhousev": [101, 107, 108], "median": [27, 90, 91, 96, 101, 107, 108, 118, 123, 125, 126, 133, 136, 145, 146, 173, 174], "median_absolute_error": 145, "medic": [2, 5, 24, 73], "medicin": 5, "medinc": [101, 107, 108], "medium": 139, "member": [101, 107], "membership": 163, "memor": [17, 80, 94, 101, 102], "memori": [79, 100, 104, 105, 106, 107, 135, 139, 151, 152], "men": 5, "mention": [2, 48, 76, 92, 97, 100, 104, 109, 116, 118, 131, 132, 133, 137, 138, 141, 142, 143, 145, 146, 151, 154, 164], "mere": 72, "merg": [96, 125], "messag": [88, 89], "meta": 110, "meter": [28, 105], "method": [2, 3, 5, 13, 41, 59, 72, 73, 74, 75, 79, 80, 81, 85, 94, 100, 108, 111, 118, 119, 120, 121, 126, 129, 132, 133, 135, 139, 141, 142, 145, 150, 151, 152, 154, 165, 185], "methodolog": [5, 35, 101, 162], "methodologi": 55, "metric": [21, 26, 28, 65, 73, 76, 80, 91, 95, 99, 100, 101, 106, 110, 111, 113, 120, 121, 122, 125, 129, 133, 134, 135, 138, 139, 143, 144, 145, 146, 147, 149, 153, 156, 162, 165, 185], "metrics_classif": 3, "metrics_ex_01": 3, "metrics_ex_02": 3, "metrics_regress": 3, "metrics_sol_01": 3, "metrics_sol_02": 3, "mexico": [84, 85, 136], "mg": 28, "mid": [105, 107], "middl": [132, 151, 163], "midpoint": [105, 107], "might": [2, 5, 28, 36, 48, 72, 83, 84, 85, 87, 89, 94, 95, 99, 100, 101, 102, 105, 108, 117, 134, 142, 145, 154], "millimet": 140, "min": [79, 81, 91, 105, 107, 110, 128, 133, 134, 138, 140, 161, 162, 164, 174], "min_frequ": [84, 130, 136], "min_impurity_decreas": 161, "min_samples_leaf": [117, 118, 153, 154, 161, 179], "min_samples_split": [118, 121, 161], "mind": [21, 73, 101, 108, 117, 124, 125, 126, 127, 154], "mindset": 5, "mine": 35, "minim": [2, 5, 39, 57, 63, 91, 96, 101, 102, 137, 138, 145, 147, 157, 173], "minimum": [2, 95, 117, 137, 142, 154, 160, 161, 162, 164], "minmaxscal": [94, 110, 133, 185], "minor": 137, "minu": 156, "minut": [28, 32, 85, 105, 154], "miscfeatur": [90, 104], "misclassif": [109, 137, 142], "misclassifi": [109, 132, 137], "misclassified_samples_idx": 109, "miscval": [46, 72, 90, 104, 177], "misinterpret": 96, "mislabel": 142, "mislead": [84, 89, 100, 108], "misleadingli": 134, "misord": 84, "miss": [5, 73, 84, 94, 101, 102, 104, 106, 107, 129, 135, 154, 185], "mistak": [80, 109, 127, 137, 142], "misus": 84, "mit": 0, "mitig": [73, 117, 121], "mix": [70, 71, 73, 85, 94, 161], "ml": [2, 5, 94], "mln": 155, "mm": [17, 75, 109, 112, 121, 128, 129, 131, 134, 135, 137, 138, 140, 141, 157, 158, 159, 160, 161, 162, 163, 164, 185], "mnprv": [90, 104], "mode": [7, 8, 35, 45, 47, 49, 51, 53, 61, 66, 166], "model": [0, 9, 10, 12, 13, 14, 16, 17, 21, 22, 23, 24, 25, 26, 27, 28, 31, 32, 33, 34, 35, 36, 38, 39, 41, 42, 43, 46, 50, 52, 55, 57, 59, 60, 63, 65, 68, 70, 71, 72, 73, 75, 77, 78, 79, 82, 83, 84, 86, 87, 88, 89, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 106, 107, 110, 111, 113, 114, 115, 116, 117, 119, 120, 122, 123, 124, 125, 127, 129, 130, 131, 135, 136, 137, 138, 139, 140, 142, 143, 144, 145, 146, 147, 148, 149, 151, 153, 155, 156, 157, 160, 161, 162, 163, 164, 170, 171, 177, 179, 181, 182, 183, 185], "model_a": 27, "model_b": 27, "model_error": 138, "model_first_fold": 133, "model_grid_search": [150, 152], "model_idx": [128, 134], "model_nam": [80, 81, 151], "model_random_search": [154, 156], "model_select": [17, 28, 46, 59, 76, 78, 79, 81, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 107, 108, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 131, 133, 135, 136, 137, 141, 142, 143, 145, 146, 147, 148, 149, 150, 151, 152, 154, 155, 156, 157, 159, 161, 163, 177, 179, 185], "model_to_tun": 96, "model_transformed_target": 145, "model_with_interact": 136, "model_with_select": [125, 126], "model_without_select": [125, 126], "modern": 35, "modif": 119, "modifi": [3, 5, 116, 118, 139], "modul": [0, 2, 13, 17, 22, 33, 35, 39, 46, 57, 71, 72, 73, 79, 85, 102, 103, 107, 131, 132, 135, 137, 142, 150, 151, 152, 159, 163, 165, 171, 183], "moment": [130, 136, 157], "monei": 5, "monetari": [106, 142], "monitor": [2, 5, 28], "monoton": 145, "month": [106, 142], "mooc": [2, 73, 80, 90, 91, 92, 95, 97, 101, 102, 103, 109, 111, 112, 113, 116, 118, 119, 120, 121, 122, 128, 129, 131, 133, 134, 135, 137, 138, 140, 141, 142, 143, 144, 145, 146, 147, 157, 159, 160, 161, 162, 163, 164], "moon": 132, "more": [2, 17, 28, 35, 42, 43, 46, 52, 55, 59, 72, 73, 76, 79, 80, 81, 84, 86, 88, 89, 91, 92, 94, 95, 96, 97, 98, 99, 102, 103, 104, 105, 107, 108, 109, 110, 115, 116, 117, 118, 119, 125, 129, 131, 132, 133, 135, 136, 137, 138, 139, 140, 141, 142, 143, 145, 146, 150, 151, 152, 153, 154, 156, 157, 158, 161, 162, 163, 185], "moreov": [79, 183], "mosold": [90, 104], "most": [24, 28, 46, 59, 73, 79, 80, 83, 84, 88, 89, 92, 93, 94, 97, 98, 101, 102, 104, 108, 109, 110, 116, 125, 130, 133, 136, 137, 138, 141, 142, 150, 153, 157, 163, 172, 174, 177, 179], "most_freq_revenue_clf": 83, "most_frequ": [59, 83, 88, 98, 104, 142], "most_frequent_classifi": 98, "most_frequent_imput": 104, "mostli": [5, 136, 137, 156], "motiv": [32, 118], "move": [5, 85, 103, 105, 136, 137, 153, 156], "mpl": [157, 163], "mr": 5, "mri": 2, "msc": 94, "mse": [134, 139, 145, 147], "mse_alpha": 133, "mssubclass": [90, 104], "mszone": [90, 104], "much": [60, 73, 88, 89, 91, 98, 101, 102, 108, 116, 117, 119, 120, 125, 130, 132, 135, 136, 179], "multi": [2, 103, 125, 129, 135, 141, 153], "multiclass": [59, 157, 159, 163, 185], "multipl": [5, 84, 94, 108, 110, 115, 129, 130, 135, 136, 139, 143, 144, 146, 147, 153, 172, 173], "multipli": [28, 136, 147, 156], "multivari": 108, "must": [2, 5, 43, 108, 130, 133, 136, 149, 152, 156, 185], "m\u00fcller": 5, "n": [94, 96, 97, 99, 105, 107, 110, 116, 120, 123, 128, 131, 133, 134, 137, 141, 142, 151, 152, 155, 157], "n_bin": [116, 132, 139], "n_bootstrap": 110, "n_class": [132, 141], "n_column": 73, "n_compon": [46, 129, 131, 132, 135, 137, 139], "n_estim": [17, 109, 110, 113, 115, 116, 117, 118, 119, 120, 121, 122], "n_estimators_": 122, "n_featur": [2, 42, 117, 119, 125, 126, 132, 139, 141], "n_inform": [125, 126], "n_iter": [117, 120, 149, 154, 156, 179], "n_iter_": [81, 123], "n_iter_no_chang": [113, 122], "n_job": [91, 94, 95, 96, 97, 98, 100, 102, 107, 108, 115, 116, 117, 118, 119, 120, 122, 123, 125, 127, 133, 135, 136, 150, 152, 154, 156], "n_knot": 132, "n_neighbor": [59, 77, 82, 149, 156, 182, 185], "n_quantil": [145, 185], "n_redund": [125, 126], "n_repeat": [108, 125, 126], "n_round": 108, "n_row": 73, "n_sampl": [2, 110, 115, 125, 126, 132, 139, 141, 161], "n_samples_to_plot": 73, "n_split": [28, 91, 95, 96, 98, 99, 100, 101, 102, 108, 123, 133, 146, 150, 152], "n_string_featur": 104, "n_trial": 96, "na_filt": 177, "na_valu": [90, 104], "nafter": 81, "naiv": [5, 17, 28, 73, 96, 102, 106, 116, 119], "name": [2, 14, 28, 59, 73, 75, 79, 80, 81, 82, 83, 84, 85, 90, 91, 92, 97, 98, 99, 101, 102, 103, 104, 105, 106, 107, 108, 110, 115, 117, 120, 123, 129, 130, 131, 133, 135, 136, 137, 140, 141, 142, 150, 151, 153, 154, 181, 185], "named_step": 81, "named_transformers_": [130, 136], "nan": [86, 88, 90, 99, 104], "nanyang": 94, "narrow": 5, "nativ": [5, 73, 79, 84, 85, 101, 125, 136, 139, 150, 152, 154], "natur": [5, 21, 35, 72, 80, 84, 85, 101, 105, 110, 115, 132, 133, 154], "navig": [7, 8, 45, 47, 49, 51, 53, 61, 66, 166], "nb": 108, "nbefor": 81, "nbviewer": [79, 81, 82, 85, 90, 96, 101, 109, 132, 133, 136, 138, 139, 142, 150, 152, 154, 157, 163, 164], "ncol": [104, 132, 134, 142, 145, 163], "nearest": [2, 59, 79, 80, 81, 102, 182, 185], "nearli": 137, "necess": [21, 101], "necessari": [2, 5, 43, 73, 117, 132, 152], "necessarili": [73, 90, 101, 102, 118, 132, 133, 147, 150, 182], "need": [2, 5, 21, 23, 28, 36, 39, 59, 65, 73, 77, 79, 81, 82, 84, 85, 90, 92, 96, 97, 101, 102, 105, 108, 109, 110, 115, 117, 118, 119, 120, 130, 132, 133, 136, 137, 138, 139, 143, 144, 146, 147, 148, 151, 152, 154, 155, 161, 175, 177], "neg": [27, 28, 46, 65, 100, 101, 102, 103, 108, 133, 134, 140, 142], "neg_": [95, 101, 129, 135, 147, 149, 156], "neg_mean_absolute_error": [28, 91, 95, 101, 102, 113, 115, 116, 117, 120, 122, 129, 135, 147, 149, 156], "neg_mean_squared_error": [27, 133, 147], "negat": [101, 149, 156], "negate_scor": [95, 102, 113, 122, 135], "neighbor": [2, 59, 77, 79, 80, 81, 82, 100, 102, 149, 156, 182, 185], "neighborhood": [90, 91, 104, 108], "neither": [57, 99], "nest": [21, 22, 25, 117, 133, 148, 150, 152, 155, 165, 177, 183, 185], "netherland": 84, "neutral": 133, "never": [2, 17, 55, 73, 79, 80, 84, 85, 109, 136, 137, 145, 150, 154, 179], "nevertheless": 132, "new": [2, 5, 28, 39, 46, 73, 77, 79, 80, 81, 82, 84, 85, 86, 88, 92, 93, 94, 95, 97, 98, 100, 101, 108, 109, 112, 113, 115, 117, 121, 122, 129, 135, 136, 139, 142, 148, 150, 151, 155, 160, 162, 163, 164], "new_donor": 142, "newaxi": [132, 161], "newli": [112, 121], "newly_misclassified_samples_idx": 109, "newton": 28, "next": [7, 8, 45, 47, 49, 51, 53, 57, 61, 66, 79, 80, 84, 87, 89, 90, 92, 93, 97, 98, 99, 102, 109, 117, 118, 129, 132, 133, 135, 137, 139, 151, 157, 166], "nicaragua": 84, "nice": [85, 109, 139], "nightlif": 48, "nin": [104, 115], "nine": 99, "nip": 94, "nist": 94, "nistir": 94, "node": [101, 117, 119, 150, 155, 157, 161, 163, 172, 173, 174], "nois": [2, 46, 52, 57, 110, 115, 132, 137, 139], "noisi": [34, 60, 102, 108, 110, 137, 156, 161], "nomin": [87, 89, 103], "non": [5, 19, 28, 38, 41, 42, 43, 46, 65, 68, 81, 87, 89, 92, 95, 96, 97, 98, 103, 104, 105, 106, 107, 108, 110, 125, 129, 133, 135, 141, 156, 157, 162, 164, 165, 171, 172], "non_nest": 96, "none": [17, 46, 92, 94, 97, 101, 104, 105, 106, 107, 110, 113, 117, 118, 122, 132, 139, 153, 185], "nonoverlap": 94, "nor": 57, "norm": [157, 163], "normal": [59, 72, 81, 90, 91, 94, 103, 104, 105, 106, 107, 108, 110, 115, 116, 141, 142, 145, 149, 156, 157, 163], "notabl": [71, 142], "notat": 46, "note": [2, 7, 8, 35, 45, 46, 47, 49, 51, 53, 61, 66, 73, 76, 79, 80, 81, 84, 85, 89, 91, 92, 96, 97, 98, 101, 102, 106, 107, 108, 110, 118, 119, 122, 137, 140, 141, 150, 151, 152, 156, 157, 162, 166, 177], "notebook": [17, 22, 35, 46, 63, 72, 75, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 94, 95, 96, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 113, 115, 116, 117, 118, 119, 120, 122, 125, 126, 129, 132, 133, 135, 136, 138, 140, 141, 142, 143, 145, 146, 147, 149, 150, 151, 152, 153, 154, 156, 157, 158, 159, 161, 162, 163, 164, 165, 179], "noth": [106, 110], "notic": [17, 80, 81, 90, 91, 97, 101, 107, 108, 130, 132, 133, 135, 136, 137, 139, 141, 142, 147, 149, 150, 156, 163], "notion": 2, "now": [5, 17, 28, 46, 59, 72, 76, 77, 79, 80, 81, 82, 83, 84, 85, 86, 88, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 139, 140, 141, 142, 143, 146, 148, 150, 152, 153, 154, 155, 157, 159, 160, 161, 163, 164, 177, 185], "np": [17, 28, 46, 59, 80, 83, 91, 92, 94, 95, 97, 98, 99, 102, 103, 105, 107, 108, 109, 110, 113, 115, 116, 119, 121, 122, 124, 125, 127, 128, 129, 132, 133, 134, 135, 136, 138, 139, 140, 142, 144, 145, 147, 149, 153, 154, 156, 161, 162, 164, 174, 179], "npredict": 115, "nrow": [104, 142, 163], "ntransform": 145, "nuanc": 5, "null": [104, 105, 106, 107, 141], "num": [46, 73, 79, 84, 85, 86, 87, 88, 89, 90, 91, 92, 95, 97, 98, 103, 107, 110, 115, 119, 121, 128, 130, 133, 134, 136, 138, 140, 148, 149, 150, 152, 154, 155, 156], "num_points_to_plot": 81, "num_var": 108, "number": [2, 5, 15, 16, 17, 28, 36, 41, 43, 46, 48, 52, 59, 68, 72, 73, 76, 77, 79, 80, 81, 82, 84, 85, 86, 88, 92, 94, 95, 97, 99, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 113, 114, 115, 116, 117, 119, 121, 122, 123, 125, 126, 127, 129, 130, 132, 133, 134, 135, 136, 137, 139, 142, 144, 145, 147, 149, 150, 152, 153, 154, 156, 157, 161, 162, 172, 177, 179, 183, 185], "number_of_correct_predict": 82, "number_of_predict": 82, "numer": [2, 15, 46, 63, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 80, 82, 83, 84, 93, 98, 101, 102, 103, 104, 105, 107, 129, 130, 132, 133, 135, 136, 139, 149, 150, 151, 156, 165, 177, 185], "numeric_featur": 90, "numeric_transform": 90, "numerical_column": [73, 76, 78, 79, 81, 83, 85, 87, 89, 130, 136, 151], "numerical_columns_selector": [85, 87, 89], "numerical_data": 104, "numerical_featur": [46, 72, 104, 177], "numerical_preprocessor": 85, "numpi": [17, 28, 35, 59, 63, 65, 70, 81, 91, 94, 95, 97, 98, 99, 102, 103, 105, 107, 108, 109, 110, 115, 116, 121, 122, 124, 125, 127, 128, 132, 133, 134, 135, 136, 138, 139, 140, 142, 144, 145, 147, 153, 156, 161, 162, 164, 179], "nuniqu": [100, 105, 161, 185], "nwith": [109, 115], "nwithout": 145, "nystroem": [46, 129, 131, 132, 135, 137, 139], "nystroem__n_compon": 135, "nystroem_regress": [135, 139], "nystroemnystroem": [132, 139], "nystr\u00f6m": [132, 135], "o": [79, 163], "object": [2, 5, 28, 46, 57, 77, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 101, 103, 104, 105, 106, 119, 133, 136, 141, 142, 143, 146, 148, 149, 150, 151, 152, 154, 155, 156, 157, 162, 177], "obliqu": [141, 157], "observ": [5, 27, 43, 46, 52, 73, 75, 81, 83, 84, 85, 87, 89, 91, 93, 94, 95, 96, 97, 98, 99, 101, 102, 103, 105, 106, 109, 110, 113, 115, 117, 119, 122, 123, 125, 132, 133, 135, 136, 137, 139, 141, 142, 145, 150, 153, 156, 157, 158, 161, 162, 164, 174], "obsev": 145, "obtain": [28, 46, 59, 72, 76, 80, 84, 85, 86, 88, 90, 96, 98, 100, 101, 105, 107, 108, 111, 117, 118, 120, 127, 131, 132, 133, 137, 138, 142, 145, 152, 153, 154, 155, 179], "obviou": [73, 94, 105], "occas": 139, "occup": [73, 79, 84, 85, 107, 108, 136, 150, 152, 154], "occupation_": 136, "occupation_infrequent_sklearn": 136, "occur": [2, 84, 133, 143, 146], "occurr": 133, "off": [55, 57, 73, 76, 91, 104, 131, 132, 137, 142, 161, 165, 170, 172], "offer": [87, 89, 100, 113, 122, 154], "offici": 5, "offlin": 2, "offset": 164, "often": [5, 39, 41, 43, 57, 60, 73, 84, 85, 91, 96, 98, 102, 110, 137, 139, 145, 147, 152, 177, 182, 185], "omit": [73, 133], "onc": [2, 5, 17, 28, 79, 81, 89, 90, 94, 99, 101, 103, 110, 117, 119, 141, 142, 144, 147, 148, 149, 150, 153, 154, 155, 156, 163, 179], "one": [2, 5, 13, 17, 24, 36, 41, 42, 46, 57, 65, 68, 72, 73, 75, 76, 78, 79, 80, 81, 83, 84, 85, 87, 89, 92, 93, 94, 96, 97, 98, 99, 101, 102, 103, 105, 107, 108, 109, 115, 116, 117, 119, 122, 126, 130, 132, 133, 135, 136, 138, 139, 141, 142, 143, 145, 146, 150, 151, 152, 156, 157, 158, 161, 163, 173, 185], "onehot": 132, "onehotencod": [2, 46, 71, 72, 84, 85, 86, 87, 88, 89, 90, 130, 133, 136], "onehotencoderonehotencod": [85, 90, 136], "ones": [80, 84, 137, 150, 151, 154], "onli": [2, 5, 17, 28, 36, 41, 46, 48, 63, 65, 68, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 91, 92, 93, 96, 97, 98, 99, 101, 103, 105, 106, 109, 110, 115, 117, 119, 122, 125, 127, 129, 130, 131, 132, 133, 135, 136, 137, 140, 141, 142, 145, 150, 151, 152, 156, 157, 161, 174, 177, 181], "onlin": [2, 91, 93, 98], "oob_scor": 120, "op": [73, 79, 84, 136, 150, 154], "opac": 110, "open": [28, 46, 59, 72, 77, 79, 82, 100, 104, 105, 177], "openml": [73, 80], "openporchsf": [46, 72, 104, 177], "oper": [5, 81, 110, 132, 142, 152], "opportun": 5, "opposit": 2, "opt": [116, 133], "optic": 94, "optim": [2, 21, 22, 43, 63, 81, 91, 92, 95, 96, 97, 109, 114, 116, 118, 123, 127, 129, 133, 135, 138, 142, 145, 147, 150, 151, 152, 154, 156, 161, 177, 185], "optimist": [28, 80, 94, 96, 100, 101], "optimum": 17, "option": [16, 81, 84, 91, 99, 101, 105, 113, 122, 182, 185], "orang": [2, 73, 110, 115, 142, 157, 162, 163], "order": [2, 5, 17, 28, 39, 46, 68, 73, 81, 85, 87, 88, 89, 90, 94, 99, 100, 101, 104, 114, 119, 123, 133, 140, 150, 153, 154, 185], "ordin": [68, 72, 85, 87, 88, 89, 103, 116], "ordinalencod": [71, 84, 85, 86, 87, 88, 89, 119, 148, 150, 152, 154, 155, 177], "ordinalencoderordinalencod": [150, 152, 154], "org": [2, 73, 79, 80, 81, 82, 85, 90, 96, 101, 109, 132, 133, 136, 138, 139, 142, 150, 152, 154, 157, 163, 164], "organ": [63, 163], "orient": [2, 108, 131, 137, 163], "origin": [15, 65, 76, 79, 80, 84, 85, 99, 100, 103, 104, 109, 110, 115, 116, 119, 129, 132, 135, 136, 137, 139, 140, 145, 157, 158, 162], "oscil": 97, "other": [5, 13, 26, 28, 35, 43, 59, 72, 73, 76, 80, 81, 84, 85, 86, 87, 88, 89, 91, 92, 93, 94, 97, 98, 99, 100, 103, 105, 108, 110, 117, 125, 129, 133, 135, 136, 138, 139, 141, 145, 150, 152, 153, 154, 160, 163, 164, 179, 182, 185], "otherwis": [5, 17, 79, 88, 95, 96, 119, 132, 139, 140, 161], "our": [5, 17, 24, 28, 41, 46, 62, 63, 70, 76, 78, 79, 80, 81, 83, 85, 86, 88, 90, 91, 92, 94, 95, 96, 97, 99, 100, 101, 102, 103, 104, 105, 106, 108, 109, 110, 113, 114, 115, 117, 122, 123, 124, 125, 127, 128, 131, 132, 133, 134, 137, 138, 139, 140, 141, 142, 143, 145, 146, 151, 153, 157, 158, 162, 164, 165, 185], "ourself": [28, 142], "out": [2, 5, 24, 72, 73, 76, 79, 80, 84, 93, 95, 98, 101, 108, 110, 113, 114, 117, 118, 122, 123, 127, 131, 133, 136, 137, 142, 150, 152, 159, 160, 163, 164], "outcom": [100, 108, 141, 142, 156], "outdat": 103, "outer": [25, 96, 114, 122, 123, 132, 133, 152, 177, 185], "outer_cv": 96, "outli": 84, "outlier": [43, 91, 107, 133, 147, 161], "outperform": 85, "output": [2, 17, 41, 73, 76, 81, 84, 85, 101, 105, 108, 110, 128, 129, 132, 134, 135, 141, 142, 147, 163], "output_distribut": 145, "outsid": [17, 150, 157, 160, 164], "outstand": 100, "over": [2, 14, 23, 28, 38, 39, 73, 92, 96, 97, 100, 102, 108, 112, 116, 118, 121, 145, 148, 150, 155, 161, 170, 173], "overal": [91, 101, 113, 115, 118, 122, 133, 137, 142, 161], "overallcond": [72, 104], "overallqu": [72, 104], "overcom": [116, 118, 132, 139], "overconfid": 141, "overcrowd": 5, "overestim": 126, "overfit": [12, 15, 17, 21, 28, 32, 38, 43, 46, 50, 52, 55, 57, 58, 59, 60, 87, 89, 95, 96, 100, 108, 109, 110, 113, 115, 117, 118, 119, 121, 122, 126, 131, 132, 133, 135, 136, 137, 152, 156, 161, 165, 170, 172, 175], "overflow": 5, "overlap": [65, 75, 76, 110, 147, 154], "overli": [94, 96, 101], "overlook": 117, "oversimplifi": 79, "overview": [91, 92, 95, 97, 101, 102, 107, 109, 111, 112, 113, 116, 118, 119, 120, 121, 122, 128, 129, 131, 133, 134, 135, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 157, 159, 160, 161, 162, 163, 164, 165], "overwrite_a": 133, "own": [73, 79, 84, 85, 117, 136, 139, 150, 154], "p": [7, 8, 45, 47, 49, 51, 53, 61, 66, 94, 135, 166], "p_": 28, "pac": 136, "pace": [35, 101, 107], "packag": [63, 116, 133], "page": [2, 35, 79, 81, 82, 85, 90, 96, 101, 109, 132, 133, 136, 138, 139, 142, 150, 152, 154, 157, 163, 164], "pager": [77, 82], "pai": [5, 84, 109], "pair": [81, 103, 105, 106, 107, 128, 130, 134, 136, 153], "pairplot": [73, 74, 75, 103, 105, 106, 107, 108, 158], "pairplot_figur": [75, 158], "pairwis": 103, "palett": [105, 107, 109, 131, 137, 141, 157, 161, 163], "palmer": 158, "panda": [17, 28, 35, 46, 59, 63, 70, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 112, 115, 117, 119, 120, 121, 123, 125, 126, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 177, 179, 185], "parallel": [13, 15, 73, 103, 115, 152, 153, 156, 179], "parallel_coordin": [153, 156, 179], "param": [120, 123, 132, 150, 153], "param_": [117, 120, 123, 150, 154], "param_classifier__": 150, "param_classifier__l2_regular": 153, "param_classifier__learning_r": [150, 152, 153], "param_classifier__max_bin": 153, "param_classifier__max_leaf_nod": [150, 152, 153], "param_classifier__min_samples_leaf": 153, "param_distribut": [117, 154, 156], "param_estimator__max_depth": 120, "param_grid": [96, 118, 120, 150, 152, 161, 179, 185], "param_kneighborsregressor__n_neighbor": 156, "param_learning_r": 117, "param_max_featur": [117, 120], "param_max_it": 117, "param_max_leaf_nod": 117, "param_max_sampl": 120, "param_min_samples_leaf": 117, "param_n_estim": 120, "param_nam": [97, 102, 122, 135, 150, 153, 154, 179, 185], "param_rang": [59, 97, 102, 113, 122, 129, 135], "param_standardscaler__with_mean": 156, "param_standardscaler__with_std": 156, "param_valu": 185, "paramet": [0, 5, 14, 17, 25, 26, 27, 28, 36, 39, 41, 43, 46, 52, 55, 57, 59, 65, 72, 76, 77, 78, 79, 81, 82, 83, 84, 86, 88, 92, 96, 97, 98, 101, 102, 108, 109, 110, 111, 113, 114, 117, 118, 119, 120, 122, 123, 126, 129, 130, 132, 135, 136, 138, 140, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 159, 163, 170, 177, 179, 181, 182, 185], "parameter_tuning_ex_02": 3, "parameter_tuning_ex_03": 3, "parameter_tuning_grid_search": 3, "parameter_tuning_manu": 3, "parameter_tuning_nest": 3, "parameter_tuning_parallel_plot": 3, "parameter_tuning_randomized_search": 3, "parameter_tuning_sol_02": 3, "parameter_tuning_sol_03": 3, "parametr": [38, 128, 134, 138, 140, 154, 157, 162, 164, 171], "parcoord": 103, "parenthesi": 28, "pars": [104, 105], "parse_d": [28, 100, 105], "part": [28, 73, 84, 85, 96, 102, 108, 116, 127, 128, 133, 134, 137, 142, 157, 159, 163], "particip": 5, "particular": [2, 5, 26, 70, 73, 76, 77, 79, 80, 82, 85, 89, 91, 94, 96, 97, 101, 103, 115, 119, 132, 137, 141, 142, 145, 150, 152, 153, 156], "particularli": [5, 72, 86, 88, 117], "partit": [72, 76, 157, 159, 161, 162, 163], "partli": 108, "pass": [2, 17, 28, 46, 59, 65, 72, 76, 84, 85, 86, 88, 92, 96, 97, 101, 110, 140, 143, 144, 146, 147, 149, 150, 152, 156, 157, 163, 185], "passthrough": [85, 87, 89, 119, 148, 150, 152, 154, 155], "passthroughpassthrough": [150, 152, 154], "past": [100, 106, 128, 130, 134, 136, 142], "patholog": 150, "patient": [2, 5, 24, 129, 135], "pattern": [2, 22, 73, 94, 124, 127, 152], "pave": [90, 104], "paveddr": 104, "pcolormesh": [131, 132, 137], "pd": [17, 28, 46, 59, 63, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 112, 115, 117, 119, 120, 121, 123, 125, 126, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 177, 179, 185], "peak": 73, "pearson": 103, "peculiar": [73, 104], "pedal": [28, 105], "penal": [43, 133], "penalti": [43, 46, 108, 137], "penguin": [1, 17, 74, 75, 109, 112, 121, 128, 129, 131, 134, 135, 137, 138, 140, 141, 157, 159, 160, 162, 163, 164, 165, 185], "penguins_classif": [74, 75, 109, 131, 137, 141, 157, 158, 159, 161, 163], "penguins_non_miss": [129, 135, 185], "penguins_regress": [112, 121, 128, 134, 138, 140, 158, 160, 161, 162, 164], "penguins_test": [131, 137, 141], "penguins_train": [131, 137, 141], "peopl": [2, 5, 72, 73, 94, 101, 107, 108, 142], "per": [2, 28, 73, 76, 78, 79, 80, 81, 83, 84, 85, 96, 101, 103, 105, 107, 116, 119, 130, 136, 139, 142, 150, 151, 152, 154], "percentag": [101, 110, 145], "percentil": 125, "perfect": [17, 41, 55, 91, 101, 108, 115, 124, 126, 127, 142, 161], "perfectli": [75, 89, 103, 106, 115, 117, 139, 145], "perform": [14, 16, 17, 18, 21, 22, 23, 25, 28, 33, 34, 46, 59, 63, 65, 72, 73, 75, 76, 78, 79, 80, 81, 83, 84, 85, 86, 88, 90, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 106, 108, 109, 111, 112, 113, 115, 116, 117, 118, 119, 120, 121, 122, 124, 125, 126, 127, 129, 130, 133, 135, 136, 139, 142, 144, 145, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 161, 170, 177, 179, 182, 183, 185], "perm_importance_result": 108, "perm_importance_result_train": 108, "permit": 90, "permut": [98, 179], "permutation_import": 108, "permutation_test_scor": 98, "permuted_scor": 108, "permuted_score_train": 108, "perpendicular": [137, 157], "person": [5, 63, 73, 79, 103, 106, 142, 150], "perspect": [81, 87, 89, 91, 102], "perturb": [102, 108], "peru": 84, "pessimist": 28, "petal": 2, "petrol": 105, "phenomena": 101, "philippin": 84, "philosoph": 96, "phone": 68, "phrase": 5, "physic": [91, 102], "physiolog": 105, "pick": [5, 28, 90, 96, 104, 105, 108, 116, 151, 182], "piecewis": [2, 132, 162, 174], "pillar": 35, "pipelin": [0, 2, 12, 21, 32, 33, 35, 38, 42, 46, 49, 55, 59, 65, 67, 70, 71, 72, 76, 81, 85, 86, 88, 92, 93, 94, 96, 97, 98, 99, 104, 107, 108, 116, 119, 124, 125, 126, 127, 129, 130, 131, 132, 133, 135, 136, 137, 139, 141, 148, 149, 150, 151, 152, 154, 155, 156, 170, 179, 181, 182, 185], "pipelineifittedpipelin": [90, 133], "pipelineinot": [81, 85, 90, 132, 136, 139, 150, 152, 154], "pipelinepipelin": [150, 152, 154], "pitfal": [96, 136], "pivot_t": 150, "pivoted_cv_result": 150, "pixel": 94, "pl2oka_2qdj": 35, "pla": 5, "place": [73, 115, 127, 133, 139], "plan": [43, 159, 163], "plant": 105, "plateau": [17, 95, 113, 122], "platform": 35, "playlist": 35, "pleas": [79, 81, 82, 85, 90, 93, 96, 98, 101, 109, 122, 132, 133, 136, 138, 139, 142, 150, 152, 154, 157, 163, 164, 177, 185], "plot": [2, 17, 28, 46, 59, 65, 73, 74, 75, 81, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 112, 114, 115, 121, 123, 125, 126, 128, 131, 132, 133, 134, 136, 137, 138, 139, 140, 141, 142, 143, 145, 146, 152, 153, 154, 156, 157, 158, 159, 160, 161, 162, 163, 164, 179], "plot_chance_level": 142, "plot_decision_boundari": [131, 132, 137], "plot_decision_tree_with_residu": 115, "plot_feature_import": 108, "plot_kw": [73, 108], "plot_list": 103, "plot_method": [131, 132, 137], "plot_tre": [157, 159, 161, 162, 163], "plotli": [103, 153, 156, 179], "plt": [73, 81, 91, 94, 96, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 115, 121, 123, 125, 126, 131, 132, 133, 134, 136, 137, 138, 141, 142, 145, 146, 157, 161, 162, 163, 164], "pm": 28, "po": 133, "point": [2, 5, 14, 73, 77, 79, 81, 82, 89, 94, 99, 100, 105, 106, 107, 108, 110, 116, 117, 119, 122, 132, 133, 137, 139, 141, 142, 145, 152, 154, 157, 161, 163], "pointer": 5, "poissonregressor": 145, "poland": 84, "poli": [46, 129, 132, 135, 139], "polici": [5, 24], "poly_featur": 135, "polynomi": [2, 42, 52, 102, 110, 132, 139], "polynomial_expans": 139, "polynomial_regress": 139, "polynomial_regressor": 110, "polynomialfeatur": [110, 129, 130, 132, 133, 135, 136, 139], "polynomialfeaturespolynomialfeatur": [132, 133, 136, 139], "ponnuthurai": 94, "pool": [104, 127], "poolarea": [46, 72, 90, 104, 133, 177], "poolqc": [90, 104], "poorli": [2, 88], "popul": [5, 73, 101, 103, 107, 108, 142, 156], "popular": [85, 103, 119], "portion": 28, "portug": 84, "pos_label": [142, 143, 146], "posit": [2, 5, 26, 27, 28, 43, 46, 65, 99, 102, 103, 108, 110, 118, 131, 133, 137, 140, 141, 142, 143, 145, 146, 154, 185], "possibl": [2, 5, 14, 28, 35, 41, 42, 57, 59, 63, 68, 73, 81, 84, 91, 95, 96, 99, 101, 102, 103, 108, 110, 116, 119, 127, 129, 132, 133, 134, 135, 137, 139, 141, 143, 145, 146, 147, 149, 150, 152, 153, 156, 159, 160, 162, 163, 164, 179, 185], "possibli": [119, 128, 134], "post": [2, 48, 68, 171], "potenti": [2, 43, 48, 73, 81, 85, 95, 101, 102, 116, 138, 142, 152, 181], "power": [5, 28, 35, 105, 110, 117, 118, 129, 133, 135, 139, 157], "powertransform": 185, "pprint": 154, "ppv": 142, "pr": 142, "practic": [32, 57, 73, 76, 79, 80, 83, 90, 91, 95, 96, 98, 101, 102, 117, 119, 133, 142, 143, 145, 146, 152, 153, 154], "practition": 85, "pre": [124, 127], "preced": 115, "precis": [26, 28, 57, 85, 117, 140, 142, 143, 146, 150], "precision_scor": [142, 143, 146], "precisionrecalldisplai": 142, "predefin": 81, "predict": [0, 12, 15, 17, 21, 22, 23, 24, 28, 32, 35, 38, 39, 41, 42, 46, 48, 50, 55, 57, 59, 60, 65, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 85, 86, 88, 90, 91, 92, 93, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 112, 115, 117, 118, 119, 120, 121, 122, 124, 125, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 145, 147, 149, 151, 153, 156, 157, 158, 159, 160, 161, 162, 164, 170, 174, 177, 181, 182, 185], "predict_proba": [41, 43, 81, 131, 132, 137, 141, 142, 147, 157, 159, 163], "predicted_body_mass": [134, 138, 140], "predicted_target": 81, "predictionerrordisplai": 145, "predictor": [5, 13, 14, 15, 16, 65, 80, 81, 85, 98, 102, 120, 130, 133, 136, 157], "prefer": [85, 185], "prefix": [149, 150, 153, 156], "prepar": 43, "preprint": 94, "preprocess": [28, 35, 46, 59, 64, 65, 72, 73, 76, 84, 85, 86, 87, 88, 89, 90, 92, 94, 97, 98, 99, 103, 107, 108, 110, 116, 119, 130, 131, 132, 133, 135, 136, 137, 139, 141, 145, 148, 150, 151, 152, 154, 155, 156, 165, 177, 179, 181, 185], "preprocessor": [46, 85, 86, 87, 88, 89, 90, 94, 104, 119, 130, 136, 148, 150, 151, 152, 154, 155, 177, 185], "preprocessor__copi": 151, "preprocessor__with_mean": 151, "preprocessor__with_std": 151, "preschool": [73, 84], "presenc": [133, 147], "present": [0, 7, 8, 12, 21, 22, 28, 33, 38, 45, 47, 49, 51, 53, 55, 61, 66, 70, 72, 73, 80, 84, 91, 94, 95, 99, 100, 102, 103, 104, 105, 106, 107, 109, 110, 115, 116, 118, 119, 126, 138, 140, 142, 143, 145, 146, 152, 154, 158, 162, 166, 170, 171, 172], "preserv": [81, 99, 100, 150], "press": [7, 8, 45, 47, 49, 51, 53, 61, 66, 166], "pressur": 5, "pretrain": 5, "pretti": 108, "preval": 142, "prevent": [17, 42, 84, 88, 110, 117, 179], "previou": [7, 8, 15, 17, 21, 28, 45, 46, 47, 49, 51, 53, 59, 61, 63, 65, 66, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 93, 95, 96, 98, 99, 100, 101, 102, 110, 111, 114, 115, 116, 117, 119, 120, 123, 124, 127, 128, 129, 131, 132, 133, 134, 135, 137, 138, 139, 143, 146, 148, 150, 151, 152, 153, 154, 155, 156, 157, 159, 160, 163, 164, 166, 177, 179, 182, 185], "previous": [28, 46, 72, 84, 85, 92, 97, 100, 104, 109, 115, 116, 117, 118, 129, 132, 135, 137, 139, 141, 145, 148, 150, 152, 154, 155, 161, 162], "price": [2, 48, 72, 90, 91, 101, 102, 104, 107, 108, 144, 145, 147], "primari": 125, "princip": 125, "principl": [12, 21, 33, 142], "print": [73, 76, 79, 80, 81, 82, 83, 84, 85, 87, 88, 89, 90, 94, 96, 97, 99, 100, 101, 107, 108, 109, 110, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 125, 127, 128, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 145, 146, 147, 149, 150, 151, 152, 154, 155, 156, 157, 163], "prior": [35, 70], "priori": 162, "prioriti": 75, "prioritis": 5, "privat": [48, 73, 79, 84, 85, 136, 150, 154], "probabl": [5, 41, 90, 101, 107, 108, 109, 131, 132, 135, 137, 147, 157, 179], "problem": [2, 14, 17, 21, 28, 38, 39, 43, 48, 49, 55, 59, 63, 72, 73, 78, 83, 84, 86, 88, 90, 92, 94, 97, 98, 99, 101, 105, 106, 107, 115, 125, 130, 132, 133, 136, 138, 139, 140, 141, 142, 145, 150, 152, 153, 156, 157, 158, 159, 161, 162, 170, 171, 177, 185], "problemat": [96, 133, 154], "procedur": [2, 5, 17, 46, 76, 79, 96, 100, 101, 110, 117, 119, 122, 125, 126, 139, 150, 152, 156, 172, 179, 181], "process": [2, 5, 35, 57, 65, 71, 72, 73, 85, 94, 100, 102, 110, 116, 124, 125, 127, 133, 147, 151, 152, 182], "processor": 154, "produc": [28, 163], "product": [5, 96, 99, 101, 129, 133, 135, 150], "prof": [73, 84, 85, 136], "profession": 105, "profit": 68, "program": [2, 17, 28, 35, 46, 59, 70, 72, 80, 94, 124, 127, 177, 185], "progress": [70, 149, 156], "project": [35, 103, 153], "promot": [117, 136], "proof": 91, "proper": [100, 107, 139, 152], "properli": [85, 96, 105, 113, 122, 161], "proport": [59, 91, 95, 98, 99, 106, 137, 142, 145, 185], "propos": [59, 185], "protect": [73, 79, 84, 136, 150, 154], "provid": [2, 36, 72, 73, 79, 81, 84, 85, 101, 103, 105, 107, 111, 114, 115, 116, 120, 123, 127, 131, 133, 136, 137, 138, 140, 141, 142, 143, 146, 147, 152, 185], "proxi": [5, 142], "prune": 171, "pt": [101, 107], "public": 2, "publicli": 35, "publish": [35, 101, 107], "puerto": 84, "pure": [5, 127, 163], "purpos": [35, 84, 85, 94, 96, 98, 99, 100, 101, 102, 112, 115, 121, 122, 124, 125, 127, 131, 133, 137, 142, 151, 177], "purposefulli": 109, "put": [5, 32, 36, 81, 91, 102, 108, 133, 157], "px": [153, 156, 179], "py": [116, 133], "pyplot": [73, 81, 91, 94, 96, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 115, 121, 123, 125, 126, 131, 132, 133, 134, 136, 137, 138, 141, 142, 145, 146, 157, 161, 162, 163, 164], "python": [2, 5, 28, 35, 65, 70, 72, 73, 76, 86, 88, 101, 110, 116, 133], "python3": [116, 133], "python_script": 3, "pytorch": 5, "q": 100, "qcut": [105, 107], "qin": 94, "qualif": 5, "qualifi": 102, "qualit": [109, 110, 128, 134, 163], "qualiti": [5, 108, 141, 145, 152], "quantifi": [55, 73, 102, 108, 128, 134, 185], "quantil": [116, 132], "quantiletransform": [145, 185], "quantit": [72, 79, 115, 128, 134, 138], "quantiti": [2, 5, 84, 85, 105, 108], "quantiz": [105, 107], "quarter": [68, 100], "queri": [131, 133, 137], "question": [5, 79, 87, 89, 104, 128, 129, 130, 134, 135, 136, 137, 142, 156], "quick": [35, 70, 73, 104, 107, 116, 158], "quickli": [79, 103, 104, 107, 108, 117, 139, 141, 142, 153, 154, 163], "quit": [98, 99, 100, 102, 103, 105, 116, 141], "quiz": [9, 10, 11, 18, 19, 20, 29, 30, 35, 37, 40, 44, 54, 56, 58, 62, 64, 67, 156, 165, 167, 168, 169, 176, 178, 180], "quizz": 35, "quot": 100, "quotat": 100, "r": [27, 91, 97, 100, 101, 107, 108, 144, 145, 147], "r2": [17, 27, 91, 100, 107, 118, 123, 145, 147], "r2_score": 100, "r2_test_score_dummy_regressor": 91, "r2_train_score_dummy_regressor": 91, "race": [73, 79, 84, 85, 105, 136, 150, 152, 154], "race_": 136, "race_infrequent_sklearn": 136, "radial": 132, "radian": 28, "rais": [86, 88, 109, 110, 119, 121, 143, 146], "rand": 139, "randint": [108, 120, 124, 127], "randn": [99, 124, 127, 139], "random": [2, 10, 12, 13, 14, 16, 17, 36, 46, 50, 72, 79, 94, 98, 99, 100, 101, 102, 105, 107, 108, 110, 112, 113, 116, 118, 121, 122, 124, 125, 126, 127, 132, 139, 149, 150, 153, 156, 165, 173, 178, 179, 183], "random_forest": [115, 119], "random_st": [17, 36, 79, 81, 83, 85, 91, 92, 96, 97, 98, 99, 100, 101, 102, 108, 109, 110, 111, 112, 113, 115, 116, 117, 118, 119, 120, 121, 122, 123, 125, 126, 127, 131, 132, 133, 135, 137, 139, 141, 142, 145, 148, 149, 150, 152, 154, 155, 156, 157, 159, 161, 163, 177], "randomforestclassifi": [119, 125, 126], "randomforestregressor": [17, 108, 115, 117, 119, 121, 122], "randomized_search_result": [153, 154, 179], "randomizedsearchcv": [111, 117, 120, 149, 154, 156, 179], "randomizedsearchcvifittedrandomizedsearchcv": 154, "randomli": [17, 28, 96, 98, 101, 102, 108, 110, 119, 154], "randomsearchcv": 5, "randomst": [105, 107, 108, 127, 132, 139], "rang": [17, 39, 46, 59, 65, 79, 81, 94, 96, 99, 101, 103, 105, 106, 107, 108, 110, 132, 133, 140, 152, 153, 154, 156, 158, 160, 164, 174, 177, 179, 185], "rangeindex": [104, 105, 106, 107], "rank": [103, 117, 123, 185], "rank_test_scor": [150, 152, 153, 154], "rapid": 136, "rapidli": 150, "rare": [72, 73, 84, 85, 104, 119, 133], "rarest": 84, "rate": [16, 28, 80, 91, 95, 101, 105, 117, 142, 150, 153, 155], "rather": [2, 28, 33, 76, 79, 100, 102, 105, 106, 141, 142, 147, 154, 156], "ratio": 142, "ratio_unique_sampl": 110, "ravel": [104, 105, 134, 141], "raw": [85, 133, 137, 145, 185], "rbf": [92, 97, 131, 132, 137], "rcond": 133, "rdbu": [109, 132, 161], "rdbu_r": [131, 137, 141], "re": [133, 177, 185], "reach": [17, 81, 91, 95, 102, 113, 122, 150, 153, 161, 179], "reachabl": 145, "read": [2, 5, 73, 94, 105, 108, 145, 185], "read_csv": [17, 28, 46, 59, 63, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 93, 97, 98, 100, 103, 104, 105, 106, 109, 112, 119, 121, 128, 129, 130, 131, 133, 134, 135, 136, 137, 138, 140, 141, 142, 143, 144, 145, 146, 147, 148, 150, 151, 152, 153, 154, 155, 157, 158, 159, 160, 161, 162, 163, 164, 177, 179, 185], "readabl": [73, 150, 153, 179], "reader": [73, 87, 89, 94, 99, 110, 136, 139, 141, 145], "readi": 141, "readthedoc": 2, "real": [2, 48, 73, 79, 80, 84, 85, 99, 100, 101, 102, 128, 134, 142, 145, 162, 185], "realist": [99, 108, 130, 136, 141], "realli": [84, 100, 102, 105, 109, 137, 142], "reason": [2, 5, 17, 46, 57, 73, 75, 81, 84, 87, 89, 96, 100, 101, 105, 108, 115, 117, 127, 131, 132, 137, 142, 156, 161], "reassur": [83, 122, 152], "recal": [17, 26, 59, 81, 84, 91, 96, 101, 116, 118, 131, 137, 142, 151, 152, 154, 156, 185], "recall_scor": 142, "receiv": [36, 142], "recenc": [106, 142], "recogn": [84, 94], "recognit": 94, "recogniz": 151, "recommend": [5, 35, 70, 73, 84], "record": [24, 28, 48, 73, 80, 84, 101, 105, 106, 108], "recov": [5, 17, 94, 100, 113, 122, 133], "rectangular": 132, "recurs": 33, "red": [2, 26, 76, 101, 103, 109, 110, 115, 131, 132, 136, 137, 141, 150, 152, 161], "redefin": 140, "reduc": [16, 17, 39, 43, 46, 90, 94, 95, 116, 117, 118, 119, 121, 125, 133, 134], "reduct": [52, 108], "redund": [73, 101, 125, 129, 130, 135, 136, 139], "refer": [2, 13, 22, 33, 39, 57, 71, 72, 73, 80, 81, 86, 88, 91, 92, 93, 94, 95, 97, 98, 101, 102, 107, 109, 110, 111, 112, 113, 115, 116, 118, 119, 120, 121, 122, 128, 129, 131, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 150, 151, 157, 159, 160, 161, 162, 163, 164, 171, 183], "refin": [115, 173], "refit": [117, 148, 150, 152, 155], "reflect": [5, 101, 110, 119], "reg": [90, 104, 108], "regard": [2, 12, 13, 21, 23, 32, 33, 38, 41, 70, 72, 81, 84, 86, 88, 91, 92, 95, 97, 99, 102, 105, 106, 107, 109, 111, 112, 113, 114, 116, 117, 118, 119, 120, 121, 122, 123, 124, 127, 128, 129, 131, 133, 134, 135, 137, 138, 140, 141, 142, 143, 144, 145, 146, 147, 148, 155, 157, 159, 160, 161, 162, 163, 164], "regardless": [42, 103, 132, 133, 157], "regim": 97, "region": [73, 101, 102, 132, 137, 141, 150, 154], "regress": [5, 17, 21, 26, 37, 38, 39, 40, 41, 42, 43, 44, 48, 52, 57, 59, 65, 72, 79, 81, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 97, 98, 99, 101, 107, 110, 112, 115, 121, 124, 127, 129, 130, 131, 135, 136, 137, 141, 142, 144, 147, 149, 151, 156, 157, 160, 161, 164, 165, 170, 171, 172, 177, 185], "regressor": [17, 23, 27, 28, 39, 43, 46, 80, 81, 91, 95, 100, 101, 102, 110, 111, 114, 115, 116, 117, 118, 119, 120, 122, 123, 133, 139, 145, 171, 174, 177], "regressor_predict": 110, "regular": [26, 36, 38, 39, 43, 46, 108, 110, 132, 154, 162, 165, 183], "rel": [88, 93, 98, 101, 118, 126, 130, 136, 139, 142, 145], "relat": [2, 13, 17, 22, 33, 39, 55, 57, 71, 73, 85, 100, 103, 137, 139, 142, 150, 171, 182, 183], "relationship": [38, 73, 79, 84, 85, 103, 108, 132, 133, 136, 139, 140, 142, 150, 152, 154, 158], "relationship_": 136, "releas": 90, "relev": [2, 59, 79, 94, 102, 108, 142], "reli": [23, 73, 79, 81, 108, 109, 115, 126, 142], "reliabl": [5, 94, 141], "reload": [130, 136, 150, 154], "remain": [79, 80, 99, 100, 108, 110, 125, 133, 154], "remaind": [59, 84, 85, 87, 89, 119, 148, 150, 152, 154, 155], "remaining_misclassified_samples_idx": 109, "rememb": [27, 46, 73, 100, 103, 110, 113, 122, 129, 130, 135, 136, 137, 149, 156, 181, 185], "remind": 28, "remot": 2, "remov": [23, 34, 46, 73, 108, 116, 127, 129, 135, 150, 153, 185], "renam": [123, 150, 153, 154, 156, 179], "render": [79, 81, 82, 85, 90, 96, 101, 103, 109, 132, 133, 136, 138, 139, 142, 150, 152, 153, 154, 156, 157, 163, 164, 179], "reopen": 105, "repair": 136, "repeat": [2, 17, 28, 46, 57, 59, 76, 78, 83, 86, 88, 94, 95, 96, 101, 113, 122, 125, 131, 137, 143, 146, 149, 156, 159, 160, 162, 163, 164, 185], "repeatedkfold": 108, "repetit": [76, 94, 150], "replac": [14, 28, 46, 84, 104, 105, 107, 110, 132], "report": [91, 95, 96, 101, 102], "repositori": [35, 101, 107], "repres": [2, 5, 63, 65, 68, 72, 73, 79, 80, 81, 84, 85, 91, 99, 103, 104, 106, 108, 115, 116, 128, 133, 134, 139, 140, 141, 142, 145, 149, 156, 157], "represent": [68, 79, 81, 82, 84, 85, 87, 89, 90, 95, 96, 101, 106, 107, 109, 116, 119, 132, 133, 136, 138, 139, 142, 150, 152, 154, 157, 163, 164], "reproduc": [90, 105, 110, 152], "republ": 84, "request": [28, 84, 116, 133], "requir": [2, 5, 12, 17, 21, 28, 32, 35, 38, 39, 42, 46, 55, 59, 70, 72, 73, 76, 79, 81, 85, 90, 92, 97, 98, 105, 109, 116, 117, 132, 133, 147, 152, 154, 156, 161, 170, 177, 182, 185], "rerecord": 90, "rerun": [79, 81, 82, 85, 90, 96, 101, 109, 132, 133, 136, 138, 139, 142, 150, 152, 154, 157, 163, 164], "resampl": [14, 50, 72, 96, 105, 109, 130, 133, 136], "rescal": [91, 95, 102, 111, 113, 114, 115, 116, 117, 118, 120, 122, 123, 133, 145, 149, 151, 156], "reset_index": [17, 131, 137, 141], "reshap": [139, 145, 150], "resid": [101, 107], "residu": [115, 117, 145, 154], "residual_vs_predict": 145, "resist": 28, "resolut": [105, 154], "resolv": [95, 133], "resort": [101, 107], "resourc": [5, 35, 70, 73, 105, 117, 152], "respect": [2, 28, 41, 42, 84, 85, 87, 89, 92, 97, 98, 100, 102, 117, 119, 132, 133, 135, 140, 141, 145, 147], "respond": 103, "respons": 5, "response_method": [109, 131, 132, 137, 141, 157, 159, 161, 163], "rest": [108, 139, 163], "result": [2, 28, 65, 73, 76, 78, 79, 80, 81, 83, 84, 85, 88, 89, 91, 93, 94, 96, 98, 99, 100, 101, 102, 103, 110, 113, 114, 115, 117, 118, 119, 122, 123, 124, 125, 126, 127, 129, 132, 133, 135, 136, 137, 138, 139, 142, 145, 150, 151, 152, 154, 156, 157, 163, 165, 178, 179, 181, 185], "result_dummi": 91, "retail": 68, "retain": [90, 91, 130, 133, 136], "retbin": [105, 107], "retir": 73, "retriev": [92, 97, 101, 108, 133], "return": [17, 27, 28, 41, 43, 65, 76, 79, 80, 84, 86, 88, 103, 108, 110, 115, 128, 130, 133, 134, 136, 140, 141, 142, 149, 150, 151, 153, 154, 156, 179], "return_estim": [28, 46, 101, 107, 108, 114, 118, 123, 125, 130, 133, 136, 152, 185], "return_train_scor": [17, 28, 59, 91, 102, 133], "return_x_i": [91, 96, 99, 108, 111, 113, 114, 115, 116, 117, 118, 120, 122, 123, 149, 156, 179], "reus": [28, 128, 134, 152], "reveal": [73, 101], "revenu": [73, 83, 84, 103, 133], "revert": [101, 129, 135], "review": 99, "revolut": 5, "rf_1_tree": 17, "rho": 28, "rich": 5, "richer": [90, 139], "rico": 84, "ride": [1, 28, 165], "rider": 28, "ridg": [2, 39, 43, 46, 107, 108, 110, 131, 133, 137], "ridge_large_alpha": 133, "ridgecv": [28, 46, 107, 108, 133], "right": [2, 73, 74, 75, 79, 84, 86, 88, 98, 108, 113, 117, 122, 124, 127, 137, 142, 150, 152, 161], "rise": 108, "risk": [5, 117, 129, 135], "rl": [90, 104], "rna": [124, 127], "rnd_bin": 108, "rnd_num": 108, "rng": [105, 107, 108, 110, 115, 127, 139], "roadblock": 5, "robust": [43, 89, 101, 102, 108, 118, 133], "roc": [26, 142], "roccurvedisplai": 142, "rocket": 35, "role": 117, "roll": 28, "ronald": [101, 107], "roofmatl": 104, "roofstyl": 104, "room": [2, 48, 101, 107, 108], "rotat": [28, 73, 94, 103, 105], "rotation": 132, "rough": 152, "roughli": [78, 83, 110, 122, 140], "round": [76, 93, 98, 102, 109], "routin": [5, 17, 94, 185], "row": [2, 17, 59, 63, 73, 79, 80, 84, 85, 90, 101, 104, 107, 110, 129, 130, 133, 135, 136, 139, 140, 150, 151, 153, 154, 156, 158, 185], "rsplit": [150, 153, 154, 179], "rubric": [101, 107], "rule": [2, 79, 80, 118, 132, 133, 136, 137, 141, 157, 173], "run": [3, 5, 17, 35, 43, 65, 77, 82, 91, 95, 96, 108, 114, 116, 117, 123, 133, 150, 153, 154, 177, 179, 185], "rush": 102, "rv": 154, "rv_continuous_frozen": 154, "safe": 108, "saga": 133, "sai": [2, 59, 108, 131, 137, 150, 157], "said": [139, 145, 147], "sake": [80, 90, 93, 98, 117, 139, 152, 157, 177], "salari": [63, 107], "sale": [68, 72, 85, 136], "salecondit": [90, 104], "salepric": [46, 72, 90, 104, 133, 144, 145, 147, 177], "saletyp": [90, 104], "salt": 133, "salvador": 84, "same": [2, 17, 25, 28, 36, 39, 43, 46, 65, 73, 78, 79, 80, 81, 83, 84, 85, 89, 94, 95, 96, 98, 99, 101, 102, 103, 104, 108, 109, 110, 115, 116, 125, 128, 129, 130, 132, 133, 134, 135, 136, 138, 139, 140, 142, 145, 147, 153, 154, 162, 163, 177, 185], "sampl": [5, 13, 14, 15, 17, 19, 28, 41, 43, 48, 58, 60, 63, 73, 74, 75, 76, 79, 80, 81, 84, 85, 92, 93, 96, 97, 98, 99, 100, 101, 102, 104, 105, 106, 107, 108, 109, 110, 113, 115, 116, 117, 119, 122, 124, 127, 128, 130, 132, 133, 134, 136, 137, 139, 140, 141, 142, 147, 149, 150, 152, 154, 156, 157, 158, 160, 161, 162, 164, 165, 174, 183], "sample_weight": 109, "san": 107, "sandbox": [84, 179], "sarah": 5, "satur": 108, "save": [5, 185], "saw": [13, 28, 39, 46, 79, 81, 84, 85, 101, 102, 115, 117, 133, 138, 141, 143, 146, 150, 152, 153, 154, 157, 161, 162, 163, 171], "sc_x": 28, "scalabl": 135, "scalar": [128, 134], "scalarmapp": 163, "scale": [39, 43, 46, 59, 71, 72, 81, 84, 85, 92, 94, 97, 101, 110, 116, 130, 132, 136, 145, 150, 151, 154, 156, 177, 185], "scale_": 81, "scaled_ridg": 133, "scaler": [2, 80, 81, 90, 151, 156, 179, 181, 185], "scan": [2, 5, 151], "scatter": [28, 75, 103, 107, 110, 112, 115, 121, 132, 136, 158, 160, 162, 163, 164], "scatter_kw": 108, "scatter_kwarg": 145, "scatterplot": [73, 81, 107, 109, 110, 115, 121, 131, 134, 137, 138, 139, 140, 141, 153, 157, 158, 161, 162, 163, 164], "scenario": [81, 85, 100, 133, 141, 145], "schemat": [150, 152], "scheme": [92, 97, 100, 109, 162], "school": [73, 84, 94, 136], "scienc": [2, 5, 35, 94, 105], "scientif": [35, 119], "scientist": [43, 96], "scikit": [2, 12, 13, 21, 22, 23, 26, 27, 32, 33, 36, 37, 38, 39, 41, 43, 55, 57, 59, 63, 65, 67, 70, 71, 76, 77, 79, 81, 82, 84, 85, 86, 87, 88, 89, 93, 94, 98, 99, 100, 101, 107, 109, 112, 114, 116, 119, 121, 123, 124, 127, 132, 133, 139, 141, 142, 143, 145, 146, 147, 150, 152, 154, 159, 163, 165, 170, 171, 173, 174, 180, 182, 183, 185], "scipi": [63, 103, 117, 120, 154], "scope": [17, 73, 87, 89, 103, 139, 141, 145, 147], "score": [2, 17, 26, 27, 28, 36, 46, 57, 59, 65, 72, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 97, 98, 101, 102, 107, 108, 113, 114, 115, 116, 117, 118, 120, 122, 123, 124, 125, 126, 127, 130, 131, 133, 135, 136, 137, 139, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 163, 175, 177, 179, 185], "score_bin": 153, "score_func": [125, 127], "score_nam": [95, 97], "score_tim": [76, 84, 85, 97, 101, 115, 116, 125, 146, 152], "score_typ": [95, 97], "scorer": [143, 144, 146, 147], "scores_": 125, "scores_bagged_tre": 119, "scores_random_forest": 119, "scores_tre": 119, "scotland": 84, "screen": [5, 7, 8, 45, 47, 49, 51, 53, 61, 66, 166], "screenporch": [46, 72, 90, 104, 177], "script": 90, "seaborn": [63, 73, 74, 75, 81, 103, 105, 106, 107, 108, 109, 110, 115, 121, 131, 134, 137, 138, 139, 140, 141, 150, 153, 157, 158, 161, 162, 163, 164], "search": [2, 46, 95, 96, 114, 116, 117, 118, 119, 120, 123, 133, 138, 148, 149, 152, 155, 156, 161, 165, 175, 177, 178, 179, 182, 183, 185], "search_cv": 117, "search_cv_result": 123, "second": [28, 36, 72, 81, 84, 87, 89, 93, 98, 99, 105, 109, 115, 116, 132, 145, 154, 161, 163], "section": [2, 76, 79, 84, 85, 91, 92, 95, 97, 101, 102, 109, 111, 112, 113, 115, 116, 117, 118, 119, 120, 121, 122, 128, 129, 131, 133, 134, 135, 137, 138, 140, 141, 142, 143, 144, 145, 146, 147, 150, 154, 157, 159, 160, 161, 162, 163, 164], "sector": 68, "see": [2, 7, 8, 38, 45, 47, 49, 51, 53, 61, 66, 73, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 91, 94, 95, 97, 98, 99, 100, 103, 104, 105, 106, 107, 108, 109, 110, 115, 116, 118, 120, 121, 122, 123, 125, 127, 132, 133, 135, 136, 138, 139, 140, 141, 142, 145, 147, 151, 152, 153, 154, 156, 157, 158, 159, 161, 162, 163, 166], "seed": 110, "seek": [5, 33, 117, 118, 142], "seem": [72, 73, 76, 79, 82, 83, 87, 89, 97, 103, 107, 108, 117, 119, 122, 133, 134, 145, 151, 157], "seen": [2, 24, 76, 78, 80, 81, 83, 84, 101, 113, 122, 130, 136, 137, 140, 142, 150, 151, 152, 160, 164], "segment": [73, 132], "seldom": 80, "select": [2, 5, 12, 14, 15, 16, 17, 21, 22, 23, 24, 25, 26, 27, 28, 32, 33, 34, 38, 41, 42, 43, 46, 48, 50, 52, 59, 60, 63, 65, 68, 72, 74, 75, 76, 78, 79, 81, 83, 86, 87, 88, 89, 96, 99, 101, 102, 103, 108, 110, 115, 117, 119, 123, 124, 127, 128, 130, 134, 136, 138, 139, 141, 150, 151, 152, 153, 156, 158, 170, 172, 173, 174, 175, 177, 179, 181, 185], "select_dtyp": [72, 104, 130, 136, 144, 145, 147], "selectfrommodel": 126, "selectkbest": [124, 125, 127], "selector": [33, 84, 85, 86, 87, 88, 89, 124, 127, 136, 148, 150, 152, 154, 155], "self": [35, 73, 79, 80, 84, 85, 96, 136, 150, 154], "sell": 102, "seller": 102, "send": 85, "sender": 2, "sens": [94, 109, 164], "sensit": [16, 137, 142, 153, 156, 171], "sensiv": 50, "sensor": [28, 73, 102, 105], "sent": 5, "sepal": 2, "separ": [2, 28, 41, 43, 46, 63, 73, 75, 78, 79, 83, 85, 98, 104, 105, 106, 132, 136, 137, 141, 152, 157, 161, 163, 173, 177], "septemb": 105, "seq": [124, 127], "sequenc": [84, 100, 115, 148, 155], "sequenti": [13, 15, 52, 81, 116, 117, 153, 163, 179], "seri": [5, 91, 93, 94, 98, 100, 108, 110, 115, 137, 141, 157], "serv": [73, 79, 84, 87, 89, 105, 136, 142, 150, 154], "servic": [2, 35, 85, 136], "session": [152, 154], "set": [0, 5, 17, 25, 28, 34, 36, 38, 39, 41, 43, 46, 52, 55, 57, 59, 60, 63, 65, 72, 73, 76, 78, 79, 80, 81, 83, 84, 85, 86, 88, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 101, 102, 105, 107, 108, 109, 110, 111, 112, 113, 114, 115, 117, 118, 119, 120, 121, 122, 123, 124, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 145, 146, 147, 148, 149, 152, 154, 155, 156, 157, 158, 159, 161, 162, 163, 165, 175, 177, 179, 180, 181, 182, 185], "set_index": [123, 131, 137, 141, 147], "set_output": [81, 84, 129, 133, 135], "set_palett": 153, "set_param": [135, 137, 151, 155, 181, 185], "set_size_inch": 158, "set_titl": [104, 115, 139, 140, 142, 163], "set_xlabel": [105, 145, 163], "set_xlim": 115, "set_xscal": 153, "set_xtick": 103, "set_xticklabel": 103, "set_ylabel": [132, 145, 163], "set_yscal": 153, "set_ytick": [103, 108], "set_yticklabel": [103, 108], "setosa": 2, "settabl": 157, "sever": [2, 12, 15, 22, 65, 76, 94, 96, 101, 102, 108, 109, 110, 113, 115, 116, 118, 119, 122, 128, 132, 134, 137, 139, 154, 163], "sex": [73, 79, 84, 85, 136, 150, 152, 154], "sex_": 136, "shade": 110, "shall": [108, 110], "shallow": [16, 109, 115, 117, 161], "shape": [2, 41, 42, 73, 79, 80, 84, 104, 105, 107, 108, 109, 110, 132, 139, 141, 145, 157, 162], "share": [5, 110], "sharei": 163, "shed": [90, 104], "shift": [5, 81, 137], "shock": 106, "short": [5, 33, 79, 107, 118, 158], "shortcom": 5, "shorten": 150, "shorten_param": [150, 153, 154, 179], "shorter": 140, "shortest": 164, "shorthand": 81, "should": [5, 17, 22, 28, 33, 34, 43, 46, 68, 72, 73, 80, 81, 85, 94, 96, 98, 99, 100, 101, 102, 104, 105, 106, 108, 109, 114, 115, 117, 118, 122, 123, 124, 125, 127, 128, 133, 134, 137, 139, 142, 143, 146, 147, 148, 150, 151, 152, 155, 156, 157, 161, 162, 175, 177, 179, 182, 183], "show": [22, 38, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 90, 95, 96, 99, 101, 102, 103, 104, 107, 109, 110, 117, 118, 119, 122, 126, 128, 129, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 146, 150, 151, 152, 153, 154, 156, 157, 159, 162, 163, 164, 170, 179, 182], "shown": [72, 73, 91, 101, 115, 132, 137, 141, 145, 150, 152, 159, 163], "shrink": [2, 43, 133, 137], "shrunk": 133, "shuffl": [17, 36, 79, 94, 96, 99, 100, 101, 108, 114, 123, 142, 145], "shufflesplit": [28, 91, 92, 93, 95, 97, 98, 99, 100, 101, 102, 133], "side": [80, 91, 119, 122, 152, 162], "sigmoid": [137, 141], "sign": [113, 122, 129, 135], "signific": [89, 102, 108, 117, 125, 132, 133, 152], "significantli": [85, 102, 108, 150], "silenc": [84, 150, 152, 154], "similar": [2, 43, 46, 59, 65, 77, 81, 82, 84, 87, 89, 91, 99, 103, 104, 110, 125, 129, 130, 132, 133, 135, 136, 139, 142, 147, 150, 151, 152, 153, 154, 157, 185], "similarli": [73, 91, 92, 93, 95, 97, 98, 100, 103, 112, 115, 117, 119, 121, 133, 141, 142, 154, 164], "simpl": [2, 5, 46, 73, 78, 83, 92, 97, 102, 104, 109, 110, 118, 119, 131, 133, 137, 139, 140, 141, 151, 157, 163, 165], "simpleimput": [90, 104], "simpleimputersimpleimput": 90, "simpler": [5, 13, 46, 85, 137], "simplest": [78, 83], "simpli": [110, 163], "simplic": [80, 90, 93, 98, 139, 157, 177], "simplifi": [28, 46, 100, 101, 125, 141, 156, 158, 177], "simplist": 97, "simultan": 15, "sin": 28, "sinc": [2, 73, 76, 81, 92, 96, 97, 98, 99, 101, 102, 105, 106, 107, 108, 109, 110, 115, 116, 117, 119, 125, 133, 136, 138, 141, 142, 145, 149, 156, 157, 158, 162, 163], "sine": 28, "singl": [2, 14, 15, 17, 23, 24, 26, 27, 28, 34, 41, 42, 43, 46, 48, 52, 59, 63, 65, 68, 72, 73, 76, 79, 81, 84, 85, 90, 93, 96, 98, 99, 100, 101, 108, 110, 115, 118, 119, 120, 125, 126, 128, 129, 132, 134, 135, 139, 142, 145, 151, 152, 157, 159, 161, 163, 172, 173, 174, 175, 177, 179, 181, 185], "site": [116, 133], "situat": 154, "six": 99, "size": [2, 48, 55, 57, 58, 84, 91, 92, 97, 105, 107, 109, 110, 115, 117, 127, 132, 157, 165], "skill": [12, 21, 32, 38, 55, 70, 170, 182], "sklearn": [17, 28, 43, 46, 59, 72, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 131, 132, 133, 135, 136, 137, 138, 139, 141, 142, 143, 145, 146, 147, 148, 149, 150, 151, 152, 154, 155, 156, 157, 159, 161, 162, 163, 164, 177, 179, 181, 185], "slice": 28, "slide": [7, 8, 45, 47, 49, 51, 53, 61, 66, 103, 131, 132, 137, 141, 153, 156, 166], "slightli": [2, 15, 73, 82, 84, 85, 91, 101, 105, 109, 110, 117, 118, 119, 122, 137], "slope": [2, 28, 105, 140], "slow": [52, 87, 89, 117], "slower": [113, 122, 135, 152], "small": [17, 39, 72, 75, 76, 85, 94, 97, 99, 101, 102, 108, 110, 115, 116, 117, 129, 131, 133, 135, 137, 139, 148, 150, 155, 179], "smaller": [76, 101, 116, 117, 122, 135, 137, 147, 174], "smallest": [101, 107, 116, 133], "smile": 145, "smooth": [110, 132, 137], "smoother": [105, 110], "sn": [73, 81, 105, 106, 107, 108, 109, 110, 115, 121, 131, 134, 137, 138, 139, 140, 141, 150, 153, 157, 158, 161, 162, 163, 164], "snippet": [110, 130, 136, 179], "so": [2, 5, 13, 28, 46, 65, 75, 81, 83, 84, 86, 88, 90, 91, 92, 97, 98, 99, 100, 101, 102, 104, 105, 108, 109, 114, 115, 117, 123, 129, 132, 133, 135, 137, 138, 139, 142, 143, 146, 150, 151, 152, 156, 177], "societi": 5, "socioeconom": 24, "soft": [132, 141], "softer": [103, 141], "softmax": 141, "softwar": [5, 35, 80], "sole": [72, 127, 145], "solid": [5, 115], "solut": [5, 9, 10, 11, 17, 18, 29, 30, 31, 35, 37, 40, 44, 58, 62, 64, 67, 81, 84, 100, 124, 125, 138, 150, 165, 167, 176, 178, 180], "solv": [2, 5, 28, 43, 59, 73, 79, 85, 94, 97, 99, 101, 105, 106, 133, 138, 139, 140, 141, 145, 162, 185], "solver": [81, 133, 147], "some": [2, 5, 17, 21, 22, 28, 32, 35, 46, 59, 63, 65, 70, 72, 73, 74, 75, 76, 78, 79, 80, 81, 83, 84, 85, 86, 88, 90, 94, 95, 96, 98, 99, 100, 101, 102, 103, 104, 105, 107, 108, 109, 110, 112, 114, 115, 116, 117, 118, 119, 121, 123, 124, 126, 127, 132, 133, 136, 137, 139, 140, 141, 145, 147, 150, 152, 153, 154, 156, 157, 161, 162, 164, 177, 185], "somehow": 108, "someon": [2, 73], "someth": [2, 87, 89, 91, 151], "sometim": [2, 5, 57, 85, 98, 142, 145, 150, 152, 154], "somewhat": [73, 110], "sort": 139, "sort_index": [84, 100], "sort_valu": [117, 120, 150, 154, 156], "soup": 105, "sourc": [0, 102, 117], "south": [84, 108], "space": [103, 110, 112, 114, 115, 121, 123, 127, 129, 132, 133, 135, 136, 137, 141, 157, 159, 162, 163], "spaciou": 48, "spam": 2, "span": [81, 133], "spars": [84, 87, 89, 101, 107], "sparse_output": [84, 87, 89], "spatial": [101, 103, 107], "spearman": 103, "spearmanr": 103, "speci": [2, 17, 74, 75, 109, 131, 137, 141, 157, 158, 159, 161, 163, 185], "special": [73, 99, 100, 105], "specialti": [85, 136], "specif": [2, 12, 21, 22, 36, 63, 80, 81, 86, 87, 88, 89, 94, 101, 102, 105, 107, 108, 109, 115, 116, 119, 125, 133, 142, 150, 151, 153, 157, 161, 162, 177], "specifi": [79, 84, 85, 104, 105, 119, 140, 143, 146, 152, 154, 182], "speech": 5, "speed": [9, 28, 33, 34, 43, 105, 108, 125, 152, 154, 165], "spell": 151, "spend": [5, 105, 117], "spent": [105, 125], "spline": [132, 139], "spline_regress": 139, "splinetransform": [46, 132, 139], "splinetransformersplinetransform": [132, 139], "split": [5, 14, 17, 22, 28, 36, 41, 43, 65, 73, 76, 78, 83, 84, 85, 91, 96, 99, 100, 101, 104, 108, 111, 115, 116, 117, 119, 120, 122, 124, 127, 142, 143, 145, 146, 150, 152, 154, 157, 159, 161, 162, 163, 172, 173], "split0_test_scor": [150, 153], "split1_test_scor": [150, 153], "split2_test_scor": 153, "split3_test_scor": 153, "split4_test_scor": 153, "splitter": 91, "spot": [107, 133], "spous": [73, 79, 84, 85, 136, 150, 154], "spread": [135, 137, 153, 179], "spuriou": 17, "sqrt": 119, "squar": [2, 27, 57, 105, 133, 134, 138, 139, 145, 147], "squared_error": 147, "squareform": 103, "stabil": [76, 108], "stabl": [2, 108, 110, 133, 177, 185], "stack": 13, "stackoverflow": 5, "staff": 5, "stage": [96, 99, 116, 124, 125, 127, 139, 142, 163], "stai": [5, 39, 43, 110, 133], "stand": [2, 80, 110, 179], "standard": [2, 28, 46, 73, 76, 81, 85, 86, 88, 93, 96, 98, 101, 102, 108, 114, 123, 133, 142, 150, 152], "standardscal": [2, 28, 46, 59, 65, 72, 76, 81, 85, 87, 89, 90, 92, 94, 97, 98, 99, 107, 108, 130, 131, 132, 136, 137, 141, 149, 151, 156, 177, 179, 181, 185], "standardscaler__with_mean": 156, "standardscaler__with_std": 156, "standardscalerifittedstandardscal": 81, "standardscalerstandardscal": [81, 85, 90, 132, 136], "standpoint": 5, "start": [5, 46, 73, 77, 78, 79, 81, 82, 83, 84, 85, 87, 89, 91, 94, 95, 98, 99, 100, 101, 105, 106, 115, 117, 118, 124, 125, 127, 129, 130, 131, 132, 133, 135, 136, 137, 140, 141, 142, 145, 147, 149, 151, 156, 157, 158], "stat": [103, 117, 120, 154], "state": [5, 13, 59, 73, 79, 80, 81, 84, 85, 107, 136, 150, 154], "statement": [15, 17, 26, 28, 41, 72, 179, 185], "static": 35, "statist": [5, 17, 55, 73, 75, 80, 81, 84, 87, 89, 101, 103, 107, 109, 110, 124, 127, 142, 185], "statistician": 145, "statlib": [101, 107], "statu": [3, 24, 73, 79, 84, 85, 136, 150, 152, 154], "status_": 136, "status_infrequent_sklearn": 136, "std": [76, 79, 81, 84, 85, 87, 88, 89, 90, 91, 94, 96, 97, 99, 100, 101, 107, 108, 110, 115, 116, 118, 119, 123, 129, 133, 135, 146, 147, 151, 152], "std_display_styl": [95, 97, 102, 122, 135], "std_fit_tim": [150, 153], "std_score_tim": [150, 153], "std_test_error": [117, 120], "std_test_scor": [117, 120, 150, 152, 153, 154], "steadi": 52, "steep": 137, "stem": [133, 145], "step": [35, 46, 59, 73, 79, 80, 81, 85, 90, 117, 119, 125, 126, 129, 130, 131, 133, 135, 136, 137, 139, 141, 144, 147, 150, 151, 152, 154, 185], "stick": 2, "still": [57, 76, 81, 84, 85, 89, 100, 102, 107, 108, 109, 111, 120, 121, 131, 132, 133, 137, 139, 145, 153, 154, 157], "stimuli": 105, "stochast": 154, "stock": 100, "stone": 35, "stop": [13, 28, 86, 88, 91, 98, 105, 113, 114, 117, 122, 123], "store": [2, 17, 28, 73, 79, 81, 91, 93, 98, 99, 101, 104, 105, 106, 110, 118, 125, 126, 132, 133, 138, 149, 150, 156, 185], "store_cv_result": 133, "str": 105, "straight": [42, 85, 132, 137, 139, 141, 162], "straightforward": [73, 110], "strategi": [12, 13, 17, 21, 22, 23, 24, 28, 59, 76, 78, 80, 83, 86, 87, 88, 89, 90, 91, 93, 94, 96, 98, 99, 100, 101, 104, 109, 110, 113, 116, 119, 122, 129, 130, 134, 135, 136, 138, 142, 145, 150, 152, 182, 185], "stratif": [19, 165], "stratifi": [24, 93, 98, 99, 185], "stratified_dummi": 98, "stratifiedkfold": [99, 143, 146], "street": [90, 104], "strength": [43, 46, 131, 133, 137, 154], "strictli": [27, 94, 185], "strike": 106, "string": [68, 79, 84, 85, 86, 88, 101, 104, 143, 146, 147, 152, 185], "string_data": 104, "strong": [35, 103, 108, 137, 156], "stronger": [26, 105, 110, 137, 156], "strongest": 98, "strongli": [5, 108], "structur": [5, 57, 70, 73, 81, 84, 85, 94, 100, 103, 117, 145, 156, 157, 162, 171], "student": [5, 35], "studi": [48, 59, 73, 92, 94, 97, 101, 185], "sub": [49, 96, 97, 157], "subdivid": [2, 157], "subfigur": 75, "subject": [5, 73, 142], "subplot": [103, 104, 108, 115, 132, 133, 136, 142, 145, 157, 161, 162, 163], "subplots_adjust": [104, 105, 107, 108, 163], "subsampl": [15, 107, 119, 124, 127], "subsequ": [28, 70, 94, 102, 114, 117, 123, 124, 127, 129, 130, 132, 133, 135, 136, 181, 182], "subset": [2, 16, 41, 43, 46, 70, 73, 76, 79, 80, 85, 90, 101, 105, 107, 117, 119, 122, 124, 125, 126, 127, 148, 155, 157, 158, 161, 177], "substanti": [17, 177, 185], "substitut": 147, "subtract": [73, 81], "succeed": 127, "success": [80, 81, 115, 163], "successfulli": [39, 98, 115, 132], "suffer": [57, 81, 106], "suffic": [5, 117], "suffici": [126, 132], "suffix": 150, "suffl": 99, "suganthan": 94, "suggest": 2, "suit": [5, 87, 89, 94, 132, 145, 171], "suitabl": 24, "sum": [2, 39, 41, 57, 73, 80, 82, 115, 134, 136, 141, 142], "sum_i": 91, "summar": [2, 5, 101, 109, 117, 119], "summari": 133, "superimpos": [160, 164], "superior": 157, "supervis": [5, 48, 101, 172], "support": [2, 5, 73, 79, 81, 84, 85, 87, 89, 92, 97, 132, 136, 139, 143, 146, 150, 154], "suppos": [24, 84, 94], "suptitl": [81, 132, 142, 145, 163], "sure": [5, 17, 59, 79, 81, 84, 87, 89, 99, 102, 114, 123], "surfac": [2, 28, 102], "surgeri": 5, "surpris": [17, 83, 94, 99, 100, 124, 127, 157, 163], "surprisingli": [100, 101, 107, 127], "surrog": 147, "survei": 63, "suspect": 24, "svc": [92, 96, 97], "svc__gamma": [92, 97], "svcsvc": 96, "svg": 0, "svm": [92, 96, 97, 139], "svr": 139, "svrinot": 139, "swap": 125, "swaplevel": [125, 126], "sweet": 133, "swim": 104, "sy": [76, 85, 102, 118, 150, 154], "symbol": [84, 100], "symlog": 133, "symmetr": [103, 117, 133, 145, 161], "syntax": [144, 147], "synthet": [95, 110, 115, 125, 130, 132, 136, 142, 160, 161, 162, 164], "system": [5, 73, 94], "systemat": [24, 50, 57, 72, 97, 145, 151], "t": [2, 5, 72, 77, 82, 84, 92, 94, 97, 100, 101, 102, 103, 106, 108, 116, 123, 124, 127, 133, 142, 145, 146, 150, 185], "tab": [109, 110, 115, 131, 132, 136, 137, 141, 142, 157, 161, 162, 163], "tab10": [157, 163], "tab10_norm": [157, 163], "tabl": [70, 73, 117, 119, 150], "tabular": [5, 63, 70, 73, 80, 85, 165], "tackl": [33, 57, 115, 126, 130, 136], "tag": [101, 128, 134], "tail": [104, 105, 107, 141], "taiwan": 84, "take": [2, 15, 28, 46, 72, 73, 76, 79, 80, 81, 85, 89, 91, 93, 96, 98, 101, 104, 105, 106, 107, 110, 128, 134, 140, 142, 150, 153, 154, 157, 165, 177, 179], "taken": [84, 108, 119, 140], "talk": 102, "tang": 94, "tangent": 28, "target": [5, 17, 28, 38, 39, 42, 46, 48, 57, 59, 63, 65, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 154, 155, 156, 157, 158, 159, 163, 177, 185], "target_bootstrap": 110, "target_bootstrap_sampl": 110, "target_clf_column": 161, "target_column": [73, 109, 131, 137, 141, 157, 158, 159, 163], "target_gauss": 132, "target_moon": 132, "target_nam": [17, 28, 46, 59, 72, 76, 78, 80, 81, 83, 84, 85, 86, 87, 88, 89, 90, 103, 104, 105, 112, 119, 121, 128, 129, 132, 133, 134, 135, 138, 140, 148, 150, 151, 152, 154, 155, 160, 161, 162, 164, 177, 185], "target_predict": [80, 100, 101, 109, 120, 121, 128, 134, 139, 142, 145, 161, 162], "target_predicted_linear_regress": 164, "target_predicted_tre": 164, "target_proba_predict": 142, "target_reg_column": 161, "target_rid": 105, "target_test": [28, 79, 80, 81, 82, 83, 85, 99, 100, 101, 111, 112, 113, 117, 120, 121, 122, 127, 131, 137, 141, 142, 145, 148, 149, 150, 152, 154, 155, 156, 157, 159, 163], "target_test_predict": 115, "target_test_predicted_residu": 115, "target_test_subset": 28, "target_train": [28, 79, 81, 83, 85, 99, 100, 101, 110, 111, 112, 113, 115, 117, 120, 121, 122, 127, 131, 137, 141, 142, 145, 148, 149, 150, 152, 154, 155, 156, 157, 159, 160, 162, 163, 164], "target_train_hug": 110, "target_train_predict": 115, "target_train_predicted_residu": 115, "target_tru": 115, "target_true_residu": 115, "target_xor": 132, "targetencod": [87, 89], "task": [5, 41, 72, 73, 79, 90, 94, 101, 103, 106, 132, 141, 172], "teach": [35, 49], "tealros": 156, "team": 5, "teas": 108, "tech": [73, 79, 84, 85, 136, 150, 154], "technic": [2, 12, 21, 32, 35, 38, 55, 70, 80, 170, 182], "techniqu": [12, 32, 73, 94, 99, 108, 132], "technolog": [5, 94], "technologi": 5, "tell": [2, 73, 102, 108], "temperatur": 102, "template_nam": 100, "tempor": 105, "ten": [124, 127], "tend": [15, 28, 94, 118, 132, 133, 137, 145, 152], "tensorflow": 5, "term": [28, 73, 80, 81, 91, 100, 101, 110, 112, 115, 116, 119, 121, 125, 130, 136, 140, 157, 162], "terminologi": 2, "test": [5, 17, 25, 28, 33, 34, 36, 43, 46, 52, 55, 57, 58, 59, 65, 71, 72, 76, 77, 78, 81, 82, 83, 84, 85, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 108, 110, 111, 112, 113, 114, 115, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 130, 131, 133, 135, 136, 137, 141, 142, 145, 148, 150, 151, 152, 154, 155, 156, 159, 162, 163, 165, 175, 177, 179, 185], "test_accuraci": 146, "test_balanced_accuraci": 146, "test_cv_count": 99, "test_error": [101, 133], "test_idx": 99, "test_index": 99, "test_indic": 28, "test_neg_mean_absolute_error": 147, "test_neg_mean_squared_error": 147, "test_penguin": 141, "test_penguin_1": 157, "test_penguin_2": 157, "test_penguin_3": 157, "test_r2": 147, "test_scor": [28, 76, 84, 85, 87, 88, 89, 90, 91, 94, 96, 97, 98, 99, 100, 101, 102, 107, 115, 116, 118, 123, 125, 126, 127, 133, 135, 136, 151, 152, 155, 157, 163], "test_score_complex_lr": 136, "test_score_dummy_stratifi": 98, "test_score_dummy_uniform": 98, "test_score_interact": 136, "test_score_logistic_regress": 98, "test_score_lr": 136, "test_score_most_frequ": 98, "test_score_nest": 96, "test_score_no_shuffl": 94, "test_score_not_nest": 96, "test_score_with_shuffl": 94, "test_siz": [79, 91, 95, 98, 101, 102, 111, 113, 120, 122, 131, 137, 142, 152], "text": [5, 35, 48, 105, 132, 140], "thailand": 84, "than": [2, 5, 17, 26, 27, 28, 33, 35, 42, 46, 52, 59, 72, 73, 76, 79, 80, 81, 82, 83, 84, 85, 88, 89, 91, 93, 94, 96, 97, 98, 99, 100, 101, 102, 104, 105, 108, 109, 110, 115, 116, 117, 118, 119, 122, 124, 126, 127, 128, 129, 130, 132, 133, 134, 135, 136, 137, 141, 142, 145, 147, 149, 150, 152, 153, 154, 156, 157, 160, 161, 164, 174, 177, 179, 185], "thank": [5, 132, 152], "thei": [2, 5, 28, 46, 55, 65, 73, 77, 79, 81, 82, 84, 85, 87, 89, 90, 98, 99, 102, 104, 107, 108, 110, 115, 116, 117, 119, 132, 133, 139, 142, 143, 145, 146, 151, 157, 162, 164, 171, 177, 182], "them": [2, 5, 12, 59, 73, 75, 76, 81, 84, 85, 91, 93, 96, 98, 101, 108, 110, 112, 116, 117, 118, 121, 129, 132, 133, 135, 139, 141, 142, 143, 145, 146, 150, 151, 152, 156, 160, 163, 164, 177, 185], "themselv": 76, "theoret": 95, "theori": [5, 109], "therefor": [17, 28, 72, 73, 76, 81, 83, 85, 91, 94, 95, 96, 98, 101, 102, 107, 109, 115, 116, 117, 119, 125, 126, 127, 133, 134, 135, 139, 141, 142, 145, 147, 152, 156, 161], "thesi": 94, "thi": [12, 13, 17, 21, 22, 26, 28, 32, 33, 35, 38, 39, 42, 46, 48, 49, 55, 57, 59, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 170, 171, 174, 177, 179, 182, 183, 185], "thing": [5, 73, 84, 85, 90, 100, 139, 150], "think": [5, 74, 75, 105, 129, 135, 137], "third": [28, 115, 132, 135], "those": [2, 14, 27, 55, 79, 91, 93, 98, 102, 110, 116, 133, 139, 145, 150, 152, 156, 163, 174, 181, 185], "though": [5, 98, 116, 144, 147], "thousand": [48, 101, 107, 108], "three": [2, 76, 98, 99, 102, 107, 108, 109, 112, 121, 132, 157, 158], "threshold": [5, 14, 26, 73, 90, 107, 157, 162, 163, 173], "through": [12, 21, 28, 32, 35, 38, 55, 70, 92, 97, 102, 108, 114, 123, 133, 140, 141, 143, 146, 150, 170, 179, 182], "thu": [2, 5, 28, 36, 60, 79, 81, 85, 92, 94, 96, 97, 100, 101, 102, 103, 105, 106, 107, 108, 109, 113, 116, 117, 118, 119, 122, 124, 127, 132, 133, 140, 141, 145, 150, 151, 152, 153, 156, 157, 161, 162, 177], "ti": 2, "tibshirani": 5, "tick": 153, "ticktext": 103, "tickval": 103, "tight_layout": 103, "time": [2, 17, 28, 46, 65, 72, 73, 76, 79, 81, 84, 85, 86, 87, 88, 89, 90, 94, 96, 99, 100, 101, 102, 105, 106, 108, 110, 113, 115, 116, 117, 118, 119, 122, 125, 130, 133, 136, 137, 141, 142, 143, 146, 150, 153, 154, 157, 159, 163, 165, 172, 177], "time_slic": 28, "timeseriessplit": 100, "timestamp": 105, "tire": 28, "titl": [2, 91, 94, 95, 96, 97, 98, 99, 100, 101, 102, 104, 105, 106, 107, 108, 109, 110, 115, 122, 123, 125, 126, 131, 132, 133, 135, 137, 138, 139, 140, 141, 142, 146, 153, 157, 161, 162, 163, 164], "tn": 142, "to_csv": 154, "to_list": 103, "to_numpi": [109, 110, 121, 142], "to_period": 100, "tobago": 84, "todai": 5, "togeth": [12, 13, 67, 72, 80, 81, 88, 93, 94, 96, 98, 109, 115, 165], "toggl": [7, 8, 45, 47, 49, 51, 53, 61, 66, 166], "toi": [99, 139], "tolist": [104, 130, 136, 157, 163], "too": [2, 5, 60, 80, 90, 96, 101, 102, 113, 116, 117, 119, 122, 130, 132, 133, 135, 136, 137, 150, 156, 161, 179], "tool": [33, 35, 76, 85, 100, 103, 110, 141, 152, 154], "top": [35, 73, 142, 153, 154, 160, 161, 164], "topic": 2, "tot": 100, "total": [76, 85, 94, 100, 101, 102, 104, 105, 106, 107, 108, 118, 130, 135, 136, 142, 150, 154, 156], "totalbsmtsf": [46, 72, 104, 177], "totrmsabvgrd": [46, 72, 104, 177], "touch": 5, "toward": [2, 43, 108, 125, 133, 137, 141], "tp": 142, "tpr": 142, "tr": 94, "traceback": [86, 88], "track": 105, "trade": [55, 57, 76, 131, 132, 137, 161, 165, 170, 172], "tradeoff": [52, 57, 102], "tradit": 17, "train": [5, 14, 15, 16, 17, 24, 25, 28, 33, 34, 36, 39, 41, 43, 46, 50, 52, 55, 57, 58, 59, 60, 65, 68, 70, 71, 73, 76, 77, 78, 81, 82, 83, 84, 85, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 103, 105, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 133, 135, 136, 137, 138, 139, 141, 142, 145, 147, 148, 149, 150, 151, 152, 154, 155, 156, 157, 159, 160, 161, 162, 163, 164, 165, 174, 175, 177, 182, 185], "train_cv_count": 99, "train_dataset": 108, "train_error": 133, "train_idx": 99, "train_index": 99, "train_indic": 28, "train_scor": [17, 28, 91, 102, 133], "train_siz": [95, 97, 148, 155], "train_test_split": [78, 79, 81, 83, 85, 100, 101, 108, 111, 112, 113, 117, 120, 121, 122, 124, 127, 131, 137, 141, 142, 145, 148, 149, 150, 152, 154, 155, 156, 157, 159, 163], "transform": [5, 28, 39, 42, 65, 68, 80, 81, 84, 85, 90, 93, 96, 98, 101, 102, 105, 110, 116, 119, 124, 127, 129, 130, 132, 133, 135, 136, 137, 139, 145, 150, 151, 152, 153, 154, 179], "transform_input": 151, "transformedtargetregressor": 145, "transfus": [1, 92, 97, 142, 143, 146, 165], "transpar": 97, "transport": [85, 136], "treat": [72, 85, 90, 133], "treatement": 5, "treatment": [5, 57, 73], "tree": [2, 5, 9, 12, 14, 15, 16, 17, 57, 73, 81, 84, 85, 87, 89, 91, 95, 100, 101, 102, 108, 109, 110, 112, 113, 114, 116, 118, 119, 120, 121, 122, 123, 126, 132, 139, 143, 146, 148, 150, 154, 155, 158, 159, 160, 163, 164, 170, 171, 172, 173, 174, 175, 177], "tree_clf": 161, "tree_idx": [110, 121], "tree_predict": [110, 121], "tree_reg": 161, "tree_residu": 115, "trees_classif": 3, "trees_dataset": 3, "trees_ex_01": 3, "trees_ex_02": 3, "trees_hyperparamet": 3, "trees_regress": 3, "trees_sol_01": 3, "trees_sol_02": 3, "tri": [2, 5, 65, 105, 109, 115, 133, 137], "trial": 96, "trick": 109, "tricki": 163, "trigger": 5, "trinadad": 84, "troubl": [17, 99], "true": [15, 17, 26, 28, 41, 46, 52, 57, 59, 65, 80, 91, 94, 95, 96, 99, 100, 101, 102, 104, 105, 106, 107, 108, 110, 111, 113, 114, 115, 116, 117, 118, 120, 122, 123, 125, 128, 129, 130, 132, 133, 134, 135, 136, 141, 142, 145, 147, 149, 150, 152, 156, 157, 163, 175, 179, 185], "true_": 142, "true_valu": [128, 134], "trust": [5, 79, 80, 81, 82, 85, 90, 96, 101, 109, 132, 133, 136, 137, 138, 139, 142, 150, 151, 152, 154, 157, 163, 164], "trustworthi": [76, 96], "truth": [5, 115, 142], "try": [2, 5, 28, 46, 73, 79, 81, 82, 85, 86, 88, 90, 96, 101, 105, 109, 110, 115, 117, 119, 121, 129, 131, 132, 133, 135, 136, 137, 138, 139, 142, 143, 145, 146, 147, 148, 150, 152, 154, 155, 156, 157, 163, 164, 177, 182], "tumor": 5, "tune": [2, 12, 16, 25, 34, 39, 42, 43, 46, 92, 96, 97, 102, 103, 111, 118, 119, 120, 121, 122, 125, 127, 131, 132, 137, 149, 153, 155, 156, 171, 177, 182, 185], "turn": [28, 72, 105, 129, 135], "tutori": 73, "tweak": 75, "tweedieregressor": 145, "twice": [59, 142, 185], "two": [2, 12, 13, 28, 36, 46, 63, 65, 72, 73, 74, 75, 79, 80, 81, 84, 85, 94, 96, 98, 99, 101, 102, 103, 104, 106, 108, 115, 117, 118, 125, 126, 127, 128, 132, 133, 134, 136, 137, 138, 139, 141, 142, 148, 150, 152, 153, 154, 155, 156, 157, 158, 160, 162, 163, 164, 177, 185], "type": [2, 13, 22, 28, 42, 46, 49, 70, 71, 73, 79, 80, 86, 87, 88, 89, 90, 101, 104, 105, 106, 107, 110, 124, 127, 142, 145, 177], "typic": [5, 12, 48, 70, 73, 84, 91, 101, 107, 115, 117, 124, 127, 132, 141, 145, 147, 152, 154, 172], "u": [5, 17, 35, 42, 65, 72, 73, 81, 84, 90, 91, 93, 96, 98, 101, 102, 103, 104, 105, 107, 108, 110, 118, 131, 132, 133, 137, 139, 141, 150, 152, 154, 162], "uci": 94, "ui": 90, "ulterior": 100, "un": [108, 133], "unabl": [79, 81, 82, 85, 90, 96, 99, 101, 109, 132, 133, 136, 138, 139, 142, 150, 152, 154, 157, 163, 164], "uncertainti": [2, 76, 85, 95, 110, 124, 127, 152], "unclear": 108, "uncom": [116, 117, 123], "uncorrel": 117, "under": [0, 23, 28, 35, 39, 73, 92, 97, 102, 118, 132, 139, 142, 145, 161, 170], "underconfid": 141, "underestim": 96, "underfit": [5, 12, 21, 28, 32, 38, 42, 43, 50, 52, 55, 57, 58, 59, 60, 95, 115, 117, 118, 119, 131, 132, 135, 137, 139, 157, 165, 170, 172], "underli": [80, 81, 84, 94, 109, 115, 142], "underscor": [73, 81, 151], "understand": [5, 12, 21, 32, 38, 55, 77, 79, 82, 84, 92, 94, 95, 97, 99, 102, 110, 128, 134, 142, 143, 146, 161, 162, 170, 171, 182], "undo": [103, 153], "uneven": 5, "unexpect": [73, 99], "unfold": 91, "uniform": [93, 98, 107, 110, 115, 132, 154], "uniform_dummi": 98, "uniformli": [98, 110, 137], "uniqu": [28, 59, 79, 84, 89, 94, 99, 110, 116, 150, 154], "unit": [5, 73, 79, 81, 84, 85, 91, 101, 103, 105, 107, 108, 109, 133, 136, 138, 140, 145, 150, 154], "univari": 125, "univers": 94, "unknown": [70, 84, 86, 88, 119, 145], "unknown_valu": [84, 85, 86, 87, 88, 89, 119, 148, 150, 152, 154, 155], "unlabel": 2, "unless": [87, 89], "unlik": [91, 102, 109, 113, 122, 157], "unlimit": [101, 117], "unmarri": [73, 79, 84, 85, 136, 150, 154], "unmeasur": 102, "unnecessari": [87, 89, 113, 122], "unnotic": 107, "unobserv": [102, 145], "unpredict": [102, 133], "unseen": [2, 65, 73, 94, 99, 100, 117, 157], "unselect": 110, "unstabl": [101, 133], "unsupervis": [5, 48], "unsur": [132, 141], "until": [28, 117, 142], "untract": 154, "unusu": 79, "unwant": [84, 107], "up": [2, 5, 9, 34, 43, 73, 76, 79, 84, 94, 96, 98, 101, 102, 103, 107, 108, 125, 128, 134, 139, 140, 142, 150, 162, 165, 182], "upcom": [73, 142, 151], "updat": [35, 48, 90, 185], "uphil": 105, "upon": [5, 90, 97, 98], "upper": [91, 94, 98, 99, 100, 105, 107, 108, 110, 115, 121, 131, 136, 137, 142, 157, 161, 163], "upper_bound": 94, "uranium": 105, "us": [0, 13, 16, 17, 21, 22, 23, 25, 26, 27, 28, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 43, 46, 48, 55, 59, 63, 64, 65, 67, 68, 70, 71, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 86, 88, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 151, 152, 153, 155, 156, 157, 158, 159, 160, 162, 163, 164, 165, 172, 173, 177, 179, 182, 185], "usa": 103, "usabl": 85, "usag": [12, 21, 32, 38, 55, 79, 104, 105, 106, 107, 119, 139, 152, 170, 182], "usd": 103, "use_encoded_valu": [84, 85, 86, 87, 88, 89, 119, 148, 150, 152, 154, 155], "useless": 89, "user": [2, 5, 76, 85, 90, 94, 99, 102, 117, 118, 119, 121, 132, 133, 135, 141, 145, 150, 154, 182, 185], "userwarn": 116, "usual": [2, 16, 79, 81, 85, 94, 100, 102, 105, 129, 135, 142, 153, 154], "usvi": 84, "util": [87, 89, 90, 98, 104], "v": [2, 81, 84, 131, 137, 142, 163], "v1": [150, 152, 154], "v_": 28, "v_a": 28, "v_d": 28, "vacat": [101, 107], "valero": 100, "valid": [12, 17, 21, 22, 23, 24, 25, 28, 32, 33, 38, 39, 43, 46, 55, 56, 57, 59, 64, 65, 72, 84, 86, 87, 88, 89, 91, 92, 93, 94, 97, 98, 99, 100, 107, 108, 113, 114, 115, 116, 117, 118, 119, 122, 123, 124, 125, 126, 127, 129, 130, 133, 135, 136, 142, 143, 144, 146, 147, 148, 151, 152, 153, 154, 155, 161, 162, 165, 170, 175, 177, 179, 182, 183, 185], "validationcurvedisplai": [92, 97, 102, 122, 129, 135], "valu": [2, 12, 14, 17, 23, 27, 28, 35, 36, 39, 41, 43, 46, 48, 59, 63, 65, 68, 72, 73, 77, 79, 82, 84, 85, 86, 88, 89, 91, 94, 96, 97, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 112, 113, 114, 115, 116, 117, 118, 119, 121, 122, 123, 125, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 145, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 160, 161, 162, 163, 164, 173, 174, 177, 179, 181, 182, 185], "valuabl": 154, "value_count": [59, 73, 74, 75, 83, 84, 99, 104, 106, 142, 185], "valueerror": [143, 146], "van": 5, "var": 73, "vari": [2, 17, 79, 92, 95, 97, 99, 102, 105, 108, 122, 123, 125, 133, 138, 140, 142, 151, 156, 177], "variabl": [5, 23, 28, 35, 39, 46, 48, 57, 59, 63, 65, 67, 68, 72, 76, 77, 80, 81, 82, 86, 88, 91, 94, 95, 100, 101, 102, 103, 104, 106, 107, 110, 117, 119, 130, 133, 136, 140, 141, 145, 150, 152, 154, 156, 158, 165, 177, 179, 185], "varianc": [50, 55, 57, 94, 102, 108, 110, 145, 165], "variant": 101, "variat": [57, 76, 91, 94, 95, 101, 102, 108, 110, 133, 145], "varieti": [5, 35, 76], "variou": [5, 55, 90], "vast": 35, "vector": [0, 41, 42, 92, 97, 99, 101, 115, 124, 127, 128, 132, 134, 139, 142], "veget": 105, "verbos": [120, 151, 154, 156, 181], "veri": [5, 17, 55, 59, 73, 79, 85, 87, 89, 94, 96, 97, 99, 102, 106, 107, 108, 110, 115, 117, 119, 123, 132, 133, 137, 139, 142, 150, 151, 152, 153, 154, 161, 179], "verifi": [100, 113, 122, 139], "versatil": 103, "versicolor": 2, "version": [35, 81, 104, 116, 118, 132, 154, 159, 163], "versu": [108, 145, 165], "vert": [96, 107, 123, 125, 126, 133, 136, 146], "vertic": [73, 103, 141], "via": [2, 28, 35, 76, 84, 96, 102, 107, 114, 115, 116, 118, 119, 123, 125, 141, 150, 151, 161], "video": [35, 90, 172], "vietnam": 84, "view": [89, 152], "violat": [84, 100], "virginica": 2, "viridi": [103, 105, 107, 153, 179], "visibl": [108, 145], "visit": 80, "visual": [63, 67, 72, 81, 84, 95, 97, 100, 101, 103, 105, 110, 115, 128, 130, 132, 133, 134, 136, 141, 142, 145, 150, 153, 157, 158, 162, 163, 165, 182], "vlo": 100, "vmax": [131, 132, 137, 150, 157, 163], "vmin": [131, 132, 137, 150, 157, 163], "voc": [73, 84, 136], "vocabulari": 49, "voic": 5, "vote": 152, "w": [105, 145, 163], "wa": [2, 73, 80, 81, 89, 90, 96, 97, 99, 101, 102, 104, 107, 109, 110, 115, 119, 125, 128, 133, 134, 137, 139, 142, 150, 152, 157, 162, 163], "wai": [5, 70, 72, 73, 76, 79, 83, 84, 96, 97, 98, 100, 102, 109, 110, 115, 116, 117, 124, 125, 127, 139, 142, 145, 162, 177, 185], "wall": [76, 85, 102, 118, 150, 154], "want": [2, 17, 73, 74, 75, 79, 80, 84, 87, 89, 90, 91, 92, 94, 95, 97, 99, 100, 101, 102, 103, 105, 109, 111, 112, 113, 116, 118, 119, 120, 121, 122, 125, 126, 128, 129, 131, 133, 134, 135, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 151, 152, 157, 158, 159, 160, 161, 162, 163, 164, 177, 179, 185], "ward": 103, "warm_start": 120, "warn": [86, 88, 109, 110, 116, 121, 133, 143, 146, 150, 152, 154], "wasn": 133, "wast": [5, 117], "watt": [28, 105], "wd": [90, 104], "we": [2, 12, 13, 17, 21, 22, 24, 27, 28, 32, 33, 36, 38, 39, 41, 46, 48, 52, 55, 57, 59, 63, 65, 68, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 170, 171, 177, 179, 181, 182, 185], "weak": [5, 15, 117, 137, 154], "weaker": 137, "weakest": 98, "webpag": 73, "websit": [2, 35, 48, 77, 82], "week": [73, 76, 78, 79, 80, 81, 83, 84, 85, 103, 130, 136, 150, 151, 152, 154], "weight": [2, 28, 39, 41, 43, 46, 79, 92, 94, 97, 108, 109, 115, 128, 129, 133, 134, 135, 138, 139, 140, 141, 157, 172], "weight_flipper_length": [128, 134, 138, 140], "weights_linear_regress": 133, "weights_ridg": 133, "weights_ridge_scaled_data": 133, "welcom": 156, "well": [2, 5, 12, 14, 21, 65, 75, 81, 87, 89, 98, 99, 100, 102, 106, 107, 108, 109, 110, 115, 117, 118, 126, 137, 142, 151, 154, 161], "went": 28, "were": [2, 28, 65, 76, 79, 85, 94, 101, 102, 106, 115, 124, 127, 128, 134, 142, 154, 156, 161, 164], "what": [2, 5, 14, 23, 26, 27, 28, 34, 41, 42, 46, 48, 49, 57, 59, 72, 73, 74, 75, 77, 78, 79, 81, 82, 83, 85, 91, 93, 96, 98, 99, 100, 101, 108, 118, 119, 130, 131, 132, 135, 136, 137, 138, 140, 141, 142, 143, 146, 151, 152, 157, 162, 177, 179], "whatev": [23, 153], "wheel": 2, "when": [2, 5, 15, 16, 17, 21, 22, 26, 27, 28, 34, 39, 43, 50, 52, 57, 59, 65, 72, 73, 76, 79, 80, 81, 84, 85, 86, 87, 88, 89, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 104, 105, 106, 108, 109, 113, 115, 116, 117, 119, 122, 123, 124, 125, 126, 127, 129, 132, 133, 135, 136, 137, 138, 140, 141, 142, 143, 146, 147, 150, 151, 152, 154, 157, 158, 174, 179, 182], "whenev": [85, 95, 105], "where": [2, 5, 28, 39, 41, 42, 43, 59, 68, 72, 76, 81, 86, 88, 92, 94, 97, 99, 100, 101, 102, 103, 107, 108, 113, 119, 122, 125, 130, 132, 133, 136, 137, 140, 141, 145, 150, 152, 154, 156, 157, 161, 163, 179, 185], "wherea": [81, 115, 117, 131, 135, 137, 147, 149, 156], "whether": [2, 24, 63, 72, 73, 76, 81, 85, 87, 89, 90, 92, 97, 101, 102, 103, 106, 108, 142, 147, 150, 152, 156, 160, 161, 164], "which": [5, 13, 15, 16, 17, 22, 24, 26, 28, 32, 33, 39, 41, 43, 46, 57, 65, 68, 71, 72, 73, 76, 79, 80, 81, 83, 84, 85, 90, 91, 92, 94, 95, 96, 97, 98, 100, 101, 102, 103, 105, 106, 107, 108, 109, 110, 115, 116, 117, 119, 124, 125, 126, 127, 128, 129, 130, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 145, 146, 147, 149, 150, 151, 153, 154, 156, 157, 161, 162, 163, 170, 171, 172, 177, 179, 181, 183, 185], "while": [2, 17, 28, 79, 80, 81, 84, 91, 97, 100, 101, 102, 103, 105, 107, 108, 109, 115, 116, 117, 119, 122, 125, 127, 129, 130, 132, 133, 135, 136, 137, 142, 145, 153, 156, 161, 177], "whisker": [96, 107, 123, 125, 126, 133, 136, 146], "white": [73, 79, 84, 85, 94, 103, 131, 132, 136, 137, 141, 150, 154, 163], "who": [2, 5, 72, 85, 142], "whole": [110, 112, 117, 119, 121, 133, 139], "whose": [2, 80, 116, 151], "why": [55, 59, 65, 73, 85, 93, 98, 105, 108, 137, 147, 161, 179], "wide": [43, 79], "wider": [5, 35, 52], "widow": [73, 79, 84, 136, 150, 154], "width": [2, 116], "wife": [73, 79, 84, 85, 136, 150, 154], "wikipedia": [2, 57, 110], "wilson": 94, "wind": 28, "winner": 5, "wise": [22, 132, 152, 153, 183], "with_mean": [149, 156], "with_std": [149, 156], "within": [28, 33, 76, 94, 96, 99, 101, 107, 108, 110, 114, 116, 123, 125, 132, 133, 144, 147, 152, 153, 157, 164, 183], "without": [14, 35, 37, 46, 59, 73, 77, 80, 82, 91, 94, 96, 97, 100, 101, 104, 105, 106, 117, 118, 125, 126, 129, 130, 131, 133, 135, 136, 137, 138, 142, 143, 145, 146, 165, 185], "witten": 5, "women": 5, "won": [84, 102, 108], "wooddecksf": [46, 72, 104, 177], "word": [2, 28, 99, 138, 160, 164, 179], "work": [5, 39, 59, 64, 73, 81, 84, 85, 99, 105, 110, 117, 119, 126, 127, 130, 133, 136, 148, 149, 151, 153, 155, 156, 157, 159, 162, 163, 165, 170, 185], "workaround": [87, 89], "workclass": [73, 79, 84, 85, 136, 150, 152, 154], "workclass_": [84, 136], "workclass_infrequent_sklearn": 136, "workflow": 73, "world": [2, 100, 162], "wors": [5, 17, 27, 46, 72, 91, 97, 98, 100, 138, 177], "worst": [133, 142], "worth": 108, "would": [2, 17, 28, 48, 73, 74, 75, 76, 78, 79, 80, 81, 83, 84, 86, 88, 94, 96, 97, 98, 99, 100, 101, 104, 106, 107, 108, 110, 116, 117, 119, 122, 125, 129, 132, 133, 135, 137, 138, 139, 140, 141, 142, 143, 145, 146, 149, 152, 156, 157, 161, 162], "wouldn": [101, 185], "wrap": [110, 133, 137, 165], "write": [2, 35, 74, 77, 78, 86, 87, 89, 92, 93, 94, 111, 112, 113, 114, 115, 124, 128, 129, 130, 131, 139, 143, 144, 148, 149, 155, 159, 160], "writer": 94, "writer_boundari": 94, "written": [73, 94, 107], "wrong": [98, 103, 124, 127, 153], "wrongli": [80, 85], "wrote": 94, "wspace": [104, 107], "www": [35, 73, 80, 101, 107], "x": [2, 23, 27, 28, 41, 42, 65, 73, 79, 80, 81, 95, 103, 105, 107, 108, 109, 110, 115, 121, 128, 129, 131, 132, 134, 135, 137, 138, 139, 140, 141, 149, 150, 153, 156, 157, 158, 161, 162, 163, 164, 179], "x0": 141, "x1": [129, 135, 141], "x2": [129, 135], "x27": [81, 85, 90, 96, 132, 133, 136, 139, 150, 152, 154], "x3": [129, 135], "x64": [116, 133], "x_1": 161, "x_2": 161, "x_i": 108, "x_max": [110, 115], "x_min": [110, 115], "x_permut": 108, "x_sampl": 115, "x_test": 108, "x_train": 108, "x_with_rnd_feat": 108, "xerr": 108, "xi": 94, "xl": 84, "xlabel": [91, 94, 96, 97, 98, 99, 101, 102, 104, 105, 106, 108, 122, 123, 125, 126, 132, 133, 135, 136, 141, 142], "xlim": [94, 108, 142], "xmax": 73, "xmin": 73, "xom": 100, "xor": 132, "xscale": [95, 133], "xtick": 94, "xy": 133, "y": [2, 27, 28, 41, 65, 73, 79, 80, 81, 91, 95, 99, 107, 108, 109, 110, 115, 121, 128, 131, 132, 134, 137, 138, 139, 140, 141, 145, 153, 157, 158, 161, 162, 163, 164, 179], "y_1": 161, "y_2": 161, "y_i": 91, "y_pred": [110, 145], "y_pred_first_and_second_tre": 115, "y_pred_first_tre": 115, "y_pred_proba": [141, 157], "y_proba_class_0": 157, "y_proba_sampl": 141, "y_test": 108, "y_test_pr": 115, "y_train": [108, 115, 174], "y_train_pr": 115, "y_true": 145, "yao": 94, "ye": [23, 41, 42], "year": [73, 79, 103, 104, 133, 150], "yearbuilt": [72, 104, 133], "yearremodadd": 104, "yellow": 153, "yerr": 133, "yet": [35, 72, 85, 87, 89, 133, 139], "yield": [46, 89, 100, 117, 138], "ylabel": [94, 99, 100, 102, 122, 123, 132, 133, 135, 136, 141, 142, 157], "ylgnbu": 150, "ylgnbu_r": 153, "ylim": [110, 136, 142], "ymax": [73, 107], "ymin": [73, 107], "you": [2, 5, 13, 17, 22, 28, 33, 35, 39, 46, 49, 57, 59, 65, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 91, 92, 93, 95, 97, 98, 99, 101, 102, 103, 105, 107, 108, 109, 111, 112, 113, 114, 116, 117, 118, 119, 120, 121, 122, 123, 124, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 150, 151, 152, 153, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 171, 175, 179, 181, 183, 185], "your": [2, 5, 33, 38, 59, 72, 73, 74, 77, 78, 82, 84, 85, 86, 87, 88, 89, 91, 92, 93, 97, 99, 111, 112, 113, 114, 117, 124, 127, 128, 129, 130, 131, 135, 136, 143, 144, 148, 149, 155, 159, 160, 164, 177, 179], "yourself": [131, 137], "youtub": 35, "yrsold": [90, 104, 133], "yscale": 133, "ytick": [94, 99], "yugoslavia": 84, "zenodo": 35, "zero": [2, 27, 28, 41, 42, 43, 52, 55, 73, 84, 94, 95, 102, 108, 133, 137, 139, 141], "zeros_lik": [94, 109], "zip": [94, 105, 115, 128, 132, 134, 163], "zip_longest": 104, "zone": [110, 137], "zoom": 115}, "titles": ["Acknowledgement", "Datasets description", "Glossary", "Notebook timings", "Table of contents", "Concluding remarks", "\ud83c\udfa5 Concluding remarks", "\ud83c\udfa5 Intuitions on ensemble models: bagging", "\ud83c\udfa5 Intuitions on ensemble models: boosting", "Ensemble based on boosting", "Ensemble method using bootstrapping", "Hyperparameter tuning with ensemble methods", "Module overview", "Main take-away", "\u2705 Quiz M6.01", "\u2705 Quiz M6.02", "\u2705 Quiz M6.03", "\ud83c\udfc1 Wrap-up quiz 6", "Comparing a model with simple baselines", "Choice of cross-validation", "Nested cross-validation", "Module overview", "Main take-away", "\u2705 Quiz M7.01", "\u2705 Quiz M7.02", "\u2705 Quiz M7.03", "\u2705 Quiz M7.04", "\u2705 Quiz M7.05", "\ud83c\udfc1 Wrap-up quiz 7", "Classification metrics", "Regression metrics", "Caveats of feature selection", "Module overview", "Main take-away", "\u2705 Quiz", "Introduction", "\u2705 Quiz", "Intuitions on linear models", "Module overview", "Main take-away", "Non-linear feature engineering for linear models", "\u2705 Quiz M4.01", "\u2705 Quiz M4.02", "\u2705 Quiz M4.03", "Regularization in linear model", "\ud83c\udfa5 Intuitions on linear models", "\ud83c\udfc1 Wrap-up quiz 4", "\ud83c\udfa5 Intuitions on regularized linear models", "\u2705 Quiz Intro.01", "\ud83c\udfa5 Introducing machine-learning concepts", "\u2705 Quiz M2.03", "\ud83c\udfa5 Bias versus Variance", "\u2705 Quiz M2.02", "\ud83c\udfa5 Comparing train and test errors", "Bias versus variance trade-off", "Module overview", "Overfitting and underfitting", "Main take-away", "Validation and learning curves", "\ud83c\udfc1 Wrap-up quiz 2", "\u2705 Quiz M2.01", "\ud83c\udfa5 Overfitting and Underfitting", "Tabular data exploration", "\u2705 Quiz M1.01", "Fitting a scikit-learn model on numerical data", "\u2705 Quiz M1.02", "\ud83c\udfa5 Validation of a model", "Handling categorical data", "\u2705 Quiz M1.03", "\ud83c\udfa5 Visualizing scikit-learn pipelines in Jupyter", "Module overview", "Main take-away", "\ud83c\udfc1 Wrap-up quiz 1", "First look at our dataset", "\ud83d\udcdd Exercise M1.01", "\ud83d\udcc3 Solution for Exercise M1.01", "Model evaluation using cross-validation", "\ud83d\udcdd Exercise M1.02", "\ud83d\udcdd Exercise M1.03", "Working with numerical data", "First model with scikit-learn", "Preprocessing for numerical features", "\ud83d\udcc3 Solution for Exercise M1.02", "\ud83d\udcc3 Solution for Exercise M1.03", "Encoding of categorical variables", "Using numerical and categorical variables together", "\ud83d\udcdd Exercise M1.04", "\ud83d\udcdd Exercise M1.05", "\ud83d\udcc3 Solution for Exercise M1.04", "\ud83d\udcc3 Solution for Exercise M1.05", "Visualizing scikit-learn pipelines in Jupyter", "Comparing model performance with a simple baseline", "\ud83d\udcdd Exercise M2.01", "\ud83d\udcdd Exercise M7.01", "Sample grouping", "Effect of the sample size in cross-validation", "Nested cross-validation", "\ud83d\udcc3 Solution for Exercise M2.01", "\ud83d\udcc3 Solution for Exercise M7.01", "Stratification", "Non i.i.d. data", "Cross-validation framework", "Overfit-generalization-underfit", "The adult census dataset", "The Ames housing dataset", "The bike rides dataset", "The blood transfusion dataset", "The California housing dataset", "Feature importance", "Adaptive Boosting (AdaBoost)", "Bagging", "\ud83d\udcdd Exercise M6.01", "\ud83d\udcdd Exercise M6.02", "\ud83d\udcdd Exercise M6.03", "\ud83d\udcdd Exercise M6.04", "Gradient-boosting decision tree", "Speeding-up gradient-boosting", "Hyperparameter tuning", "Introductory example to ensemble models", "Random forests", "\ud83d\udcc3 Solution for Exercise M6.01", "\ud83d\udcc3 Solution for Exercise M6.02", "\ud83d\udcc3 Solution for Exercise M6.03", "\ud83d\udcc3 Solution for Exercise M6.04", "\ud83d\udcdd Exercise 01", "Benefits of using feature selection", "Limitation of selecting feature using a model", "\ud83d\udcc3 Solution for Exercise 01", "\ud83d\udcdd Exercise M4.01", "\ud83d\udcdd Exercise M4.02", "\ud83d\udcdd Exercise M4.03", "\ud83d\udcdd Exercise M4.04", "Non-linear feature engineering for Logistic Regression", "Regularization of linear regression model", "\ud83d\udcc3 Solution for Exercise M4.01", "\ud83d\udcc3 Solution for Exercise M4.02", "\ud83d\udcc3 Solution for Exercise M4.03", "\ud83d\udcc3 Solution for Exercise M4.04", "Linear regression using scikit-learn", "Non-linear feature engineering for Linear Regression", "Linear regression without scikit-learn", "Linear models for classification", "Classification", "\ud83d\udcdd Exercise M7.02", "\ud83d\udcdd Exercise M7.03", "Regression", "\ud83d\udcc3 Solution for Exercise M7.02", "\ud83d\udcc3 Solution for Exercise M7.03", "\ud83d\udcdd Exercise M3.01", "\ud83d\udcdd Exercise M3.02", "Hyperparameter tuning by grid-search", "Set and get hyperparameters in scikit-learn", "Evaluation and hyperparameter tuning", "Analysis of hyperparameter search results", "Hyperparameter tuning by randomized-search", "\ud83d\udcc3 Solution for Exercise M3.01", "\ud83d\udcc3 Solution for Exercise M3.02", "Build a classification decision tree", "The penguins datasets", "\ud83d\udcdd Exercise M5.01", "\ud83d\udcdd Exercise M5.02", "Importance of decision tree hyperparameters on generalization", "Decision tree for regression", "\ud83d\udcc3 Solution for Exercise M5.01", "\ud83d\udcc3 Solution for Exercise M5.02", "Table of contents", "\ud83c\udfa5 Intuitions on tree-based models", "Decision tree in classification", "Hyperparameters of decision tree", "Intuitions on tree-based models", "Module overview", "Main take-away", "\u2705 Quiz M5.01", "\u2705 Quiz M5.02", "\u2705 Quiz M5.03", "\u2705 Quiz M5.04", "Decision tree in regression", "\ud83c\udfc1 Wrap-up quiz 5", "Automated tuning", "\u2705 Quiz M3.02", "Manual tuning", "\u2705 Quiz M3.01", "Module overview", "Main take-away", "\ud83c\udfa5 Analysis of hyperparameter search results", "\ud83c\udfc1 Wrap-up quiz 3"], "titleterms": {"0": 108, "01": [14, 23, 41, 48, 60, 63, 74, 75, 92, 93, 97, 98, 111, 120, 124, 127, 128, 134, 148, 155, 159, 163, 172, 181], "02": [15, 24, 42, 52, 65, 77, 82, 112, 121, 129, 135, 143, 146, 149, 156, 160, 164, 173, 179], "03": [16, 25, 43, 50, 68, 78, 83, 113, 122, 130, 136, 144, 147, 174], "04": [26, 86, 88, 114, 123, 131, 137, 175], "05": [27, 87, 89], "1": [5, 72, 108], "2": [5, 59, 108], "3": [5, 108, 185], "4": 46, "5": 177, "6": 17, "7": 28, "A": [108, 119], "One": [87, 89], "The": [5, 73, 76, 103, 104, 105, 106, 107, 142, 150, 158, 165], "Then": 90, "To": [13, 22, 33, 39, 57, 71, 171, 183], "With": 152, "about": [5, 119], "accuraci": 142, "acknowledg": 0, "adaboost": 109, "adapt": [5, 109], "addit": 132, "adult": [73, 103], "aggreg": 110, "all": 5, "am": 104, "an": [5, 84], "analysi": [89, 153, 184], "ani": 84, "annot": 2, "api": 2, "appendix": 165, "ar": 5, "associ": 108, "assum": 84, "attribut": [0, 2], "autom": 178, "awai": [13, 22, 33, 39, 57, 71, 108, 132, 171, 183], "bag": [7, 110], "base": [9, 84, 85, 166, 169], "baselin": [18, 91, 142], "befor": [12, 21, 32, 38, 55, 70, 170, 182], "benefit": 125, "best": 165, "beyond": 5, "bia": [51, 54], "bias": 5, "big": 5, "bigger": 5, "bike": 105, "blood": 106, "boost": [8, 9, 109, 115, 116, 117], "bootstrap": [10, 110], "boundari": [131, 137], "bring": 5, "build": 157, "c": [131, 137], "california": 107, "can": 90, "categor": [67, 84, 85, 87, 89], "categori": [84, 87, 89], "causal": 5, "caveat": 31, "censu": [73, 103], "certifi": 5, "check": 108, "choic": [5, 19], "choos": 84, "class": [142, 163], "classif": [2, 29, 141, 142, 157, 158, 167], "classifi": [2, 142], "code": [87, 89], "coeffici": 108, "column": [73, 85], "commun": 5, "compar": [18, 53, 91], "comparison": 115, "complex": [5, 110], "concept": [49, 165], "conclud": [5, 6, 165], "confus": 142, "content": [4, 165], "cours": [2, 35], "covari": 2, "cover": 5, "craft": 5, "creat": [73, 90, 161], "cross": [2, 19, 20, 76, 85, 90, 95, 96, 101], "cross_valid": 101, "curv": [58, 95, 102], "d": 100, "data": [2, 5, 62, 64, 67, 73, 76, 79, 80, 81, 84, 85, 100, 128, 134], "dataset": [1, 5, 73, 79, 80, 90, 103, 104, 105, 106, 107, 108, 152, 158], "decis": [73, 115, 117, 131, 137, 157, 161, 162, 165, 167, 168, 176], "default": 119, "definit": [128, 134], "deriv": 142, "descript": 1, "descriptor": 2, "detail": [101, 119], "differ": 142, "discuss": 108, "dispatch": 85, "earli": 2, "effect": [95, 133, 161], "encod": [84, 87, 89], "engin": [40, 131, 132, 137, 139], "ensembl": [7, 8, 9, 10, 11, 118, 165], "entir": 79, "error": [53, 101], "estim": [2, 101, 141, 163], "evalu": [5, 76, 84, 85, 142, 152, 165], "exampl": 118, "exercis": [74, 75, 77, 78, 82, 83, 86, 87, 88, 89, 92, 93, 97, 98, 111, 112, 113, 114, 120, 121, 122, 123, 124, 127, 128, 129, 130, 131, 134, 135, 136, 137, 143, 144, 146, 147, 148, 149, 155, 156, 159, 160, 163, 164], "explan": 91, "explor": 62, "featur": [2, 31, 40, 81, 84, 87, 89, 108, 125, 126, 131, 132, 133, 137, 139, 165], "feature_importances_": 108, "figur": 0, "final": 90, "first": [73, 80, 90, 115], "fit": [2, 64, 80, 81, 85], "follow": 35, "forest": [115, 117, 119], "framework": 101, "function": 161, "further": [5, 13, 22, 33, 39, 57, 71, 171, 183], "gbdt": 115, "gener": [2, 102, 161], "get": [5, 12, 21, 32, 38, 55, 70, 151, 170, 182], "glossari": 2, "go": [5, 13, 22, 33, 39, 57, 71, 171, 183], "goal": 5, "gradient": [115, 116, 117], "grid": 150, "group": 94, "hand": 73, "handl": 67, "have": 5, "helper": 161, "histogram": 117, "hot": [87, 89], "hous": [104, 107], "how": 5, "hyperparamet": [2, 11, 117, 119, 150, 151, 152, 153, 154, 161, 165, 168, 184], "i": [5, 87, 89, 100, 102], "identifi": [79, 84], "imbal": 142, "impact": [5, 131, 137], "import": [108, 161], "infer": 2, "influenc": [131, 137], "input": 2, "inspect": [73, 108], "instanc": 2, "integ": [87, 89], "interact": 132, "interpret": 165, "intro": 48, "introduc": 49, "introduct": 35, "introductori": 118, "intuit": [7, 8, 37, 45, 47, 166, 169], "issu": 142, "jupyt": [69, 90], "label": [2, 5], "lasso": 108, "last": 5, "learn": [2, 5, 12, 21, 32, 35, 38, 49, 55, 58, 64, 69, 70, 80, 90, 95, 108, 110, 138, 140, 151, 165, 170, 182], "lesson": [5, 108], "limit": 126, "linear": [37, 40, 44, 45, 47, 108, 131, 132, 133, 137, 138, 139, 140, 141, 165], "load": [73, 79, 80, 90, 128, 134, 152], "logist": 132, "look": [73, 119], "m1": [63, 65, 68, 74, 75, 77, 78, 82, 83, 86, 87, 88, 89], "m2": [50, 52, 60, 92, 97], "m3": [148, 149, 155, 156, 179, 181], "m4": [41, 42, 43, 128, 129, 130, 131, 134, 135, 136, 137], "m5": [159, 160, 163, 164, 172, 173, 174, 175], "m6": [14, 15, 16, 111, 112, 113, 114, 120, 121, 122, 123], "m7": [23, 24, 25, 26, 27, 93, 98, 143, 144, 146, 147], "machin": [5, 49, 165], "main": [2, 13, 22, 33, 39, 57, 71, 128, 134, 171, 183], "make": 80, "manual": 180, "materi": 35, "mathemat": 91, "matrix": [2, 142], "matter": 5, "max_depth": 161, "messag": [5, 132], "meta": 2, "method": [10, 11], "metric": [29, 30, 142], "model": [2, 5, 7, 8, 18, 37, 40, 44, 45, 47, 64, 66, 76, 80, 81, 85, 90, 91, 108, 118, 126, 128, 132, 133, 134, 141, 150, 152, 154, 165, 166, 169], "modul": [12, 21, 32, 38, 55, 70, 170, 182], "mooc": [5, 35], "more": [5, 85, 101], "most": 5, "multi": [132, 163], "need": [76, 150], "nest": [20, 96], "network": 35, "nois": 102, "nomin": 84, "non": [40, 100, 131, 132, 137, 139], "notebook": [3, 73, 76, 79, 80, 139], "numer": [64, 79, 81, 85, 87, 89], "object": [12, 21, 32, 38, 55, 70, 170, 182], "observ": 2, "off": 54, "open": 5, "order": 84, "ordin": 84, "other": [2, 161], "our": [73, 84, 150, 152, 154], "output": 5, "overfit": [2, 56, 61, 102], "overview": [12, 21, 32, 38, 55, 70, 170, 182], "panda": 80, "paramet": [2, 131, 133, 137, 161], "part": 5, "penal": 2, "penguin": 158, "perform": [2, 91, 165], "permut": 108, "pictur": 5, "pipelin": [5, 69, 84, 87, 89, 90, 110, 165], "power": 85, "predict": [2, 5, 80, 84, 141, 142, 150, 152, 154, 163, 165], "predictor": 2, "prepar": [76, 81], "preprocess": 81, "prerequisit": [35, 128, 134], "present": [35, 108], "probabl": [141, 142, 163], "problem": [5, 163], "processor": 85, "question": [14, 15, 16, 17, 23, 24, 25, 26, 27, 28, 34, 36, 41, 42, 43, 46, 48, 50, 52, 59, 60, 63, 65, 68, 72, 172, 173, 174, 175, 177, 179, 181, 185], "quiz": [14, 15, 16, 17, 23, 24, 25, 26, 27, 28, 34, 36, 41, 42, 43, 46, 48, 50, 52, 59, 60, 63, 65, 68, 72, 172, 173, 174, 175, 177, 179, 181, 185], "random": [115, 117, 119, 154], "randomforest": 108, "recap": [73, 76, 79, 80, 139], "refer": [87, 89], "regard": 101, "regress": [2, 30, 132, 133, 138, 139, 140, 145, 158, 162, 176], "regressor": 2, "regular": [2, 44, 47, 131, 133, 137], "remark": [5, 6, 165], "resampl": 110, "result": [153, 184], "ride": 105, "rule": 73, "sampl": [2, 94, 95], "scale": [87, 89, 108, 133], "schedul": [12, 21, 32, 38, 55, 70, 170, 182], "scikit": [5, 35, 64, 69, 80, 90, 110, 138, 140, 151], "score": 90, "search": [150, 153, 154, 184], "select": [31, 84, 85, 125, 126, 165], "separ": 80, "set": [2, 150, 151], "should": [87, 89], "sign": 108, "simpl": [18, 91], "size": 95, "small": 5, "social": 35, "societ": 5, "solut": [75, 82, 83, 88, 89, 97, 98, 120, 121, 122, 123, 127, 134, 135, 136, 137, 146, 147, 155, 156, 163, 164], "sourc": 5, "spars": 108, "specif": [5, 85], "speed": 116, "split": [79, 80], "stabil": 101, "start": [12, 21, 32, 38, 55, 70, 170, 182], "state": 2, "statist": 2, "step": 132, "stop": 2, "strategi": 84, "stratif": 99, "studi": 5, "summari": [95, 101, 102, 132], "supervis": 2, "surpris": 108, "tabl": [4, 165], "tabular": 62, "take": [13, 22, 33, 39, 57, 71, 108, 132, 171, 183], "target": [2, 80], "technic": 5, "term": 2, "test": [2, 53, 79, 80, 101], "thi": [2, 5], "threshold": 142, "time": [3, 5, 12, 21, 32, 38, 55, 70, 170, 182], "togeth": 85, "topic": 5, "trade": 54, "train": [2, 53, 79, 80, 101], "transform": 2, "transfus": 106, "tree": [115, 117, 157, 161, 162, 165, 166, 167, 168, 169, 176], "tune": [11, 117, 133, 150, 152, 154, 165, 178, 180], "type": [84, 85], "underfit": [2, 56, 61, 102], "unsupervis": 2, "up": [13, 17, 22, 28, 33, 39, 46, 57, 59, 71, 72, 116, 171, 177, 183, 185], "us": [2, 5, 10, 76, 85, 87, 89, 90, 125, 126, 138, 150, 154], "v": [101, 102, 115], "valid": [2, 5, 19, 20, 58, 66, 76, 85, 90, 95, 96, 101, 102, 150], "valu": 5, "variabl": [2, 73, 84, 85, 87, 89, 108], "varianc": [51, 54], "versu": [5, 51, 54], "visual": [69, 73, 90], "we": [5, 90], "weight": [131, 137], "welcom": 35, "what": [12, 21, 32, 38, 55, 70, 102, 170, 182], "which": [87, 89], "without": [84, 140, 152], "work": 79, "wrap": [13, 17, 22, 28, 33, 39, 46, 57, 59, 71, 72, 171, 177, 183, 185], "you": [12, 21, 32, 38, 55, 70, 170, 182]}})